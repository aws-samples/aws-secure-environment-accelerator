{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation \u00b6 Accelerator Installation and Upgrade Guide \u00b6 Link to Accelerator releases and change history Sample configuration files and customization details State Machine behavior and inputs Chart containing details as to WHAT we do and WHERE we support it (regions, accounts, etc.) Accelerator central logging bucket structures Unofficial Accelerator Roadmap (GitHub projects) - Please upvote desired features Accelerator Operations/Troubleshooting Guide \u00b6 Accelerator Basic Operation and Frequently Asked Questions (FAQ) \u00b6 Accelerator Developer Guide \u00b6 Accelerator release process (AWS Internal) Contributing & Governance Guide \u00b6 Prescriptive PBMM Architecture Design Document (Early Draft) \u00b6 Accelerator Prescriptive Architecture Sample Diagrams Workshops \u00b6 Configuration File Schema \u00b6 PDF Version of Documentation \u00b6 Note: Multiple ZIP files containing the various assets are attached to each release","title":"Home"},{"location":"#documentation","text":"","title":"Documentation"},{"location":"#accelerator-installation-and-upgrade-guide","text":"Link to Accelerator releases and change history Sample configuration files and customization details State Machine behavior and inputs Chart containing details as to WHAT we do and WHERE we support it (regions, accounts, etc.) Accelerator central logging bucket structures Unofficial Accelerator Roadmap (GitHub projects) - Please upvote desired features","title":"Accelerator Installation and Upgrade Guide"},{"location":"#accelerator-operationstroubleshooting-guide","text":"","title":"Accelerator Operations/Troubleshooting Guide"},{"location":"#accelerator-basic-operation-and-frequently-asked-questions-faq","text":"","title":"Accelerator Basic Operation and Frequently Asked Questions (FAQ)"},{"location":"#accelerator-developer-guide","text":"Accelerator release process (AWS Internal)","title":"Accelerator Developer Guide"},{"location":"#contributing-governance-guide","text":"","title":"Contributing &amp; Governance Guide"},{"location":"#prescriptive-pbmm-architecture-design-document-early-draft","text":"Accelerator Prescriptive Architecture Sample Diagrams","title":"Prescriptive PBMM Architecture Design Document (Early Draft)"},{"location":"#workshops","text":"","title":"Workshops"},{"location":"#configuration-file-schema","text":"","title":"Configuration File Schema"},{"location":"#pdf-version-of-documentation","text":"Note: Multiple ZIP files containing the various assets are attached to each release","title":"PDF Version of Documentation"},{"location":"toc-generation/","text":"Table of Contents Generation \u00b6 Easy and automated ToC generation: Visual Studio Code Plugin - Markdown All in One - https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one Note: The PDF engine ( pandoc/latex ) used in the Release Documentation action does not like deeply nested markdown lists. Accordingly, use Depth From: 2 and Depth to: 4 in Table of Contents generation. Other alternatives: Table of Contents can be generated for Markdown documents using the following tool: https://github.com/AlanWalk/Markdown-TOC","title":"Table of Contents Generation"},{"location":"toc-generation/#table-of-contents-generation","text":"Easy and automated ToC generation: Visual Studio Code Plugin - Markdown All in One - https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one Note: The PDF engine ( pandoc/latex ) used in the Release Documentation action does not like deeply nested markdown lists. Accordingly, use Depth From: 2 and Depth to: 4 in Table of Contents generation. Other alternatives: Table of Contents can be generated for Markdown documents using the following tool: https://github.com/AlanWalk/Markdown-TOC","title":"Table of Contents Generation"},{"location":"about/","text":"AWS Secure Environment Accelerator \u00b6 The AWS Accelerator is a tool designed to help deploy and operate secure multi-account, multi-region AWS environments on an ongoing basis. The power of the solution is the configuration file that drives the architecture deployed by the tool. This enables extensive flexibility and for the completely automated deployment of a customized architecture within AWS without changing a single line of code. While flexible, the AWS Accelerator is delivered with a sample configuration file which deploys an opinionated and prescriptive architecture designed to help meet the security and operational requirements of many governments around the world. Tuning the parameters within the configuration file allows for the deployment of customized architectures and enables the solution to help meet the multitude of requirements of a broad range of governments and public sector organizations. The installation of the provided prescriptive architecture is reasonably simple, deploying a customized architecture does require extensive understanding of the AWS platform. The sample deployment specifically helps customers meet NIST 800-53 and/or CCCS Medium Cloud Control Profile (formerly PBMM). What specifically does the Accelerator deploy and manage? \u00b6 A common misconception is that the AWS Secure Environment Accelerator only deploys security services, not true. The Accelerator is capable of deploying a complete end-to-end hybrid enterprise multi-region cloud environment. Additionally, while the Accelerator is initially responsible for deploying a prescribed architecture, it more importantly allows for organizations to operate, evolve, and maintain their cloud architecture and security controls over time and as they grow, with minimal effort, often using native AWS tools. Customers don't have to change the way they operate in AWS. Specifically the accelerator deploys and manages the following functionality, both at initial accelerator deployment and as new accounts are created, added, or onboarded in a completely automated but customizable manner: Creates AWS Account \u00b6 Core Accounts - as many or as few as your organization requires, using the naming you desire. These accounts are used to centralize core capabilities across the organization and provide Control Panel like capabilities across the environment. Common core accounts include: Shared Network Operations Perimeter Log Archive Security Tooling Workload Accounts - automated concurrent mass account creation or use AWS organizations to scale one account at a time. These accounts are used to host a customer's workloads and applications. Scalable to 1000's of AWS accounts Supports AWS Organizations nested ou's and importing existing AWS accounts Performs 'account warming' to establish initial limits, when required Automatically submits limit increases, when required (complies with initial limits until increased) Leverages AWS Control Tower (NEW) Creates Networking \u00b6 Transit Gateways and TGW route tables (incl. inter-region TGW peering) Centralized and/or Local (bespoke) VPC's Subnets, Route tables, NACLs, Security groups, NATGWs, IGWs, VGWs, CGWs VPC Endpoints (Gateway and Interface, Centralized or Local) Route 53 Private and Public Zones, Resolver Rules and Endpoints, VPC Endpoint Overloaded Zones All completely and individually customizable (per account, VPC, subnet, or OU) Layout and customize your VPCs, subnets, CIDRs and connectivity the way you want Static or Dynamic (NEW) VPC and subnet CIDR assignments Deletes default VPC's (worldwide) AWS Network Firewall (NEW) Cross-Account Object Sharing \u00b6 VPC and Subnet sharing, including account level re-tagging (Per account security group 'replication') VPC attachments and peering (local and cross-account) Zone sharing and VPC associations Managed Active Directory sharing, including R53 DNS resolver rule creation/sharing Automated TGW inter-region peering Populate Parameter Store with all user objects to be used by customers' IaC Deploy and share SSM documents (4 provided out-of-box, ELB Logging, S3 Encryption, Instance Profile remediation, Role remediation) customer can provide their own SSM documents for automated deployment and sharing Identity \u00b6 Creates Directory services (Managed Active Directory and Active Directory Connectors) Creates Windows admin bastion host auto-scaling group Set Windows domain password policies Set IAM account password policies Creates Windows domain users and groups (initial installation only) Creates IAM Policies, Roles, Users, and Groups Fully integrates with and leverages AWS SSO for centralized and federated login Cloud Security Services \u00b6 Enables and configures the following AWS services, worldwide w/central designated admin account: Guardduty w/S3 protection Security Hub (Enables designated security standards, and disables individual controls) Firewall Manager CloudTrail w/Insights and S3 data plane logging Config Recorders/Aggregator Conformance Packs and Config rules (95 out-of-box NIST 800-53 rules, 2 custom rules, customizable per OU) Macie IAM Access Analyzer CloudWatch access from central designated admin account (and setting Log group retentions) Other Security Capabilities \u00b6 Creates, deploys and applies Service Control Policies Creates Customer Managed KMS Keys (SSM, EBS, S3), EC2 key pairs, and secrets Enables account level default EBS encryption and S3 Block Public Access Configures Systems Manager Session Manager w/KMS encryption and centralized logging Creates and configures AWS budgets (customizable per ou and per account) Imports or requests certificates into AWS Certificate Manager Deploys both perimeter and account level ALB's w/Lambda health checks, certificates and TLS policies Deploys & configures 3rd party firewall clusters and management instances (leverages marketplace) Gateway Load Balancer w/auto-scaling (NEW) and VPN IPSec BGP ECMP deployment options Protects Accelerator deployed and managed objects Sets Up SNS Alerting topics (High, Medium, Low, Blackhole priorities) Deploys CloudWatch Log Metrics and Alarms Deploys customer provided custom config rules (1 provided out-of-box, No EC2 Instance Profile) Centralized Logging and Alerting \u00b6 Deploys an rsyslog auto-scaling cluster behind a NLB, all syslogs forwarded to CloudWatch Logs Centralized access to \"Cloud Security Service\" Consoles from designated AWS account Centralizes logging to a single centralized S3 bucket (enables, configures and centralizes) VPC Flow logs w/Enhanced metadata fields (also sent to CWL) Organizational Cost and Usage Reports CloudTrail Logs including S3 Data Plane Logs (also sent to CWL) All CloudWatch Logs (includes rsyslog logs) Config History and Snapshots Route 53 Public Zone Logs (also sent to CWL) GuardDuty Findings Macie Discovery results ALB Logs SSM Session Logs (also sent to CWL) Resolver Query Logs (also sent to CWL) Email alerting for CloudTrail Metric Alarms, Firewall Manager Events (NEW) , Security Hub Findings incl. Guardduty Findings (NEW) Relationship with AWS Landing Zone Solution (ALZ) \u00b6 The ALZ was an AWS Solution designed to deploy a multi-account AWS architecture for customers based on best practices and lessons learned from some of AWS' largest customers. The AWS Accelerator draws on design patterns from the Landing Zone, and re-uses several concepts and nomenclature, but it is not directly derived from it, nor does it leverage any code from the ALZ. The Accelerator is a standalone solution with no dependence on ALZ. Relationship with AWS Control Tower \u00b6 The AWS Secure Environment Accelerator now leverages AWS Control Tower! (NEW) With the release of v1.5.0, the AWS Accelerator adds the capability to be deployed on top of AWS Control Tower. Customers get the benefits of the fully managed capabilities of AWS Control Tower combined with the power and flexibility of the Accelerators Networking and Security orchestration. Accelerator Installation Process (Summary) \u00b6 This summarizes the installation process, the full installation document can be found in the documentation section below. Create a config.json (or config.yaml) file to represent your organizations requirements ( several samples provided ) Create a Secrets Manager Secret which contains a GitHub token that provides access to the Accelerator code repository Create a unique S3 input bucket in the management account of the region you wish to deploy the solution and place your config.json and any additional custom config files in the bucket Download and execute the latest release installer CloudFormation template in your management accounts preferred 'primary' / 'home' region Wait for: CloudFormation to deploy and start the Code Pipeline (~5 mins) Code Pipeline to download the Accelerator codebase and install the Accelerator State Machine (~10 mins) The Accelerator State Machine to finish execution (~1.25 hrs Standalone version, ~2.25 hrs Control Tower Version) Perform required one-time post installation activities (configure AWS SSO, set firewall passwords, etc.) On an ongoing basis: Use AWS Organizations to create new AWS accounts, which will automatically be guardrailed by the Accelerator Update the config file in CodeCommit and run the Accelerator State Machine to: deploy, configure and guardrail multiple accounts at the same time (~25 min Standalone, ~50 min/account Control Tower) change Accelerator configuration settings (~25 min)","title":"AWS Secure Environment Accelerator"},{"location":"about/#aws-secure-environment-accelerator","text":"The AWS Accelerator is a tool designed to help deploy and operate secure multi-account, multi-region AWS environments on an ongoing basis. The power of the solution is the configuration file that drives the architecture deployed by the tool. This enables extensive flexibility and for the completely automated deployment of a customized architecture within AWS without changing a single line of code. While flexible, the AWS Accelerator is delivered with a sample configuration file which deploys an opinionated and prescriptive architecture designed to help meet the security and operational requirements of many governments around the world. Tuning the parameters within the configuration file allows for the deployment of customized architectures and enables the solution to help meet the multitude of requirements of a broad range of governments and public sector organizations. The installation of the provided prescriptive architecture is reasonably simple, deploying a customized architecture does require extensive understanding of the AWS platform. The sample deployment specifically helps customers meet NIST 800-53 and/or CCCS Medium Cloud Control Profile (formerly PBMM).","title":"AWS Secure Environment Accelerator"},{"location":"about/#what-specifically-does-the-accelerator-deploy-and-manage","text":"A common misconception is that the AWS Secure Environment Accelerator only deploys security services, not true. The Accelerator is capable of deploying a complete end-to-end hybrid enterprise multi-region cloud environment. Additionally, while the Accelerator is initially responsible for deploying a prescribed architecture, it more importantly allows for organizations to operate, evolve, and maintain their cloud architecture and security controls over time and as they grow, with minimal effort, often using native AWS tools. Customers don't have to change the way they operate in AWS. Specifically the accelerator deploys and manages the following functionality, both at initial accelerator deployment and as new accounts are created, added, or onboarded in a completely automated but customizable manner:","title":"What specifically does the Accelerator deploy and manage?"},{"location":"about/#creates-aws-account","text":"Core Accounts - as many or as few as your organization requires, using the naming you desire. These accounts are used to centralize core capabilities across the organization and provide Control Panel like capabilities across the environment. Common core accounts include: Shared Network Operations Perimeter Log Archive Security Tooling Workload Accounts - automated concurrent mass account creation or use AWS organizations to scale one account at a time. These accounts are used to host a customer's workloads and applications. Scalable to 1000's of AWS accounts Supports AWS Organizations nested ou's and importing existing AWS accounts Performs 'account warming' to establish initial limits, when required Automatically submits limit increases, when required (complies with initial limits until increased) Leverages AWS Control Tower (NEW)","title":"Creates AWS Account"},{"location":"about/#creates-networking","text":"Transit Gateways and TGW route tables (incl. inter-region TGW peering) Centralized and/or Local (bespoke) VPC's Subnets, Route tables, NACLs, Security groups, NATGWs, IGWs, VGWs, CGWs VPC Endpoints (Gateway and Interface, Centralized or Local) Route 53 Private and Public Zones, Resolver Rules and Endpoints, VPC Endpoint Overloaded Zones All completely and individually customizable (per account, VPC, subnet, or OU) Layout and customize your VPCs, subnets, CIDRs and connectivity the way you want Static or Dynamic (NEW) VPC and subnet CIDR assignments Deletes default VPC's (worldwide) AWS Network Firewall (NEW)","title":"Creates Networking"},{"location":"about/#cross-account-object-sharing","text":"VPC and Subnet sharing, including account level re-tagging (Per account security group 'replication') VPC attachments and peering (local and cross-account) Zone sharing and VPC associations Managed Active Directory sharing, including R53 DNS resolver rule creation/sharing Automated TGW inter-region peering Populate Parameter Store with all user objects to be used by customers' IaC Deploy and share SSM documents (4 provided out-of-box, ELB Logging, S3 Encryption, Instance Profile remediation, Role remediation) customer can provide their own SSM documents for automated deployment and sharing","title":"Cross-Account Object Sharing"},{"location":"about/#identity","text":"Creates Directory services (Managed Active Directory and Active Directory Connectors) Creates Windows admin bastion host auto-scaling group Set Windows domain password policies Set IAM account password policies Creates Windows domain users and groups (initial installation only) Creates IAM Policies, Roles, Users, and Groups Fully integrates with and leverages AWS SSO for centralized and federated login","title":"Identity"},{"location":"about/#cloud-security-services","text":"Enables and configures the following AWS services, worldwide w/central designated admin account: Guardduty w/S3 protection Security Hub (Enables designated security standards, and disables individual controls) Firewall Manager CloudTrail w/Insights and S3 data plane logging Config Recorders/Aggregator Conformance Packs and Config rules (95 out-of-box NIST 800-53 rules, 2 custom rules, customizable per OU) Macie IAM Access Analyzer CloudWatch access from central designated admin account (and setting Log group retentions)","title":"Cloud Security Services"},{"location":"about/#other-security-capabilities","text":"Creates, deploys and applies Service Control Policies Creates Customer Managed KMS Keys (SSM, EBS, S3), EC2 key pairs, and secrets Enables account level default EBS encryption and S3 Block Public Access Configures Systems Manager Session Manager w/KMS encryption and centralized logging Creates and configures AWS budgets (customizable per ou and per account) Imports or requests certificates into AWS Certificate Manager Deploys both perimeter and account level ALB's w/Lambda health checks, certificates and TLS policies Deploys & configures 3rd party firewall clusters and management instances (leverages marketplace) Gateway Load Balancer w/auto-scaling (NEW) and VPN IPSec BGP ECMP deployment options Protects Accelerator deployed and managed objects Sets Up SNS Alerting topics (High, Medium, Low, Blackhole priorities) Deploys CloudWatch Log Metrics and Alarms Deploys customer provided custom config rules (1 provided out-of-box, No EC2 Instance Profile)","title":"Other Security Capabilities"},{"location":"about/#centralized-logging-and-alerting","text":"Deploys an rsyslog auto-scaling cluster behind a NLB, all syslogs forwarded to CloudWatch Logs Centralized access to \"Cloud Security Service\" Consoles from designated AWS account Centralizes logging to a single centralized S3 bucket (enables, configures and centralizes) VPC Flow logs w/Enhanced metadata fields (also sent to CWL) Organizational Cost and Usage Reports CloudTrail Logs including S3 Data Plane Logs (also sent to CWL) All CloudWatch Logs (includes rsyslog logs) Config History and Snapshots Route 53 Public Zone Logs (also sent to CWL) GuardDuty Findings Macie Discovery results ALB Logs SSM Session Logs (also sent to CWL) Resolver Query Logs (also sent to CWL) Email alerting for CloudTrail Metric Alarms, Firewall Manager Events (NEW) , Security Hub Findings incl. Guardduty Findings (NEW)","title":"Centralized Logging and Alerting"},{"location":"about/#relationship-with-aws-landing-zone-solution-alz","text":"The ALZ was an AWS Solution designed to deploy a multi-account AWS architecture for customers based on best practices and lessons learned from some of AWS' largest customers. The AWS Accelerator draws on design patterns from the Landing Zone, and re-uses several concepts and nomenclature, but it is not directly derived from it, nor does it leverage any code from the ALZ. The Accelerator is a standalone solution with no dependence on ALZ.","title":"Relationship with AWS Landing Zone Solution (ALZ)"},{"location":"about/#relationship-with-aws-control-tower","text":"The AWS Secure Environment Accelerator now leverages AWS Control Tower! (NEW) With the release of v1.5.0, the AWS Accelerator adds the capability to be deployed on top of AWS Control Tower. Customers get the benefits of the fully managed capabilities of AWS Control Tower combined with the power and flexibility of the Accelerators Networking and Security orchestration.","title":"Relationship with AWS Control Tower"},{"location":"about/#accelerator-installation-process-summary","text":"This summarizes the installation process, the full installation document can be found in the documentation section below. Create a config.json (or config.yaml) file to represent your organizations requirements ( several samples provided ) Create a Secrets Manager Secret which contains a GitHub token that provides access to the Accelerator code repository Create a unique S3 input bucket in the management account of the region you wish to deploy the solution and place your config.json and any additional custom config files in the bucket Download and execute the latest release installer CloudFormation template in your management accounts preferred 'primary' / 'home' region Wait for: CloudFormation to deploy and start the Code Pipeline (~5 mins) Code Pipeline to download the Accelerator codebase and install the Accelerator State Machine (~10 mins) The Accelerator State Machine to finish execution (~1.25 hrs Standalone version, ~2.25 hrs Control Tower Version) Perform required one-time post installation activities (configure AWS SSO, set firewall passwords, etc.) On an ongoing basis: Use AWS Organizations to create new AWS accounts, which will automatically be guardrailed by the Accelerator Update the config file in CodeCommit and run the Accelerator State Machine to: deploy, configure and guardrail multiple accounts at the same time (~25 min Standalone, ~50 min/account Control Tower) change Accelerator configuration settings (~25 min)","title":"Accelerator Installation Process (Summary)"},{"location":"architectures/pbmm/","text":"AWS Secure Environment Accelerator PBMM Architecture \u00b6 The AWS Secure Environment PBMM Architecture is a comprehensive, multi-account AWS cloud architecture, initially designed for use within the Government of Canada for PBMM workloads . The AWS Secure Environment PBMM Architecture has been designed to address central identity and access management, governance, data security, comprehensive logging, and network design/segmentation per Canadian Centre for Cyber Security ITSG-33 specifications (a NIST 800-53 variant). This document specifically does NOT talk about the tooling or mechanisms that can be used to deploy the architecture. While the AWS Secure Environment Accelerator (SEA) is one tool capable of deploying this architecture (along with many other architectures), customers can use whatever mechanism they deem appropriate to deploy it. This document does not discuss the AWS SEA tooling or architecture and is strictly focused on the resulting deployed solution created by using the provided sample PBMM Accelerator configuration file. This architecture document should stand on its own in depicting the deployed architecture. Users looking for information on the SEA tooling itself, should refer to the other SEA documents. It is anticipated we will offer multiple sample architectures with the AWS SEA solution, each having its own architecture document like this. As the SEA can produce hundreds of solutions, it does not make sense to repeat that content in this document. Introduction \u00b6 The AWS Secure Environment Architecture has been built with the following design principles: Maximize agility, scalability, and availability Enable the full capability of the AWS cloud Be adaptable to evolving technological capabilities in the underlying platform being used in the AWS Secure Environment Architecture Allow for seamless auto-scaling and provide unbounded bandwidth as bandwidth requirements increase (or decrease) based on actual customer load (a key aspect of the value proposition of cloud computing) Design for high availability: the design stretches across two physical AWS Availability Zones (AZ), such that the loss of any one AZ does not impact application availability. The design can be easily extended to a third availability zone. Operate as least privilege: all principals in the accounts are intended to operate with the lowest-feasible permission set. Purpose of Document \u00b6 This document is intended to outline the technical measures that are delivered by the AWS Secure Environment Architecture that make it suitable for PBMM workloads. An explicit non-goal of this document is to explain the delivery architecture of the AWS Secure Environment Accelerator tool itself, an open-source software project built by AWS. While the central purpose of the AWS Secure Environment Accelerator is to establish an AWS Secure Environment Architecture into an AWS account footprint, this amounts to an implementation detail as far as the AWS Secure Environment Architecture is concerned. The AWS Secure Environment Architecture is a standalone design, irrespective of how it was delivered into a customer AWS environment. It is nonetheless anticipated that most customers will choose to realize their AWS Secure Environment Architecture via the delivery mechanism of the AWS Secure Environment Accelerator tool . Comprehensive details on the tool itself are available elsewhere: AWS Secure Environment Accelerator tool Operations & Troubleshooting Guide AWS Secure Environment Accelerator tool Developer Guide Except where absolutely necessary, this document will refrain from referencing the AWS Secure Environment Accelerator tool further. Overview \u00b6 The central features of the AWS Secure Environment Architecture are as follows: AWS Organization with multiple-accounts: An AWS Organization is a grouping construct for a number of separate AWS accounts that are controlled by a single customer entity. This provides consolidated billing, organizational units, and facilitates the deployment of pan-Organizational guardrails such as CloudTrail logs and Service Control Policies. The separate accounts provide strong control-plane and data-plane isolation between workloads and/or environments. Encryption: AWS KMS with customer-managed CMKs is used extensively for any data stored at rest, in S3 buckets, EBS volumes, RDS encryption. Service Control Policies: SCPs provide a guardrail mechanism principally used to deny entire categories of API operations at an AWS account, OU, or Organization level. These can be used to ensure workloads are deployed only in prescribed regions, ensure only whitelisted services are used, or prevent the disablement of detective/preventative controls. Prescriptive SCPs are provided. Centralized, Isolated Networking: Virtual Private Clouds (VPCs) are used to create data-plane isolation between workloads, centralized in a shared-network account. Connectivity to on-prem environments, internet egress, shared resources and AWS APIs are mediated at a central point of ingress/egress via the use of Transit Gateway , Site-to-Site VPN , Next-Gen Firewalls, and AWS Direct Connect (where applicable). Centralized DNS Management: Amazon Route 53 is used to provide unified public and private hosted zones across the cloud environment. Inbound and Outbound Route 53 Resolvers extend this unified view of DNS to on-premises networks. Comprehensive Logging: CloudTrail logs are enabled Organization-wide to provide auditability across the cloud environment. CloudWatch Logs, for applications, as well as VPC flow logs, are centralized and deletion is prevented via SCPs. Detective Security Controls: Potential security threats are surfaced across the cloud environment via automatic deployment of detective security controls such as GuardDuty, AWS Config, and Security Hub. Single-Sign-On : AWS SSO is used to provide AD-authenticated IAM role assumption into accounts across the Organization for authorized principals. Document Convention \u00b6 Several conventions are used throughout this document to aid understanding. AWS Account Numbers \u00b6 AWS account numbers are decimal-digit pseudorandom identifiers with 12 digits (e.g. 651278770121 ). This document will use the convention that an AWS Organization Management (root) account has the account ID 123456789012 , and child accounts are given by 111111111111 , 222222222222 , etc. For example the following ARN would refer to a VPC subnet in the ca-central-1 region in the Organization Management (root) account: arn:aws:ec2:ca-central-1:123456789012:subnet/subnet-024759b61fc305ea3 JSON Annotation \u00b6 Throughout the document, JSON snippets may be annotated with comments (starting with // ). The JSON language itself does not define comments as part of the specification; these must be removed prior to use in most situations, including the AWS Console and APIs. For example: { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:root\" // Trust the Organization Management (root) account. }, \"Action\": \"sts:AssumeRole\" } The above is not valid JSON without first removing the comment on the fourth line. IP Addresses \u00b6 The design makes use of RFC1918 addresses (e.g. 10.1.0.0/16 ) and RFC6598 (e.g. 100.96.250.0/23 ) for various networks; these will be labeled accordingly. Any specific range or IP shown is purely for illustration purposes only. Department Naming \u00b6 This document will make no reference to specific Government of Canada departments. Where naming is required (e.g. in domain names), this document will use a placeholder name as needed; e.g. dept.gc.ca . Relationship to AWS Landing Zone \u00b6 AWS Landing Zone is an AWS Solution designed to deploy multi-account cloud architectures for customers. The AWS Secure Environment Architecture draws on design patterns from Landing Zone, and re-uses several concepts and nomenclature, but it is not directly derived from it. An earlier internal release of the AWS Secure Environment Architecture presupposed the existence of an AWS Landing Zone in the Organization; this requirement has since been removed as of release v1.1.0 .","title":"AWS Secure Environment Accelerator PBMM Architecture"},{"location":"architectures/pbmm/#aws-secure-environment-accelerator-pbmm-architecture","text":"The AWS Secure Environment PBMM Architecture is a comprehensive, multi-account AWS cloud architecture, initially designed for use within the Government of Canada for PBMM workloads . The AWS Secure Environment PBMM Architecture has been designed to address central identity and access management, governance, data security, comprehensive logging, and network design/segmentation per Canadian Centre for Cyber Security ITSG-33 specifications (a NIST 800-53 variant). This document specifically does NOT talk about the tooling or mechanisms that can be used to deploy the architecture. While the AWS Secure Environment Accelerator (SEA) is one tool capable of deploying this architecture (along with many other architectures), customers can use whatever mechanism they deem appropriate to deploy it. This document does not discuss the AWS SEA tooling or architecture and is strictly focused on the resulting deployed solution created by using the provided sample PBMM Accelerator configuration file. This architecture document should stand on its own in depicting the deployed architecture. Users looking for information on the SEA tooling itself, should refer to the other SEA documents. It is anticipated we will offer multiple sample architectures with the AWS SEA solution, each having its own architecture document like this. As the SEA can produce hundreds of solutions, it does not make sense to repeat that content in this document.","title":"AWS Secure Environment Accelerator PBMM Architecture"},{"location":"architectures/pbmm/#introduction","text":"The AWS Secure Environment Architecture has been built with the following design principles: Maximize agility, scalability, and availability Enable the full capability of the AWS cloud Be adaptable to evolving technological capabilities in the underlying platform being used in the AWS Secure Environment Architecture Allow for seamless auto-scaling and provide unbounded bandwidth as bandwidth requirements increase (or decrease) based on actual customer load (a key aspect of the value proposition of cloud computing) Design for high availability: the design stretches across two physical AWS Availability Zones (AZ), such that the loss of any one AZ does not impact application availability. The design can be easily extended to a third availability zone. Operate as least privilege: all principals in the accounts are intended to operate with the lowest-feasible permission set.","title":"Introduction"},{"location":"architectures/pbmm/#purpose-of-document","text":"This document is intended to outline the technical measures that are delivered by the AWS Secure Environment Architecture that make it suitable for PBMM workloads. An explicit non-goal of this document is to explain the delivery architecture of the AWS Secure Environment Accelerator tool itself, an open-source software project built by AWS. While the central purpose of the AWS Secure Environment Accelerator is to establish an AWS Secure Environment Architecture into an AWS account footprint, this amounts to an implementation detail as far as the AWS Secure Environment Architecture is concerned. The AWS Secure Environment Architecture is a standalone design, irrespective of how it was delivered into a customer AWS environment. It is nonetheless anticipated that most customers will choose to realize their AWS Secure Environment Architecture via the delivery mechanism of the AWS Secure Environment Accelerator tool . Comprehensive details on the tool itself are available elsewhere: AWS Secure Environment Accelerator tool Operations & Troubleshooting Guide AWS Secure Environment Accelerator tool Developer Guide Except where absolutely necessary, this document will refrain from referencing the AWS Secure Environment Accelerator tool further.","title":"Purpose of Document"},{"location":"architectures/pbmm/#overview","text":"The central features of the AWS Secure Environment Architecture are as follows: AWS Organization with multiple-accounts: An AWS Organization is a grouping construct for a number of separate AWS accounts that are controlled by a single customer entity. This provides consolidated billing, organizational units, and facilitates the deployment of pan-Organizational guardrails such as CloudTrail logs and Service Control Policies. The separate accounts provide strong control-plane and data-plane isolation between workloads and/or environments. Encryption: AWS KMS with customer-managed CMKs is used extensively for any data stored at rest, in S3 buckets, EBS volumes, RDS encryption. Service Control Policies: SCPs provide a guardrail mechanism principally used to deny entire categories of API operations at an AWS account, OU, or Organization level. These can be used to ensure workloads are deployed only in prescribed regions, ensure only whitelisted services are used, or prevent the disablement of detective/preventative controls. Prescriptive SCPs are provided. Centralized, Isolated Networking: Virtual Private Clouds (VPCs) are used to create data-plane isolation between workloads, centralized in a shared-network account. Connectivity to on-prem environments, internet egress, shared resources and AWS APIs are mediated at a central point of ingress/egress via the use of Transit Gateway , Site-to-Site VPN , Next-Gen Firewalls, and AWS Direct Connect (where applicable). Centralized DNS Management: Amazon Route 53 is used to provide unified public and private hosted zones across the cloud environment. Inbound and Outbound Route 53 Resolvers extend this unified view of DNS to on-premises networks. Comprehensive Logging: CloudTrail logs are enabled Organization-wide to provide auditability across the cloud environment. CloudWatch Logs, for applications, as well as VPC flow logs, are centralized and deletion is prevented via SCPs. Detective Security Controls: Potential security threats are surfaced across the cloud environment via automatic deployment of detective security controls such as GuardDuty, AWS Config, and Security Hub. Single-Sign-On : AWS SSO is used to provide AD-authenticated IAM role assumption into accounts across the Organization for authorized principals.","title":"Overview"},{"location":"architectures/pbmm/#document-convention","text":"Several conventions are used throughout this document to aid understanding.","title":"Document Convention"},{"location":"architectures/pbmm/#department-naming","text":"This document will make no reference to specific Government of Canada departments. Where naming is required (e.g. in domain names), this document will use a placeholder name as needed; e.g. dept.gc.ca .","title":"Department Naming"},{"location":"architectures/pbmm/#relationship-to-aws-landing-zone","text":"AWS Landing Zone is an AWS Solution designed to deploy multi-account cloud architectures for customers. The AWS Secure Environment Architecture draws on design patterns from Landing Zone, and re-uses several concepts and nomenclature, but it is not directly derived from it. An earlier internal release of the AWS Secure Environment Architecture presupposed the existence of an AWS Landing Zone in the Organization; this requirement has since been removed as of release v1.1.0 .","title":"Relationship to AWS Landing Zone"},{"location":"architectures/pbmm/accounts/","text":"Account Structure \u00b6 AWS accounts are a strong isolation boundary; by default there is zero control plane or data plane access from one AWS account to another. AWS Organizations is a service that provides centralized billing across a fleet of accounts, and optionally, some integration-points for cross-account guardrails and cross-account resource sharing. The AWS Secure Environment Architecture uses these features of AWS Organizations to realize its design. Accounts \u00b6 The AWS Secure Environment Architecture includes the following AWS accounts. Note that the account structure is strictly a control plane concept - nothing about this structure implies anything about the network design or network flows. Organization Management (root) AWS Account \u00b6 The AWS Organization resides in the Organization Management (root) AWS account. This account is not used for workloads (to the full extent possible) - it functions primarily as a billing aggregator, and a gateway to the entire cloud footprint for a high-trust principal. There exists a trust relationship between child AWS accounts in the Organization and the Organization Management (root) account; i.e. the child accounts have a role of this form: { \"Role\": { \"Path\": \"/\", \"RoleName\": \"AWSCloudFormationStackSetExecutionRole\", \"Arn\": \"arn:aws:iam::111111111111:role/AWSCloudFormationStackSetExecutionRole\", // Child account. \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:root\" // Organization Management (root) account may assume this role. }, \"Action\": \"sts:AssumeRole\" } ] } } } Note that this is a different role name than the default installed by AWS Organizations ( OrganizationAccountAccessRole ). AWS SSO \u00b6 AWS SSO resides in the Organization Management (root) account in the organization, due to a current requirement of the AWS SSO service. This service deploys IAM roles into the accounts in the Organization. More details on SSO are available in the Authentication and Authorization section. Organizational Units \u00b6 Underneath the root of the Organization, Organizational Units (OUs) provide an optional mechanism for grouping accounts into logical collections. Aside from the benefit of the grouping itself, these collections serve as the attachment points for SCPs (preventative API-blocking controls), and Resource Access Manager sharing (cross-account resource sharing). Example use cases are as follows: An SCP is attached to the core OU to prevent the deletion of Transit Gateway resources in the associated accounts. The shared network account uses RAM sharing to share the development line-of-business VPC with a development OU. This makes the VPC available to a functional account in that OU used by developers, despite residing logically in the shared network account. OUs may be nested (to a total depth of five), with SCPs and RAM sharing applied at the desired level. A typical AWS Secure Environment Architecture environment will have the following OUs: Core OU \u00b6 This OU houses all administrative accounts, such as the core landing zone accounts. No application accounts or application workloads are intended to exist within this OU. This OU also contains the centralized networking infrastructure in the SharedNetwork account. Central OU \u00b6 This OU houses accounts containing centralized resources, such as a shared AWS Directory Service (Microsoft AD) instance. Other shared resources such as software development tooling (source control, testing infrastructure), or asset repositories should be created in this OU. Functional OU: Sandbox \u00b6 This OU contains a set of Sandbox accounts used by development teams for proof of concept / prototyping work. These accounts are isolated at a network level and are not connected to the VPCs hosting development, test and production workloads. These accounts have direct internet access via an internet gateway (IGW). They do not route through the Perimeter Security services VPC for internet access. Functional OU: UnClass \u00b6 Accounts in this OU host unclassified application solutions. These accounts have internet access via the Perimeter firewall. This is an appropriate place to do cross-account unclassified collaboration with other departments or entities, or test services that are not available in the Canadian region. Functional OU: Dev \u00b6 Accounts in this OU host development tools and line of business application solutions that are part of approved releases and projects. These accounts have internet access via the Perimeter firewall. Functional OU: Test \u00b6 Accounts in this OU host test tools and line of business application solutions that are part of approved releases and projects. These accounts have internet access via the Perimeter firewall. Functional OU: Prod \u00b6 Accounts in this OU host production tools and line of business application solutions that are part of approved releases and projects. These accounts have internet access via the Perimeter firewall. Accounts in this OU are locked down with only specific Operations and Security personnel having access. Suspended OU \u00b6 A suspended OU is created to act as a container for end-of-life accounts or accounts with suspected credential leakage. The DenyAll SCP is applied, which prevents all control-plane API operations from taking place by any account principal. Mandatory Accounts \u00b6 The AWS Secure Environment Architecture is an opinionated design, which partly manifests in the accounts that are deemed mandatory within the Organization. The following accounts are assumed to exist, and each has an important function with respect to the goals of the overall Architecture (mandatory in red) Organization Management (root) \u00b6 As discussed above, the Organization Management (root) account functions as the root of the AWS Organization, the billing aggregator, attachment point for SCPs. Workloads are not intended to run in this account. Note: Customers deploying the AWS Secure Environment Architecture via the AWS Secure Environment Accelerator will deploy into this account. See the Operations Guide for more details. Perimeter \u00b6 The perimeter account, and in particular the perimeter VPC therein, functions as the single point of ingress/egress from the PBMM cloud environment to the public internet and/or on-premises network. This provides a central point of network control through which all workload-generated traffic, ingress and egress, must transit. The perimeter VPC hosts next-generation firewall instances that provide security services such as virus scanning, malware protection, intrusion protection, TLS inspection, and web application firewall functionality. More details on can be found in the Networking section of this document. Shared Network \u00b6 The shared network account hosts the vast majority of the AWS-side of the networking resources throughout the AWS Secure Environment Architecture . Workload-scoped VPCs ( Dev , Test , Prod , etc) are defined here, and shared via RAM sharing to the respective OUs in the Organization. A Transit Gateway provides connectivity from the workloads to the internet or on-prem, without permitting cross-environment (AKA \"East:West traffic\") traffic (e.g. there is no Transit Gateway route from the Dev VPC to the Prod VPC). More details on can be found in the Networking section of this document. Operations \u00b6 The operations account provides a central location for the cloud team to provide cloud operation services to other AWS accounts within the Organization and is where an organizations cloud operations team \"hangs out\" or delivers tooling applicable across all accounts in the organization. It provides ViewOnly access to CloudWatch logs and metrics across the organization. It's where centralized Systems Manager Session Manager Automation (remediation) documents are located. It's where organizations centralize backup automation (if automated), SSM inventory and patch jobs, and where AWS Managed Active Directory would typically be deployed and accessible to all workloads in the organization. In some AWS documentation this is referred to as the Shared Services account. Log Archive \u00b6 The log archive account provides a central aggregation and secure storage point for all audit logs created within the AWS Organization. This account contains a centralized location for copies of every account\u2019s Audit and Configuration compliance logs. It also provides a storage location for any other audit/compliance logs, as well as application/OS logs. The AWS CloudTrail service provides a full audit history of all actions taken against AWS services, including users logging into accounts. We recommend access to this account be restricted to auditors or security teams for compliance and forensic investigations related to account activity. Additional CloudTrail trails for operational use can be created in each account. Security \u00b6 The security account is restricted to authorized security and compliance personnel, and related security or audit tools. This is an aggregation point for security services, including AWS Security Hub, and serves as the Organization Management (root) for Amazon Guard Duty. A trust relationship with a readonly permission policy exists between every Organization account and the security account for audit and compliance purposes. DevOps account and/or Shared Team accounts \u00b6 Used to deliver CI/CD capabilities - two patterns are depicted in the architecture diagrams - The first has a single org wide central CI/CD tooling account, the other has a CI/CD and shared tooling account per major application team/grouping of teams/applications. Which is used will depend entirely on the org size, maturity model, delegation model of the organization and their team structures. We would generally still recommend CI/CD tooling in each developer account (i.e. using Code Commit) and when certain branch names were leveraged, causes the branch/PR to be automatically pulled into the centralized CI/CD tooling account and the approvals and promotion process which will push the code through the SDLC cycle to Test and Prod accounts, etc. Refer to this blog for more details on automating this pattern. Functional Accounts \u00b6 Functional accounts are created on demand, and placed into an appropriate OU in the Organization structure. The purpose of functional accounts is to provide a secure and managed environment where project teams can use AWS resources. They provide an isolated control plane so that the actions of one team in one account cannot inadvertently affect the work of other teams in other accounts. Functional accounts will gain access to the RAM shared resources of their respective parent OU. Accounts created for systemA and systemB in the Dev OU would have control plane isolation from each other; however these would both have access to the Dev VPC (shared from the SharedNetwork account). Data plane isolation within the same VPC is achieved by default, by using appropriate security groups whenever ingress is warranted. For example, the app tier of systemA should only permit ingress from the systemA-web security group, not an overly broad range such as 0.0.0.0/0 , or even the VPC range. Account Level Settings \u00b6 The AWS Secure Environment Architecture recommends the enabling of certain account-wide features on account creation. Namely, these include: S3 Public Access Block By-default encryption of EBS volumes . Private Marketplace \u00b6 The AWS Secure Environment Architecture recommends that the AWS Private Marketplace is enabled for the Organization. Private Marketplace helps administrators govern which products they want their users to run on AWS by making it possible to see only products that comply with their organization's procurement policy. When Private Marketplace is enabled, it will replace the standard AWS Marketplace for all users.","title":"Account Structure"},{"location":"architectures/pbmm/accounts/#account-structure","text":"AWS accounts are a strong isolation boundary; by default there is zero control plane or data plane access from one AWS account to another. AWS Organizations is a service that provides centralized billing across a fleet of accounts, and optionally, some integration-points for cross-account guardrails and cross-account resource sharing. The AWS Secure Environment Architecture uses these features of AWS Organizations to realize its design.","title":"Account Structure"},{"location":"architectures/pbmm/accounts/#accounts","text":"The AWS Secure Environment Architecture includes the following AWS accounts. Note that the account structure is strictly a control plane concept - nothing about this structure implies anything about the network design or network flows.","title":"Accounts"},{"location":"architectures/pbmm/accounts/#organization-management-root-aws-account","text":"The AWS Organization resides in the Organization Management (root) AWS account. This account is not used for workloads (to the full extent possible) - it functions primarily as a billing aggregator, and a gateway to the entire cloud footprint for a high-trust principal. There exists a trust relationship between child AWS accounts in the Organization and the Organization Management (root) account; i.e. the child accounts have a role of this form: { \"Role\": { \"Path\": \"/\", \"RoleName\": \"AWSCloudFormationStackSetExecutionRole\", \"Arn\": \"arn:aws:iam::111111111111:role/AWSCloudFormationStackSetExecutionRole\", // Child account. \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:root\" // Organization Management (root) account may assume this role. }, \"Action\": \"sts:AssumeRole\" } ] } } } Note that this is a different role name than the default installed by AWS Organizations ( OrganizationAccountAccessRole ).","title":"Organization Management (root) AWS Account"},{"location":"architectures/pbmm/accounts/#aws-sso","text":"AWS SSO resides in the Organization Management (root) account in the organization, due to a current requirement of the AWS SSO service. This service deploys IAM roles into the accounts in the Organization. More details on SSO are available in the Authentication and Authorization section.","title":"AWS SSO"},{"location":"architectures/pbmm/accounts/#organizational-units","text":"Underneath the root of the Organization, Organizational Units (OUs) provide an optional mechanism for grouping accounts into logical collections. Aside from the benefit of the grouping itself, these collections serve as the attachment points for SCPs (preventative API-blocking controls), and Resource Access Manager sharing (cross-account resource sharing). Example use cases are as follows: An SCP is attached to the core OU to prevent the deletion of Transit Gateway resources in the associated accounts. The shared network account uses RAM sharing to share the development line-of-business VPC with a development OU. This makes the VPC available to a functional account in that OU used by developers, despite residing logically in the shared network account. OUs may be nested (to a total depth of five), with SCPs and RAM sharing applied at the desired level. A typical AWS Secure Environment Architecture environment will have the following OUs:","title":"Organizational Units"},{"location":"architectures/pbmm/accounts/#mandatory-accounts","text":"The AWS Secure Environment Architecture is an opinionated design, which partly manifests in the accounts that are deemed mandatory within the Organization. The following accounts are assumed to exist, and each has an important function with respect to the goals of the overall Architecture (mandatory in red)","title":"Mandatory Accounts"},{"location":"architectures/pbmm/accounts/#organization-management-root","text":"As discussed above, the Organization Management (root) account functions as the root of the AWS Organization, the billing aggregator, attachment point for SCPs. Workloads are not intended to run in this account. Note: Customers deploying the AWS Secure Environment Architecture via the AWS Secure Environment Accelerator will deploy into this account. See the Operations Guide for more details.","title":"Organization Management (root)"},{"location":"architectures/pbmm/accounts/#perimeter","text":"The perimeter account, and in particular the perimeter VPC therein, functions as the single point of ingress/egress from the PBMM cloud environment to the public internet and/or on-premises network. This provides a central point of network control through which all workload-generated traffic, ingress and egress, must transit. The perimeter VPC hosts next-generation firewall instances that provide security services such as virus scanning, malware protection, intrusion protection, TLS inspection, and web application firewall functionality. More details on can be found in the Networking section of this document.","title":"Perimeter"},{"location":"architectures/pbmm/accounts/#shared-network","text":"The shared network account hosts the vast majority of the AWS-side of the networking resources throughout the AWS Secure Environment Architecture . Workload-scoped VPCs ( Dev , Test , Prod , etc) are defined here, and shared via RAM sharing to the respective OUs in the Organization. A Transit Gateway provides connectivity from the workloads to the internet or on-prem, without permitting cross-environment (AKA \"East:West traffic\") traffic (e.g. there is no Transit Gateway route from the Dev VPC to the Prod VPC). More details on can be found in the Networking section of this document.","title":"Shared Network"},{"location":"architectures/pbmm/accounts/#operations","text":"The operations account provides a central location for the cloud team to provide cloud operation services to other AWS accounts within the Organization and is where an organizations cloud operations team \"hangs out\" or delivers tooling applicable across all accounts in the organization. It provides ViewOnly access to CloudWatch logs and metrics across the organization. It's where centralized Systems Manager Session Manager Automation (remediation) documents are located. It's where organizations centralize backup automation (if automated), SSM inventory and patch jobs, and where AWS Managed Active Directory would typically be deployed and accessible to all workloads in the organization. In some AWS documentation this is referred to as the Shared Services account.","title":"Operations"},{"location":"architectures/pbmm/accounts/#log-archive","text":"The log archive account provides a central aggregation and secure storage point for all audit logs created within the AWS Organization. This account contains a centralized location for copies of every account\u2019s Audit and Configuration compliance logs. It also provides a storage location for any other audit/compliance logs, as well as application/OS logs. The AWS CloudTrail service provides a full audit history of all actions taken against AWS services, including users logging into accounts. We recommend access to this account be restricted to auditors or security teams for compliance and forensic investigations related to account activity. Additional CloudTrail trails for operational use can be created in each account.","title":"Log Archive"},{"location":"architectures/pbmm/accounts/#security","text":"The security account is restricted to authorized security and compliance personnel, and related security or audit tools. This is an aggregation point for security services, including AWS Security Hub, and serves as the Organization Management (root) for Amazon Guard Duty. A trust relationship with a readonly permission policy exists between every Organization account and the security account for audit and compliance purposes.","title":"Security"},{"location":"architectures/pbmm/accounts/#devops-account-andor-shared-team-accounts","text":"Used to deliver CI/CD capabilities - two patterns are depicted in the architecture diagrams - The first has a single org wide central CI/CD tooling account, the other has a CI/CD and shared tooling account per major application team/grouping of teams/applications. Which is used will depend entirely on the org size, maturity model, delegation model of the organization and their team structures. We would generally still recommend CI/CD tooling in each developer account (i.e. using Code Commit) and when certain branch names were leveraged, causes the branch/PR to be automatically pulled into the centralized CI/CD tooling account and the approvals and promotion process which will push the code through the SDLC cycle to Test and Prod accounts, etc. Refer to this blog for more details on automating this pattern.","title":"DevOps account and/or Shared Team accounts"},{"location":"architectures/pbmm/accounts/#functional-accounts","text":"Functional accounts are created on demand, and placed into an appropriate OU in the Organization structure. The purpose of functional accounts is to provide a secure and managed environment where project teams can use AWS resources. They provide an isolated control plane so that the actions of one team in one account cannot inadvertently affect the work of other teams in other accounts. Functional accounts will gain access to the RAM shared resources of their respective parent OU. Accounts created for systemA and systemB in the Dev OU would have control plane isolation from each other; however these would both have access to the Dev VPC (shared from the SharedNetwork account). Data plane isolation within the same VPC is achieved by default, by using appropriate security groups whenever ingress is warranted. For example, the app tier of systemA should only permit ingress from the systemA-web security group, not an overly broad range such as 0.0.0.0/0 , or even the VPC range.","title":"Functional Accounts"},{"location":"architectures/pbmm/accounts/#account-level-settings","text":"The AWS Secure Environment Architecture recommends the enabling of certain account-wide features on account creation. Namely, these include: S3 Public Access Block By-default encryption of EBS volumes .","title":"Account Level Settings"},{"location":"architectures/pbmm/accounts/#private-marketplace","text":"The AWS Secure Environment Architecture recommends that the AWS Private Marketplace is enabled for the Organization. Private Marketplace helps administrators govern which products they want their users to run on AWS by making it possible to see only products that comply with their organization's procurement policy. When Private Marketplace is enabled, it will replace the standard AWS Marketplace for all users.","title":"Private Marketplace"},{"location":"architectures/pbmm/auth/","text":"Authorization and Authentication \u00b6 The AWS Secure Environment Architecture makes extensive use of AWS authorization and authentication primitives from the Identity and Access Management (IAM) service as a means to enforce the guardrail objectives of the AWS Secure Environment Architecture , and govern access to the set of accounts that makes up the Organization. Relationship to the Organization Management (root) AWS Account \u00b6 AWS accounts, as a default position, are entirely self-contained with respect to IAM principals - their Users, Roles, Groups are independent and scoped only to themselves. Accounts created by AWS Organizations deploy a default role with a trust policy back to the Organization Management (root). By default, this role is named the OrganizationAccountAccessRole ; by contrast, the AWS Secure Environment Architecture allows customers to customize this role by defining it in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ). { \"Role\": { \"Path\": \"/\", \"RoleName\": \"AWSCloudFormationStackSetExecutionRole\", \"Arn\": \"arn:aws:iam::111111111111:role/AWSCloudFormationStackSetExecutionRole\", // Child account. \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:root\" // Organization Management (root) account may assume this role. }, \"Action\": \"sts:AssumeRole\" } ] } } } As discussed, the AWS Organization resides in the Organization Management (root) account. This account is not used for workloads and is primarily a gateway to the entire cloud footprint for a high-trust principal. This is realized via the role defined in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ). It is therefore crucial that the Organization Management (root) account root credentials be handled with extreme diligence, and with a U2F hardware key enabled as a second-factor (and stored in a secure location such as a safe). Break Glass Accounts \u00b6 Given the Organizational-wide trust relationship to the role defined in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ) and its broad exclusion from SCPs (discussed below), the assumption of this role grants 'super admin' status, and is thus an extremely high privilege operation. The ability to assume this role should be considered a 'break glass' capability - to be used only in extraordinary circumstances. Access to this role can be granted by IAM Users or IAM Roles in the Organization Management (root) account (via SSO) - as with the Organization Management (root) account credentials, these should be handled with extreme diligence, and with a U2F hardware key enabled as a second-factor (and stored in a secure location such as a safe). TBD: This role was locked down starting in v1.2.5 - Add further details here /TODO Control Plane Access via AWS SSO \u00b6 The vast majority of end-users of the AWS cloud within the Organization will never use or interact with the Organization Management (root) account, or indeed the root users of any child account in the Organization. The AWS Secure Environment Architecture recommends instead that AWS SSO be provisioned in the Organization Management (root) account (a rare case where Organization Management (root) account deployment is mandated). Users will login to AWS via the web-based endpoint for the AWS SSO service: Via an AWS Directory Connector deployed in the Organization Management (root) account, AWS SSO will authenticate the user based on the underlying Microsoft AD installation (in the Central account). Based on group membership, the user will be presented with a set of roles to assume into those accounts. For example, a developer may be placed into groups that permit Admin access in the Dev account and Readonly access in Test ; meanwhile an IT Director may have high-privilege access to most, or all, accounts. In effect, AWS SSO adds SAML IdP capabilities to the AWS Managed Microsoft AD, with the AWS Console acting as a service-provider (SP) in SAML parlance. Other SAML-aware SPs may also be used with AWS SSO. SSO User Roles \u00b6 AWS SSO creates an identity provider (IdP) in each account in the Organization. The roles used by end users have a trust policy to this IdP. When a user authenticates to AWS SSO (via the underlying AD Connector) and selects a role to assume based on their group membership, the SSO service provides the user with temporary security credentials unique to the role session. In such a scenario, the user has no long-term credentials (e.g. password, or access keys) and instead uses their temporary security credentials. Users, via their AD group membership, are ultimately assigned to SSO User Roles via the use of AWS SSO Permission Sets. A permission set is an assignment of a particular permission policy to a set of accounts. For example: An organization might decide to use AWS Managed Policies for Job Functions that are located within the SSO service as the baseline for role-based-access-control (RBAC) separation within an AWS account. This enables job function policies such as: Administrator - This policy grants almost all actions for all AWS services and for all resources in the account. Developer Power User - This user performs application development tasks and can create and configure resources and services that support AWS aware application development. Database Administrator - This policy grants permissions to create, configure, and maintain databases. It includes access to AWS database services, such as Amazon DynamoDB, Amazon Relational Database Service (RDS), and Amazon Redshift. View-Only User - This policy grants List* , Describe* , Get* , View* , and Lookup* access to resources for most AWS services. Principal Authorization \u00b6 Having assumed a role, a user\u2019s permission-level within an AWS account with respect to any API operation is governed by the IAM policy evaluation logic flow ( detailed here ): Having an Allow to a particular API operation from the Role (i.e. Session Policy) does not necessarily imply that API operation will succeed. As depicted above, Deny may result due to another evaluation stage in the logic; for example a restrictive permission boundary or an explicit Deny at the Resource or SCP (account) level. SCPs are used extensively as a guardrailing mechanism in the AWS Secure Environment Architecture , and are discussed in a later section. Root Authorization \u00b6 Root credentials for individual accounts in an AWS organization may be created on demand via a password reset process on the unique account email address; however, the AWS Secure Environment Architecture specifically denies this via SCP. Root credentials authorize all actions for all AWS services and for all resources in the account (except anything denied by SCPs). There are some actions which only root has the capability to perform which are found within the AWS online documentation . These are typically rare operations (e.g. creation of X.509 keys), and should not be required in the normal course of business. Any root credentials, if ever they need to be created, should be handled with extreme diligence, with U2F MFA enabled. Service Roles \u00b6 A service role is an IAM Role that a service assumes to perform actions in an account on the user\u2019s behalf. When a user sets up AWS service environments, the user must define an IAM Role for the service to assume. This service role must include all the permissions that are required for the service to access the AWS resources that it needs. Service roles provide access only within a single account and cannot be used to grant access to services in other accounts. Users can create, modify, and delete a service role from within the IAM service. For example, a user can create a role that allows Amazon Redshift to access an Amazon S3 bucket on the user\u2019s behalf and then load data from that bucket into an Amazon Redshift cluster. In the case of SSO, during the process in which AWS SSO is enabled, the AWS Organizations service grants AWS SSO the necessary permissions to create subsequent IAM Roles. Service Control Policies \u00b6 Service Control Policies are a key preventative control recommended by the AWS Secure Environment Architecture . It is crucial to note that SCPs, by themselves, never grant permissions. They are most often used to Deny certain actions at a root, OU, or account level within an AWS Organization. Since Deny always overrides Allow in the IAM policy evaluation logic, SCPs can have a powerful effect on all principals in an account, and can wholesale deny entire categories of actions irrespective of the permission policy attached to the principal itself - even the root user of the account. SCPs follow an inheritance pattern from the root of the Organization: In order for any principal to be able to perform an action A, it is necessary (but not sufficient) that there is an Allow on action A from all levels of the hierarchy down to the account, and no explicit Deny anywhere. This is discussed in further detail in How SCPs Work . The AWS Secure Environment Architecture recommends the following SCPs in the Organization: PBMM Only \u00b6 This is a comprehensive policy whose main goal is to provide a PBMM-compliant cloud environment, namely prohibiting any non-centralized networking, and mandating data residency in Canada. It should be attached to all non- Unclass OUs. Policy Statement ID (SID) Description DenyNetworkPBMMONLY Prevents the creation of any networking infrastructure in the workload accounts such as VPCs, NATs, VPC peers, etc. DenyAllOutsideCanadaPBMMONLY Prevents the use of any service in any non-Canadian AWS region with the exception of services that are considered global; e.g. CloudFront, IAM, STS, etc ScopeSpecificGlobalActionsToCanadaUSE1 Within services that are exempted from DenyAllOutsideCanadaPBMMONLY , scope the use of those services to the us-east-1 region PBMM Unclass Only \u00b6 This is broadly similar to PBMM Only ; however it relaxes the requirement for Canadian region usage, and does not prohibit network infrastructure creation (e.g. VPCs, IGWs). This is appropriate for OUs in which AWS service experimentation is taking place. Policy Statement ID (SID) Description DenyUnclass Prevents the deletion of KMS encryption keys and IAM password policies DenyAllOutsideCanadaUS Prevents the use of any service in any region that is not ca-central-1 or us-east-1 , with the exception of services that are considered global; e.g. CloudFront, IAM, STS, etc PBMM Guardrails (Parts 1 and 2) \u00b6 PBMM Guardrails apply across the Organization. These guardrails protect key infrastructure, mandate encryption at rest, and prevent other non-PBMM configurations. Note that this guardrail is split into two parts due to a current limitation of SCP sizing, but logically it should be considered a single policy. Policy Statement ID (SID) Description DenyTag1 Prevents modification of any protected security group DenyTag2 Prevents modification of any protected IAM resource DenyS3 Prevents modification of any S3 bucket used for Accelerator purposes ProtectCloudFormation Prevents modification of any CloudFormation stack used for Accelerator tool purposes DenyAlarmDeletion Prevents modification of any cloudwatch alarm used to alert on significant control plane events ProtectKeyRoles Prevents any IAM operation on Accelerator tool IAM roles DenySSMDel Prevents modification of any ssm resource used for Accelerator tool purposes DenyLogDel Prevents the deletion of any log resource in Cloudwatch Logs DenyLeaveOrg Prevents an account from leaving the Organization DenyLambdaDel Prevents the modification of any guardrail Lambda function BlockOther Prevents miscellaneous operations; e.g. Deny ds:DisableSso BlockMarketplacePMP Prevents the modification or creation of a cloud private marketplace DenyRoot Prevents the use of the root user in an account EnforceEbsEncryption Enforces the use of volume level encryption in running instances EnforceEBSVolumeEncryption Enforces the use of volume level encryption with EBS EnforceRdsEncryption Enforces the use of RDS encryption EnforceAuroraEncryption Enforces the use of Aurora encryption DenyRDGWRole Prevents the modification of a role used for Remote Desktop Gateway DenyGDSHFMAAChange Prevents the modification of GuardDuty & Security Hub Encryption at Rest \u00b6 Note that the *Encryption* SCP statements above, taken together, mandate encryption at rest for block storage volumes used in EC2 and RDS instances. Quarantine Deny All \u00b6 This policy can be attached to an account to 'quarantine' it - to prevent any AWS operation from taking place. This is useful in the case of an account with credentials which are believed to have been compromised. Policy Statement ID (SID) Description DenyAllAWSServicesExceptBreakglassRoles Blanket denial on all AWS control plane operations for all non-break-glass roles Quarantine New Object \u00b6 This policy is applied to new accounts upon creation. After the installation of guardrails, it is removed. In the meantime, it prevents all AWS control plane operations except by principals required to deploy guardrails. Policy Statement ID (SID) Description DenyAllAWSServicesExceptBreakglassRoles Blanket denial on all AWS control plane operations for all non-break-glass roles","title":"Authentication & Authorization"},{"location":"architectures/pbmm/auth/#authorization-and-authentication","text":"The AWS Secure Environment Architecture makes extensive use of AWS authorization and authentication primitives from the Identity and Access Management (IAM) service as a means to enforce the guardrail objectives of the AWS Secure Environment Architecture , and govern access to the set of accounts that makes up the Organization.","title":"Authorization and Authentication"},{"location":"architectures/pbmm/auth/#relationship-to-the-organization-management-root-aws-account","text":"AWS accounts, as a default position, are entirely self-contained with respect to IAM principals - their Users, Roles, Groups are independent and scoped only to themselves. Accounts created by AWS Organizations deploy a default role with a trust policy back to the Organization Management (root). By default, this role is named the OrganizationAccountAccessRole ; by contrast, the AWS Secure Environment Architecture allows customers to customize this role by defining it in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ). { \"Role\": { \"Path\": \"/\", \"RoleName\": \"AWSCloudFormationStackSetExecutionRole\", \"Arn\": \"arn:aws:iam::111111111111:role/AWSCloudFormationStackSetExecutionRole\", // Child account. \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::123456789012:root\" // Organization Management (root) account may assume this role. }, \"Action\": \"sts:AssumeRole\" } ] } } } As discussed, the AWS Organization resides in the Organization Management (root) account. This account is not used for workloads and is primarily a gateway to the entire cloud footprint for a high-trust principal. This is realized via the role defined in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ). It is therefore crucial that the Organization Management (root) account root credentials be handled with extreme diligence, and with a U2F hardware key enabled as a second-factor (and stored in a secure location such as a safe).","title":"Relationship to the Organization Management (root) AWS Account"},{"location":"architectures/pbmm/auth/#break-glass-accounts","text":"Given the Organizational-wide trust relationship to the role defined in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ) and its broad exclusion from SCPs (discussed below), the assumption of this role grants 'super admin' status, and is thus an extremely high privilege operation. The ability to assume this role should be considered a 'break glass' capability - to be used only in extraordinary circumstances. Access to this role can be granted by IAM Users or IAM Roles in the Organization Management (root) account (via SSO) - as with the Organization Management (root) account credentials, these should be handled with extreme diligence, and with a U2F hardware key enabled as a second-factor (and stored in a secure location such as a safe). TBD: This role was locked down starting in v1.2.5 - Add further details here /TODO","title":"Break Glass Accounts"},{"location":"architectures/pbmm/auth/#control-plane-access-via-aws-sso","text":"The vast majority of end-users of the AWS cloud within the Organization will never use or interact with the Organization Management (root) account, or indeed the root users of any child account in the Organization. The AWS Secure Environment Architecture recommends instead that AWS SSO be provisioned in the Organization Management (root) account (a rare case where Organization Management (root) account deployment is mandated). Users will login to AWS via the web-based endpoint for the AWS SSO service: Via an AWS Directory Connector deployed in the Organization Management (root) account, AWS SSO will authenticate the user based on the underlying Microsoft AD installation (in the Central account). Based on group membership, the user will be presented with a set of roles to assume into those accounts. For example, a developer may be placed into groups that permit Admin access in the Dev account and Readonly access in Test ; meanwhile an IT Director may have high-privilege access to most, or all, accounts. In effect, AWS SSO adds SAML IdP capabilities to the AWS Managed Microsoft AD, with the AWS Console acting as a service-provider (SP) in SAML parlance. Other SAML-aware SPs may also be used with AWS SSO.","title":"Control Plane Access via AWS SSO"},{"location":"architectures/pbmm/auth/#sso-user-roles","text":"AWS SSO creates an identity provider (IdP) in each account in the Organization. The roles used by end users have a trust policy to this IdP. When a user authenticates to AWS SSO (via the underlying AD Connector) and selects a role to assume based on their group membership, the SSO service provides the user with temporary security credentials unique to the role session. In such a scenario, the user has no long-term credentials (e.g. password, or access keys) and instead uses their temporary security credentials. Users, via their AD group membership, are ultimately assigned to SSO User Roles via the use of AWS SSO Permission Sets. A permission set is an assignment of a particular permission policy to a set of accounts. For example: An organization might decide to use AWS Managed Policies for Job Functions that are located within the SSO service as the baseline for role-based-access-control (RBAC) separation within an AWS account. This enables job function policies such as: Administrator - This policy grants almost all actions for all AWS services and for all resources in the account. Developer Power User - This user performs application development tasks and can create and configure resources and services that support AWS aware application development. Database Administrator - This policy grants permissions to create, configure, and maintain databases. It includes access to AWS database services, such as Amazon DynamoDB, Amazon Relational Database Service (RDS), and Amazon Redshift. View-Only User - This policy grants List* , Describe* , Get* , View* , and Lookup* access to resources for most AWS services.","title":"SSO User Roles"},{"location":"architectures/pbmm/auth/#principal-authorization","text":"Having assumed a role, a user\u2019s permission-level within an AWS account with respect to any API operation is governed by the IAM policy evaluation logic flow ( detailed here ): Having an Allow to a particular API operation from the Role (i.e. Session Policy) does not necessarily imply that API operation will succeed. As depicted above, Deny may result due to another evaluation stage in the logic; for example a restrictive permission boundary or an explicit Deny at the Resource or SCP (account) level. SCPs are used extensively as a guardrailing mechanism in the AWS Secure Environment Architecture , and are discussed in a later section.","title":"Principal Authorization"},{"location":"architectures/pbmm/auth/#root-authorization","text":"Root credentials for individual accounts in an AWS organization may be created on demand via a password reset process on the unique account email address; however, the AWS Secure Environment Architecture specifically denies this via SCP. Root credentials authorize all actions for all AWS services and for all resources in the account (except anything denied by SCPs). There are some actions which only root has the capability to perform which are found within the AWS online documentation . These are typically rare operations (e.g. creation of X.509 keys), and should not be required in the normal course of business. Any root credentials, if ever they need to be created, should be handled with extreme diligence, with U2F MFA enabled.","title":"Root Authorization"},{"location":"architectures/pbmm/auth/#service-roles","text":"A service role is an IAM Role that a service assumes to perform actions in an account on the user\u2019s behalf. When a user sets up AWS service environments, the user must define an IAM Role for the service to assume. This service role must include all the permissions that are required for the service to access the AWS resources that it needs. Service roles provide access only within a single account and cannot be used to grant access to services in other accounts. Users can create, modify, and delete a service role from within the IAM service. For example, a user can create a role that allows Amazon Redshift to access an Amazon S3 bucket on the user\u2019s behalf and then load data from that bucket into an Amazon Redshift cluster. In the case of SSO, during the process in which AWS SSO is enabled, the AWS Organizations service grants AWS SSO the necessary permissions to create subsequent IAM Roles.","title":"Service Roles"},{"location":"architectures/pbmm/auth/#service-control-policies","text":"Service Control Policies are a key preventative control recommended by the AWS Secure Environment Architecture . It is crucial to note that SCPs, by themselves, never grant permissions. They are most often used to Deny certain actions at a root, OU, or account level within an AWS Organization. Since Deny always overrides Allow in the IAM policy evaluation logic, SCPs can have a powerful effect on all principals in an account, and can wholesale deny entire categories of actions irrespective of the permission policy attached to the principal itself - even the root user of the account. SCPs follow an inheritance pattern from the root of the Organization: In order for any principal to be able to perform an action A, it is necessary (but not sufficient) that there is an Allow on action A from all levels of the hierarchy down to the account, and no explicit Deny anywhere. This is discussed in further detail in How SCPs Work . The AWS Secure Environment Architecture recommends the following SCPs in the Organization:","title":"Service Control Policies"},{"location":"architectures/pbmm/auth/#pbmm-only","text":"This is a comprehensive policy whose main goal is to provide a PBMM-compliant cloud environment, namely prohibiting any non-centralized networking, and mandating data residency in Canada. It should be attached to all non- Unclass OUs. Policy Statement ID (SID) Description DenyNetworkPBMMONLY Prevents the creation of any networking infrastructure in the workload accounts such as VPCs, NATs, VPC peers, etc. DenyAllOutsideCanadaPBMMONLY Prevents the use of any service in any non-Canadian AWS region with the exception of services that are considered global; e.g. CloudFront, IAM, STS, etc ScopeSpecificGlobalActionsToCanadaUSE1 Within services that are exempted from DenyAllOutsideCanadaPBMMONLY , scope the use of those services to the us-east-1 region","title":"PBMM Only"},{"location":"architectures/pbmm/auth/#pbmm-unclass-only","text":"This is broadly similar to PBMM Only ; however it relaxes the requirement for Canadian region usage, and does not prohibit network infrastructure creation (e.g. VPCs, IGWs). This is appropriate for OUs in which AWS service experimentation is taking place. Policy Statement ID (SID) Description DenyUnclass Prevents the deletion of KMS encryption keys and IAM password policies DenyAllOutsideCanadaUS Prevents the use of any service in any region that is not ca-central-1 or us-east-1 , with the exception of services that are considered global; e.g. CloudFront, IAM, STS, etc","title":"PBMM Unclass Only"},{"location":"architectures/pbmm/auth/#pbmm-guardrails-parts-1-and-2","text":"PBMM Guardrails apply across the Organization. These guardrails protect key infrastructure, mandate encryption at rest, and prevent other non-PBMM configurations. Note that this guardrail is split into two parts due to a current limitation of SCP sizing, but logically it should be considered a single policy. Policy Statement ID (SID) Description DenyTag1 Prevents modification of any protected security group DenyTag2 Prevents modification of any protected IAM resource DenyS3 Prevents modification of any S3 bucket used for Accelerator purposes ProtectCloudFormation Prevents modification of any CloudFormation stack used for Accelerator tool purposes DenyAlarmDeletion Prevents modification of any cloudwatch alarm used to alert on significant control plane events ProtectKeyRoles Prevents any IAM operation on Accelerator tool IAM roles DenySSMDel Prevents modification of any ssm resource used for Accelerator tool purposes DenyLogDel Prevents the deletion of any log resource in Cloudwatch Logs DenyLeaveOrg Prevents an account from leaving the Organization DenyLambdaDel Prevents the modification of any guardrail Lambda function BlockOther Prevents miscellaneous operations; e.g. Deny ds:DisableSso BlockMarketplacePMP Prevents the modification or creation of a cloud private marketplace DenyRoot Prevents the use of the root user in an account EnforceEbsEncryption Enforces the use of volume level encryption in running instances EnforceEBSVolumeEncryption Enforces the use of volume level encryption with EBS EnforceRdsEncryption Enforces the use of RDS encryption EnforceAuroraEncryption Enforces the use of Aurora encryption DenyRDGWRole Prevents the modification of a role used for Remote Desktop Gateway DenyGDSHFMAAChange Prevents the modification of GuardDuty & Security Hub","title":"PBMM Guardrails (Parts 1 and 2)"},{"location":"architectures/pbmm/auth/#quarantine-deny-all","text":"This policy can be attached to an account to 'quarantine' it - to prevent any AWS operation from taking place. This is useful in the case of an account with credentials which are believed to have been compromised. Policy Statement ID (SID) Description DenyAllAWSServicesExceptBreakglassRoles Blanket denial on all AWS control plane operations for all non-break-glass roles","title":"Quarantine Deny All"},{"location":"architectures/pbmm/auth/#quarantine-new-object","text":"This policy is applied to new accounts upon creation. After the installation of guardrails, it is removed. In the meantime, it prevents all AWS control plane operations except by principals required to deploy guardrails. Policy Statement ID (SID) Description DenyAllAWSServicesExceptBreakglassRoles Blanket denial on all AWS control plane operations for all non-break-glass roles","title":"Quarantine New Object"},{"location":"architectures/pbmm/diagrams/","text":"Prescriptive Sample Architecture Diagrams \u00b6 Shared VPC Architecture \u00b6 Spoke VPC Architecture \u00b6 VPC and Security Group Patterns \u00b6 Additional Perimeter Patterns \u00b6","title":"Architecture Diagrams"},{"location":"architectures/pbmm/diagrams/#prescriptive-sample-architecture-diagrams","text":"","title":"Prescriptive Sample Architecture Diagrams"},{"location":"architectures/pbmm/diagrams/#shared-vpc-architecture","text":"","title":"Shared VPC Architecture"},{"location":"architectures/pbmm/diagrams/#spoke-vpc-architecture","text":"","title":"Spoke VPC Architecture"},{"location":"architectures/pbmm/diagrams/#vpc-and-security-group-patterns","text":"","title":"VPC and Security Group Patterns"},{"location":"architectures/pbmm/diagrams/#additional-perimeter-patterns","text":"","title":"Additional Perimeter Patterns"},{"location":"architectures/pbmm/log-file-locations/","text":"AWS SEA Central Logging Structures \u00b6 Accelerator Central Logging Buckets: \u00b6 Bucket Type Bucket Name Purpose AES Encrypted Bucket pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4cdwuxu ALB Logs - ALB's do not support logging to a KMS bucket KMS Encrypted Bucket pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo All other AWS Accelerator initiated logs AES or KMS Encrypted aws-controltower-logs-123456789012-ca-central-1 All Control Tower initiated logs AES or KMS Encrypted aws-controltower-s3-access-logs-123456789012-ca-central-1 S3 Access logs for the Control Tower logs bucket Notes: \u00b6 Every customer has two Accelerator logging buckets Control Tower installations have an additional two Control Tower logging buckets Customers could use any account name for their central logging account Bucket name format is: {Accel-Prefix}-{Account-Name}-{Accel-Phase}-xxx{Region}-{Random} {Accel-Prefix} defaults to 'asea' (previously 'pbmmaccel' for Canada) {Accel-Phase} should always be 'phase0' {region} should always be 'cacentral1' for Canada {account} is likely to be 'log-archive' xxx is either \"aes\" or \"\" (nothing) Accelerator Bucket Folders: \u00b6 Log Type Folder Path Example ELB (in AES bucket) {account#}/elb-{elbname}/AWSLogs/{account#}/* s3://pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4ucwuxu/123456789012/elb-Core-mydevacct1-alb/AWSLogs/123456789012/ELBAccessLogTestFile s3://pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4ucwuxu/123456789013/elb-Public-Prod-perimeter-alb/AWSLogs/123456789013/ELBAccessLogTestFile VPC Flow Logs {account#}/{vpc-name}/AWSLogs/{account#}/vpcflowlogs/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789012/Test-East-lcl/AWSLogs/123456789012/vpcflowlogs/us-east-1/2020/08/31/123456789012_vpcflowlogs_us-east-1_fl-04af3543c74402594_20200831T1720Z_73d3922a.log.gz Macie Reports {account#}/macietestobject s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789014/macie-test-object Cost and Usage Reports {account#}/cur/Cost-and-Usage-Report/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789015/cur/Cost-and-Usage-Report/* Config History* AWSLogs/{account#}/Config/{region}/{year}/{month}/{day}/ConfigHistory/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789016/Config/ca-central-1/2020/8/31/ConfigHistory/123456789016_Config_ca-central-1_ConfigHistory_AWS::CloudFormation::Stack_20200831T011226Z_20200831T025845Z_1.json.gz Config Snapshot* AWSLogs/{account#}/Config/{region}/{year}/{month}/{day}/ConfigSnapshot/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789016/Config/ca-central-1/2020/8/30/ConfigSnapshot/123456789016_Config_ca-central-1_ConfigSnapshot_20200830T193058Z_5d173149-e6d0-41e4-af7f-031ff736f8c8.json.gz GuardDuty AWSLogs/{account#}/GuardDuty/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789014/GuardDuty/ca-central-1/2020/09/02/294c9171-4867-3774-9756-f6f6c209616f.jsonl.gz CloudWatch Logs CloudWatchLogs/{year}/{month}/{day}/{hour}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/CloudWatchLogs/2020/08/30/00/PBMMAccel-Kinesis-Delivery-Stream-1-2020-08-30-00-53-33-35aeea4c-582a-444b-8afa-848567924094 CloudTrail Digest*** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail-Digest/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789016/CloudTrail-Digest/ca-central-1/2020/08/30/123456789016_CloudTrail-Digest_ca-central-1_PBMMAccel-Org-Trail_ca-central-1_20200830T190938Z.json.gz CloudTrail Insights** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail-Insights/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789015/CloudTrail-Insight/ca-central-1/2020/09/23/123456789015_CloudTrail-Insight_ca-central-1_20200923T0516Z_KL5e9VCV2SS7IqzB.json.gz CloudTrail*** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789016/CloudTrail/ca-central-1/2020/08/30/123456789016_CloudTrail_ca-central-1_20200830T0115Z_3YQJxwt5qUaOzMtL.json.gz CT S3 Access Logs {no folders} s3://aws-controltower-s3-access-logs-123456789012-ca-central-1/2021-04-26-18-11-21-8647E1080048E5CB Notes: \u00b6 * Located in Control Tower bucket when installed, Control Tower adds the {org-id} (i.e. o-h9ho05hcxl/) as the top level folder ** Only available in Accelerator Standalone deployments *** CloudTrail control plane logs located in Control Tower bucket when installed, Control Tower drops the {org-id} (i.e. o-h9ho05hcxl/) from the middle of the folder path. This may change when Control Tower migrates to Organization Trails. CloudTrail data plane logs remain in the Accelerator bucket. Account number is sometimes duplicated in path because logs replicated from another account always need to start with the source account number Macie reports will only appear in the {account#} for the central security account, and only if a customer schedules PII discovery reports All CloudWatch Logs from all accounts are mixed in the same folder, the embedded log format contains the source account information as documented here: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/ValidateLogEventFlow.html With the exception of CloudWatch Logs, all logs are in the original format provided by the log source/service. ...Return to Accelerator Table of Contents","title":"AWS SEA Central Logging Structures"},{"location":"architectures/pbmm/log-file-locations/#aws-sea-central-logging-structures","text":"","title":"AWS SEA Central Logging Structures"},{"location":"architectures/pbmm/log-file-locations/#accelerator-central-logging-buckets","text":"Bucket Type Bucket Name Purpose AES Encrypted Bucket pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4cdwuxu ALB Logs - ALB's do not support logging to a KMS bucket KMS Encrypted Bucket pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo All other AWS Accelerator initiated logs AES or KMS Encrypted aws-controltower-logs-123456789012-ca-central-1 All Control Tower initiated logs AES or KMS Encrypted aws-controltower-s3-access-logs-123456789012-ca-central-1 S3 Access logs for the Control Tower logs bucket","title":"Accelerator Central Logging Buckets:"},{"location":"architectures/pbmm/log-file-locations/#notes","text":"Every customer has two Accelerator logging buckets Control Tower installations have an additional two Control Tower logging buckets Customers could use any account name for their central logging account Bucket name format is: {Accel-Prefix}-{Account-Name}-{Accel-Phase}-xxx{Region}-{Random} {Accel-Prefix} defaults to 'asea' (previously 'pbmmaccel' for Canada) {Accel-Phase} should always be 'phase0' {region} should always be 'cacentral1' for Canada {account} is likely to be 'log-archive' xxx is either \"aes\" or \"\" (nothing)","title":"Notes:"},{"location":"architectures/pbmm/log-file-locations/#accelerator-bucket-folders","text":"Log Type Folder Path Example ELB (in AES bucket) {account#}/elb-{elbname}/AWSLogs/{account#}/* s3://pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4ucwuxu/123456789012/elb-Core-mydevacct1-alb/AWSLogs/123456789012/ELBAccessLogTestFile s3://pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4ucwuxu/123456789013/elb-Public-Prod-perimeter-alb/AWSLogs/123456789013/ELBAccessLogTestFile VPC Flow Logs {account#}/{vpc-name}/AWSLogs/{account#}/vpcflowlogs/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789012/Test-East-lcl/AWSLogs/123456789012/vpcflowlogs/us-east-1/2020/08/31/123456789012_vpcflowlogs_us-east-1_fl-04af3543c74402594_20200831T1720Z_73d3922a.log.gz Macie Reports {account#}/macietestobject s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789014/macie-test-object Cost and Usage Reports {account#}/cur/Cost-and-Usage-Report/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789015/cur/Cost-and-Usage-Report/* Config History* AWSLogs/{account#}/Config/{region}/{year}/{month}/{day}/ConfigHistory/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789016/Config/ca-central-1/2020/8/31/ConfigHistory/123456789016_Config_ca-central-1_ConfigHistory_AWS::CloudFormation::Stack_20200831T011226Z_20200831T025845Z_1.json.gz Config Snapshot* AWSLogs/{account#}/Config/{region}/{year}/{month}/{day}/ConfigSnapshot/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789016/Config/ca-central-1/2020/8/30/ConfigSnapshot/123456789016_Config_ca-central-1_ConfigSnapshot_20200830T193058Z_5d173149-e6d0-41e4-af7f-031ff736f8c8.json.gz GuardDuty AWSLogs/{account#}/GuardDuty/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789014/GuardDuty/ca-central-1/2020/09/02/294c9171-4867-3774-9756-f6f6c209616f.jsonl.gz CloudWatch Logs CloudWatchLogs/{year}/{month}/{day}/{hour}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/CloudWatchLogs/2020/08/30/00/PBMMAccel-Kinesis-Delivery-Stream-1-2020-08-30-00-53-33-35aeea4c-582a-444b-8afa-848567924094 CloudTrail Digest*** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail-Digest/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789016/CloudTrail-Digest/ca-central-1/2020/08/30/123456789016_CloudTrail-Digest_ca-central-1_PBMMAccel-Org-Trail_ca-central-1_20200830T190938Z.json.gz CloudTrail Insights** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail-Insights/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789015/CloudTrail-Insight/ca-central-1/2020/09/23/123456789015_CloudTrail-Insight_ca-central-1_20200923T0516Z_KL5e9VCV2SS7IqzB.json.gz CloudTrail*** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail/{region}/{year}/{month}/{day}/* s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789016/CloudTrail/ca-central-1/2020/08/30/123456789016_CloudTrail_ca-central-1_20200830T0115Z_3YQJxwt5qUaOzMtL.json.gz CT S3 Access Logs {no folders} s3://aws-controltower-s3-access-logs-123456789012-ca-central-1/2021-04-26-18-11-21-8647E1080048E5CB","title":"Accelerator Bucket Folders:"},{"location":"architectures/pbmm/log-file-locations/#notes_1","text":"* Located in Control Tower bucket when installed, Control Tower adds the {org-id} (i.e. o-h9ho05hcxl/) as the top level folder ** Only available in Accelerator Standalone deployments *** CloudTrail control plane logs located in Control Tower bucket when installed, Control Tower drops the {org-id} (i.e. o-h9ho05hcxl/) from the middle of the folder path. This may change when Control Tower migrates to Organization Trails. CloudTrail data plane logs remain in the Accelerator bucket. Account number is sometimes duplicated in path because logs replicated from another account always need to start with the source account number Macie reports will only appear in the {account#} for the central security account, and only if a customer schedules PII discovery reports All CloudWatch Logs from all accounts are mixed in the same folder, the embedded log format contains the source account information as documented here: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/ValidateLogEventFlow.html With the exception of CloudWatch Logs, all logs are in the original format provided by the log source/service. ...Return to Accelerator Table of Contents","title":"Notes:"},{"location":"architectures/pbmm/logging/","text":"Logging and Monitoring \u00b6 The AWS Secure Environment Architecture recommends the following detective controls across the Organization. These controls, taken together, provide a comprehensive picture of the full set of control plane and data plane operations across the set of accounts. CloudTrail \u00b6 A CloudTrail Organizational trail should be deployed into the Organization. For each account, this captures management events and optionally S3 data plane events taking place by every principal in the account. These records are sent to an S3 bucket in the log archive account, and the trail itself cannot be modified or deleted by any principal in any child account. This provides an audit trail for detective purposes in the event of the need for forensic analysis into account usage. The logs themselves provide an integrity guarantee: every hour, CloudTrail produces a digest of that hour's logs files, and signs with its own private key. The authenticity of the logs may be verified using the corresponding public key. This process is detailed here . VPC Flow Logs \u00b6 VPC Flow Logs capture information about the IP traffic going to and from network interfaces in an AWS Account VPC such as source and destination IPs, protocol, ports, and success/failure of the flow. The AWS Secure Environment Architecture recommends enabling ALL (i.e. both accepted and rejected traffic) logs for all VPCs in the Shared Network account with an S3 destination in the log-archive account. More details about VPC Flow Logs are available here . Note that certain categories of network flows are not captured, including traffic to and from Traffic to and from 169.254.169.254 for instance metadata, and DNS traffic with an Amazon VPC resolver. GuardDuty \u00b6 Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. GuardDuty uses a number of data sources including VPC Flow Logs and CloudTrail logs. The AWS Secure Environment Architecture recommends enabling GuardDuty at the Organization level , and delegating the security account as the GuardDuty Administrative account. The GuardDuty Administrative account should be auto-enabled to add new accounts as they come online. Note that this should be done in every region as a defense in depth measure, with the understanding that the PBMM SCP will prevent service usage in all other regions. Config \u00b6 AWS Config provides a detailed view of the resources associated with each account in the AWS Organization, including how they are configured, how they are related to one another, and how the configurations have changed on a recurring basis. Resources can be evaluated on the basis of their compliance with Config Rules - for example, a Config Rule might continually examine EBS volumes and check that they are encrypted. Config may be enabled at the Organization level - this provides an overall view of the compliance status of all resources across the Organization. Note: At the time of writing, the Config Multi-Account Multi-Region Data Aggregation sits in the Organization Management (root) account. The AWS Secure Environment Architecture will recommend that this be situated in the security account, once that becomes easily-configurable in Organizations. Cloudwatch Logs \u00b6 CloudWatch Logs is AWS' logging aggregator service, used to monitor, store, and access log files from EC2 instances, AWS CloudTrail, Route 53, and other sources. The AWS Secure Environment Architecture recommends that log subscriptions are created for all log groups in all workload accounts, and streamed into S3 in the log-archive account (via Kinesis) for analysis and long-term audit purposes. SecurityHub \u00b6 The primary dashboard for Operators to assess the security posture of the AWS footprint is the centralized AWS Security Hub service. Security Hub should be configured to aggregate findings from Amazon GuardDuty, AWS Config and IAM Access Analyzers. Events from security integrations are correlated and displayed on the Security Hub dashboard as 'findings' with a severity level (informational, low, medium, high, critical). The AWS Secure Environment Architecture recommends that certain Security Hub frameworks be enabled, specifically: AWS Foundational Security Best Practices v1.0.0 PCI DSS v3.2.1 CIS AWS Foundations Benchmark v1.2.0 These frameworks will perform checks against the accounts via Config Rules that are evaluated against the AWS Config resources in scope. See the above links for a definition of the associated controls. Systems Manager Session Manager \u00b6 Session Manager is a fully managed AWS Systems Manager capability that lets you manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, on-premises instances, and virtual machines (VMs) through an interactive one-click browser-based shell or through the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also makes it easy to comply with corporate policies that require controlled access to instances, strict security practices, and fully auditable logs with instance access details, while still providing end users with simple one-click cross-platform access to your managed instances. 1 The AWS Secure Environment Architecture recommends that you choose to store session log data in a centralized S3 bucket for auditing purposes and encrypt with Key Management Service (KMS). In addition, session log data should also be configured to be sent to Amazon CloudWatch Logs with KMS encryption using your AWS KMS key.","title":"Logging and Monitoring"},{"location":"architectures/pbmm/logging/#logging-and-monitoring","text":"The AWS Secure Environment Architecture recommends the following detective controls across the Organization. These controls, taken together, provide a comprehensive picture of the full set of control plane and data plane operations across the set of accounts.","title":"Logging and Monitoring"},{"location":"architectures/pbmm/logging/#cloudtrail","text":"A CloudTrail Organizational trail should be deployed into the Organization. For each account, this captures management events and optionally S3 data plane events taking place by every principal in the account. These records are sent to an S3 bucket in the log archive account, and the trail itself cannot be modified or deleted by any principal in any child account. This provides an audit trail for detective purposes in the event of the need for forensic analysis into account usage. The logs themselves provide an integrity guarantee: every hour, CloudTrail produces a digest of that hour's logs files, and signs with its own private key. The authenticity of the logs may be verified using the corresponding public key. This process is detailed here .","title":"CloudTrail"},{"location":"architectures/pbmm/logging/#vpc-flow-logs","text":"VPC Flow Logs capture information about the IP traffic going to and from network interfaces in an AWS Account VPC such as source and destination IPs, protocol, ports, and success/failure of the flow. The AWS Secure Environment Architecture recommends enabling ALL (i.e. both accepted and rejected traffic) logs for all VPCs in the Shared Network account with an S3 destination in the log-archive account. More details about VPC Flow Logs are available here . Note that certain categories of network flows are not captured, including traffic to and from Traffic to and from 169.254.169.254 for instance metadata, and DNS traffic with an Amazon VPC resolver.","title":"VPC Flow Logs"},{"location":"architectures/pbmm/logging/#guardduty","text":"Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. GuardDuty uses a number of data sources including VPC Flow Logs and CloudTrail logs. The AWS Secure Environment Architecture recommends enabling GuardDuty at the Organization level , and delegating the security account as the GuardDuty Administrative account. The GuardDuty Administrative account should be auto-enabled to add new accounts as they come online. Note that this should be done in every region as a defense in depth measure, with the understanding that the PBMM SCP will prevent service usage in all other regions.","title":"GuardDuty"},{"location":"architectures/pbmm/logging/#config","text":"AWS Config provides a detailed view of the resources associated with each account in the AWS Organization, including how they are configured, how they are related to one another, and how the configurations have changed on a recurring basis. Resources can be evaluated on the basis of their compliance with Config Rules - for example, a Config Rule might continually examine EBS volumes and check that they are encrypted. Config may be enabled at the Organization level - this provides an overall view of the compliance status of all resources across the Organization. Note: At the time of writing, the Config Multi-Account Multi-Region Data Aggregation sits in the Organization Management (root) account. The AWS Secure Environment Architecture will recommend that this be situated in the security account, once that becomes easily-configurable in Organizations.","title":"Config"},{"location":"architectures/pbmm/logging/#cloudwatch-logs","text":"CloudWatch Logs is AWS' logging aggregator service, used to monitor, store, and access log files from EC2 instances, AWS CloudTrail, Route 53, and other sources. The AWS Secure Environment Architecture recommends that log subscriptions are created for all log groups in all workload accounts, and streamed into S3 in the log-archive account (via Kinesis) for analysis and long-term audit purposes.","title":"Cloudwatch Logs"},{"location":"architectures/pbmm/logging/#securityhub","text":"The primary dashboard for Operators to assess the security posture of the AWS footprint is the centralized AWS Security Hub service. Security Hub should be configured to aggregate findings from Amazon GuardDuty, AWS Config and IAM Access Analyzers. Events from security integrations are correlated and displayed on the Security Hub dashboard as 'findings' with a severity level (informational, low, medium, high, critical). The AWS Secure Environment Architecture recommends that certain Security Hub frameworks be enabled, specifically: AWS Foundational Security Best Practices v1.0.0 PCI DSS v3.2.1 CIS AWS Foundations Benchmark v1.2.0 These frameworks will perform checks against the accounts via Config Rules that are evaluated against the AWS Config resources in scope. See the above links for a definition of the associated controls.","title":"SecurityHub"},{"location":"architectures/pbmm/logging/#systems-manager-session-manager","text":"Session Manager is a fully managed AWS Systems Manager capability that lets you manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, on-premises instances, and virtual machines (VMs) through an interactive one-click browser-based shell or through the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also makes it easy to comply with corporate policies that require controlled access to instances, strict security practices, and fully auditable logs with instance access details, while still providing end users with simple one-click cross-platform access to your managed instances. 1 The AWS Secure Environment Architecture recommends that you choose to store session log data in a centralized S3 bucket for auditing purposes and encrypt with Key Management Service (KMS). In addition, session log data should also be configured to be sent to Amazon CloudWatch Logs with KMS encryption using your AWS KMS key.","title":"Systems Manager Session Manager"},{"location":"architectures/pbmm/network/","text":"Networking \u00b6 Overview \u00b6 The AWS Secure Environment Architecture networking is built on a principle of centralized on-premises and Internet ingress/egress, while enforcing data plane isolation between workloads in different environments. Connectivity to on-prem environments, internet egress, shared resources and AWS APIs are mediated at a central point of ingress/egress via the use of a Transit Gateway . Consider the following overall network diagram: All functional accounts use RAM-shared networking infrastructure as depicted above. The workload VPCs (Dev, Test, Prod, etc) are hosted in the Shared Network account and made available to the appropriate OU in the Organization. Perimeter \u00b6 The perimeter VPC hosts the Organization's perimeter security services. The Perimeter VPC is used to control the flow of traffic between AWS Accounts and external networks for IaaS workloads: both public (internet) and in some cases private (access to on-premises datacenters). This VPC hosts Next Generation Firewalls (NGFW) that provide perimeter security services including virus scanning / malware protection, Intrusion Protection services, TLS Inspection and Web Application Firewall protection. If applicable, this VPC also hosts reverse proxy servers. Note that this VPC is in its own isolated account, separate from Shared Network, in order to facilitate networking and security 'separation of duties'. Internal networking teams may administer the cloud networks in Shared Network without being granted permission to administer the security perimeter itself. IP Ranges \u00b6 Primary Range : The AWS Secure Environment Architecture recommends that the perimeter VPC have a primary range in the RFC1918 block (e.g. 10.7.4.0/22 ), used only for subnets dedicated to 'detonation' purposes. This primary range, in an otherwise-unused RFC1918 range, is not intended to be routable outside of the VPC, and is reserved for future use with malware detonation capabilities of NGFW devices. Secondary Range : This VPC should also have a secondary range in the RFC6598 block (e.g. 100.96.250.0/23 ) used for the overlay network (NGFW devices inside VPN tunnel) for all other subnets. This secondary range is assigned by an external entity (e.g. Shared Services Canada), and should be carefully selected in order to co-exist with AWS Secure Environment Architecture deployments that exist at peer organizations; for instance other government departments that maintain a relationship with the same shared entity in a carrier-grade NAT topology. Although this is a 'secondary' range in VPC parlance, this VPC CIDR should be interpreted as the more 'significant' of the two with respect to Transit Gateway routing; the Transit Gateway will only ever interact with this 'secondary' range. This VPC has four subnets per AZ, each of which hosts a port used by the NGFW devices, which are deployed in an HA pair. The purpose of these subnets is as follows. Detonation : This is an unused subnet reserved for future use with malware detonation capabilities of the NGFW devices. e.g. 10.7.4.0/24 - not routable except local. Proxy : This subnet hosts reverse proxy services for web and other protocols. It also contains the three interface endpoints necessary for AWS Systems Manager Session Manager, which enables SSH-less CLI access to authorized and authenticated principals in the perimeter account. e.g. 100.96.251.64/26 On-Premises : This subnet hosts the private interfaces of the firewalls, corresponding to connections from the on-premises network. e.g. 100.96.250.192/26 FW-Management : This subnet is used to host management tools and the management of the Firewalls itself. e.g. 100.96.251.160/27 - a smaller subnet is permissible due to modest IP requirements for management instances. Public : This subnet is the public-access zone for the perimeter VPC. It hosts the public interface of the firewalls, as well as application load balancers that are used to balance traffic across the firewall pair. There is one Elastic IPv4 address per public subnet that corresponds to the IPSec Customer Gateway (CGW) for the VPN connections into the Transit Gateway in Shared Networking. e.g. 100.96.250.0/26 Outbound internet connections (for software updates, etc.) can be initiated from within the workload VPCs, and use the transparent proxy feature of the next-gen Firewalls. Note on VPN Tunnel Redundancy : Each NGFW device manifests as a unique CGW on the AWS side (shared network account) of the IPSec VPNs. Moreover, there are two Site-to-Site VPNs in this architecture, each with one active tunnel (and one inactive tunnel); taken together, the pair is redundant. In many hybrid networking configurations, a single Site-to-Site VPN resource is used with dual active tunnels. Customers may receive the following email notification from the AWS VPC service team: You're receiving this message because you have at least one VPN Connection in the ca-central-1 Region, for which your VPN Customer Gateway is not using both tunnels. This mode of operation is not recommended as you may experience connectivity issues if your active tunnel fails. This message may be disregarded, as it is premised on a traditional hybrid configuration with dual tunnels. Customers may create a VPN support case (in shared network account) requesting that these informational emails are disabled. Shared Network \u00b6 The shared network account, and the AWS networking resources therein, form the core of the cloud networking infrastructure across the account structure. Rather than the individual accounts defining their own networks, these are instead centralized here and shared out to the relevant OUs. Principals in a Dev OU will have access to a Dev VPC, Test OU will have access to a Test VPC and so on - all of which are owned by this account. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with AWS Resource Access Manager (RAM). The RAM service eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account. Transit Gateway \u00b6 The Transit Gateway is a central hub that performs several core functions within the Shared Network account. Routing of permitted flows; for example a Workload to On-premises via the Perimeter VPC. All routing tables in SharedNetwork VPCs send 0.0.0.0/0 traffic to the TGW, where its handling will be determined by the TGW Route Table (TGW-RT) that its attachment is associated with. For example: an HTTP request to registry.hub.docker.com from the Test VPC will go to the TGW The Segregated TGW RT will direct that traffic to the Perimeter VPC via the IPsec VPNs The request will be proxied to the internet, via GC-CAP if appropriate The return traffic will again transit the IPsec VPNs The 10.3.0.0/16 bound response traffic will arrive at the Core TGW RT, where a propagation in that TGW RT will direct the response back to the Test VPC. Defining separate routing domains that prohibit undesired east-west flows at the network level; for example, by prohibiting Dev to Prod traffic. For example: All routing tables in SharedNetwork VPCs send 0.0.0.0/0 traffic to the TGW, which defines where the next permissible hop is. For example, 10.2.0.0/16 Dev traffic destined for the 10.0.4.0/16 Prod VPC will be blocked by the blackhole route in the Segregated TGW RT. Enabling centralization of shared resources; namely a shared Microsoft AD installation in the Central VPC, and access to shared VPC Endpoints in the Endpoint VPC. The Central VPC, and the Endpoint VPC are routable from Workload VPCs. This provides an economical way to share Organization wide resources that are nonetheless isolated into their own VPCs. For example: a git request in the Dev VPC to git.private-domain.ca resolves to a 10.1.0.0/16 address in the Central VPC. The request from the Dev VPC will go to the TGW due to the VPC routing table associated with that subnet The TGW will send the request to the Central VPC via an entry in the Segregated TGW RT The git response will go to the TGW due to the VPC routing table associated with that subnet The Shared TGW RT will direct the response back to the Dev VPC The four TGW RTs exist to serve the following main functions: Segregated TGW RT : Used as the association table for the workload VPCs; prevents east-west traffic, except to shared resources. Core TGW RT : Used for internet/on-premises response traffic, and Endpoint VPC egress. Shared TGW RT : Used to provide Central VPC access east-west for the purposes of response traffic to shared workloads Standalone TGW RT : Reserved for future use. Prevents TGW routing except to the Endpoint VPC. Note that a unique BGP ASN will need to be available for the TGW. Endpoint VPC \u00b6 DNS functionality for the network architecture is centralized in the Endpoint VPC. It is recommended that the Endpoint VPC use a RFC1918 range - e.g. 10.7.0.0/22 with sufficient capacity to support 60+ AWS services and future endpoint expansion, and inbound and outbound resolvers (all figures per AZ). Endpoint VPC: Interface Endpoints \u00b6 The endpoint VPC hosts VPC Interface Endpoints (VPCEs) and associated Route 53 private hosted zones for all applicable services in the ca-central-1 region. This permits traffic destined for an eligible AWS service; for example SQS, to remain entirely within the SharedNetwork account rather than transiting via the IPv4 public endpoint for the service: From within an associated workload VPC such as Dev , the service endpoint (e.g. sqs.ca-central-1.amazonaws.com ) will resolve to an IP in the Endpoint VPC: sh-4.2$ nslookup sqs.ca-central-1.amazonaws.com Server: 10.2.0.2 # Dev VPC's .2 resolver. Address: 10.2.0.2#53 Non-authoritative answer: Name: sqs.ca-central-1.amazonaws.com Address: 10.7.1.190 # IP in Endpoint VPC - AZ-a. Name: sqs.ca-central-1.amazonaws.com Address: 10.7.0.135 # IP in Endpoint VPC - AZ-b. This cross-VPC resolution of the service-specific private hosted zone functions via the association of each VPC to each private hosted zone, as depicted above. Endpoint VPC: Hybrid DNS \u00b6 The Endpoint VPC also hosts the common DNS infrastructure used to resolve DNS queries: within the cloud from the cloud to on-premises from on-premises to the cloud Within The Cloud \u00b6 In-cloud DNS resolution applies beyond the DNS infrastructure that is put in place to support the Interface Endpoints for the AWS services in-region. Other DNS zones, associated with the Endpoint VPC, are resolvable the same way via an association to workload VPCs. From Cloud to On-Premises \u00b6 DNS Resolution from the cloud to on-premises is handled via the use of a Route 53 Outbound Endpoint, deployed in the Endpoint VPC, with an associated Resolver rule that forwards DNS traffic to the outbound endpoint. Each VPC is associated to this rule. From On-Premises to Cloud \u00b6 Conditional forwarding from on-premises networks is made possible via the use of a Route 53 Inbound Endpoint. On-prem networks send resolution requests for relevant domains to the endpoints deployed in the Endpoint VPC: Workload VPCs \u00b6 The workload VPCs are where line of business applications ultimately reside, segmented by environment ( Dev , Test , Prod , etc). It is recommended that the Workload VPC use a RFC1918 range (e.g. 10.2.0.0/16 for Dev , 10.3.0.0/16 for Test , etc). Note that security groups are recommended as the primary data-plane isolation mechanism between applications that may coexist in the same VPC. It is anticipated that unrelated applications would coexist in their respective tiers without ever permitting east-west traffic flows. The following subnets are defined by the AWS Secure Environment Architecture : TGW subnet : This subnet hosts the elastic-network interfaces for the TGW attachment. A /27 subnet is sufficient. Web subnet : This subnet hosts front-end or otherwise 'client' facing infrastructure. A /20 or larger subnet is recommended to facilitate auto-scaling. App subnet : This subnet hosts app-tier code (EC2, containers, etc). A /19 or larger subnet is recommended to facilitate auto-scaling. Data subnet : This subnet hosts data-tier code (RDS instances, ElastiCache instances). A /21 or larger subnet is recommended. Mgmt subnet : This subnet hosts bastion or other management instances. A /21 or larger subnet is recommended. Each subnet is associated with a Common VPC Route Table, as depicted above. Gateway Endpoints for relevant services (Amazon S3, Amazon DynamoDB) are installed in the Common route tables of all Workload VPCs. Aside from local traffic or gateway-bound traffic, 0.0.0.0/0 is always destined for the TGW. Security Groups \u00b6 Security Groups are instance level firewalls, and represent a foundational unit of network segmentation across AWS networking. Security groups are stateful, and support ingress/egress rules based on protocols and source/destinations. While CIDR ranges are supported by the latter, it is preferable to instead use other security groups as source/destinations. This permits a higher level of expressiveness that is not coupled to particular CIDR choices and works well with autoscaling; e.g. \"permit port 3306 traffic from the App tier to the Data tier\" versus \"permit port 3306 traffic from 10.0.1.0/24 to 10.0.2.0/24 . Security group egress rules are often used in 'allow all' mode ( 0.0.0.0/0 ), with the focus primarily being on consistently whitelisting required ingress traffic. This ensures day to day activities like patching, access to DNS, or to directory services access can function on instances without friction. The provided sample security groups in the workload accounts offers a good balance that considers both security, ease of operations, and frictionless development. They allow developers to focus on developing, enabling them to simply use the pre-created security constructs for their workloads, and avoid the creation of wide-open security groups. Developers can equally choose to create more appropriate least-privilege security groups more suitable for their application, if they are skilled in this area. It is expected as an application is promoted through the SDLC cycle from Dev through Test to Prod, these security groups will be further refined by the extended customers teams to further reduce privilege, as appropriate. It is expected that each customer will review and tailor their Security Groups based on their own security requirements. NACLs \u00b6 Network Access-Control Lists (NACLs) are stateless constructs used sparingly as a defense-in-depth measure in this architecture. AWS generally discourages the use of NACLs given the added complexity and management burden, given the availability and ease of use provided by security groups. Each network flow often requires four NACL entries (egress from ephemeral, ingress to destination, egress from destination, ingress to ephemeral). The architecture recommends NACLs as a segmentation mechanism for Data subnets; i.e. DENY all inbound traffic to such a subnet except that which originates in the App subnet for the same VPC. As with security groups, we encourage customers to review and tailor their NACLs based on their own security requirements. Central VPC \u00b6 The Central VPC is a network for localizing operational infrastructure that may be needed across the Organization, such as code repositories, artifact repositories, and notably, the managed Directory Service (Microsoft AD). Instances that are domain joined will connect to this AD domain - a network flow that is made possible from anywhere in the network structure due to the inclusion of the Central VPC in all relevant association TGW RTs. It is recommended that the Central VPC use a RFC1918 range (e.g. 10.1.0.0/16 ) for the purposes of routing from the workload VPCs, and a secondary range from the RFC6598 block (e.g. 100.96.252.0/23 ) to support the Microsoft AD workload. Note that this VPC also contains a peering relationship to the ForSSO VPC in the Organization Management (root) account. This exists purely to support connectivity from an AD-Connector instance in the Organization Management (root) account, which in turn enables AWS SSO for federated login to the AWS control plane. Domain Joining \u00b6 An EC2 instance deployed in the Workload VPCs can join the domain corresponding to the Microsoft AD in Central provided the following conditions are all true: The instance needs a network path to the Central VPC (given by the Segregated TGW RT), and appropriate security group assignment The Microsoft AD should be 'shared' with the account the EC2 instance resides in (The AWS Secure Environment Architecture recommends these directories are shared to workload accounts) The instance has the AWS managed policies AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess attached to its IAM role, or runs under a role with at least the permission policies given by the combination of these two managed policies. The EC2's VPC has an associated resolver rule that directs DNS queries for the AD domain to the Central VPC. Sandbox VPC \u00b6 A sandbox VPC, not depicted, may be included in the AWS Secure Environment Architecture . This is not connected to the Transit Gateway, Perimeter VPC, on-premises network, or other common infrastructure. It contains its own Internet Gateway, and is an entirely separate VPC with respect to the rest of the AWS Secure Environment Architecture . The sandbox VPC should be used exclusively for time-limited experimentation, particularly with out-of-region services, and never used for any line of business workload or data.","title":"Networking"},{"location":"architectures/pbmm/network/#networking","text":"","title":"Networking"},{"location":"architectures/pbmm/network/#overview","text":"The AWS Secure Environment Architecture networking is built on a principle of centralized on-premises and Internet ingress/egress, while enforcing data plane isolation between workloads in different environments. Connectivity to on-prem environments, internet egress, shared resources and AWS APIs are mediated at a central point of ingress/egress via the use of a Transit Gateway . Consider the following overall network diagram: All functional accounts use RAM-shared networking infrastructure as depicted above. The workload VPCs (Dev, Test, Prod, etc) are hosted in the Shared Network account and made available to the appropriate OU in the Organization.","title":"Overview"},{"location":"architectures/pbmm/network/#perimeter","text":"The perimeter VPC hosts the Organization's perimeter security services. The Perimeter VPC is used to control the flow of traffic between AWS Accounts and external networks for IaaS workloads: both public (internet) and in some cases private (access to on-premises datacenters). This VPC hosts Next Generation Firewalls (NGFW) that provide perimeter security services including virus scanning / malware protection, Intrusion Protection services, TLS Inspection and Web Application Firewall protection. If applicable, this VPC also hosts reverse proxy servers. Note that this VPC is in its own isolated account, separate from Shared Network, in order to facilitate networking and security 'separation of duties'. Internal networking teams may administer the cloud networks in Shared Network without being granted permission to administer the security perimeter itself.","title":"Perimeter"},{"location":"architectures/pbmm/network/#ip-ranges","text":"Primary Range : The AWS Secure Environment Architecture recommends that the perimeter VPC have a primary range in the RFC1918 block (e.g. 10.7.4.0/22 ), used only for subnets dedicated to 'detonation' purposes. This primary range, in an otherwise-unused RFC1918 range, is not intended to be routable outside of the VPC, and is reserved for future use with malware detonation capabilities of NGFW devices. Secondary Range : This VPC should also have a secondary range in the RFC6598 block (e.g. 100.96.250.0/23 ) used for the overlay network (NGFW devices inside VPN tunnel) for all other subnets. This secondary range is assigned by an external entity (e.g. Shared Services Canada), and should be carefully selected in order to co-exist with AWS Secure Environment Architecture deployments that exist at peer organizations; for instance other government departments that maintain a relationship with the same shared entity in a carrier-grade NAT topology. Although this is a 'secondary' range in VPC parlance, this VPC CIDR should be interpreted as the more 'significant' of the two with respect to Transit Gateway routing; the Transit Gateway will only ever interact with this 'secondary' range. This VPC has four subnets per AZ, each of which hosts a port used by the NGFW devices, which are deployed in an HA pair. The purpose of these subnets is as follows. Detonation : This is an unused subnet reserved for future use with malware detonation capabilities of the NGFW devices. e.g. 10.7.4.0/24 - not routable except local. Proxy : This subnet hosts reverse proxy services for web and other protocols. It also contains the three interface endpoints necessary for AWS Systems Manager Session Manager, which enables SSH-less CLI access to authorized and authenticated principals in the perimeter account. e.g. 100.96.251.64/26 On-Premises : This subnet hosts the private interfaces of the firewalls, corresponding to connections from the on-premises network. e.g. 100.96.250.192/26 FW-Management : This subnet is used to host management tools and the management of the Firewalls itself. e.g. 100.96.251.160/27 - a smaller subnet is permissible due to modest IP requirements for management instances. Public : This subnet is the public-access zone for the perimeter VPC. It hosts the public interface of the firewalls, as well as application load balancers that are used to balance traffic across the firewall pair. There is one Elastic IPv4 address per public subnet that corresponds to the IPSec Customer Gateway (CGW) for the VPN connections into the Transit Gateway in Shared Networking. e.g. 100.96.250.0/26 Outbound internet connections (for software updates, etc.) can be initiated from within the workload VPCs, and use the transparent proxy feature of the next-gen Firewalls. Note on VPN Tunnel Redundancy : Each NGFW device manifests as a unique CGW on the AWS side (shared network account) of the IPSec VPNs. Moreover, there are two Site-to-Site VPNs in this architecture, each with one active tunnel (and one inactive tunnel); taken together, the pair is redundant. In many hybrid networking configurations, a single Site-to-Site VPN resource is used with dual active tunnels. Customers may receive the following email notification from the AWS VPC service team: You're receiving this message because you have at least one VPN Connection in the ca-central-1 Region, for which your VPN Customer Gateway is not using both tunnels. This mode of operation is not recommended as you may experience connectivity issues if your active tunnel fails. This message may be disregarded, as it is premised on a traditional hybrid configuration with dual tunnels. Customers may create a VPN support case (in shared network account) requesting that these informational emails are disabled.","title":"IP Ranges"},{"location":"architectures/pbmm/network/#shared-network","text":"The shared network account, and the AWS networking resources therein, form the core of the cloud networking infrastructure across the account structure. Rather than the individual accounts defining their own networks, these are instead centralized here and shared out to the relevant OUs. Principals in a Dev OU will have access to a Dev VPC, Test OU will have access to a Test VPC and so on - all of which are owned by this account. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with AWS Resource Access Manager (RAM). The RAM service eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account.","title":"Shared Network"},{"location":"architectures/pbmm/network/#transit-gateway","text":"The Transit Gateway is a central hub that performs several core functions within the Shared Network account. Routing of permitted flows; for example a Workload to On-premises via the Perimeter VPC. All routing tables in SharedNetwork VPCs send 0.0.0.0/0 traffic to the TGW, where its handling will be determined by the TGW Route Table (TGW-RT) that its attachment is associated with. For example: an HTTP request to registry.hub.docker.com from the Test VPC will go to the TGW The Segregated TGW RT will direct that traffic to the Perimeter VPC via the IPsec VPNs The request will be proxied to the internet, via GC-CAP if appropriate The return traffic will again transit the IPsec VPNs The 10.3.0.0/16 bound response traffic will arrive at the Core TGW RT, where a propagation in that TGW RT will direct the response back to the Test VPC. Defining separate routing domains that prohibit undesired east-west flows at the network level; for example, by prohibiting Dev to Prod traffic. For example: All routing tables in SharedNetwork VPCs send 0.0.0.0/0 traffic to the TGW, which defines where the next permissible hop is. For example, 10.2.0.0/16 Dev traffic destined for the 10.0.4.0/16 Prod VPC will be blocked by the blackhole route in the Segregated TGW RT. Enabling centralization of shared resources; namely a shared Microsoft AD installation in the Central VPC, and access to shared VPC Endpoints in the Endpoint VPC. The Central VPC, and the Endpoint VPC are routable from Workload VPCs. This provides an economical way to share Organization wide resources that are nonetheless isolated into their own VPCs. For example: a git request in the Dev VPC to git.private-domain.ca resolves to a 10.1.0.0/16 address in the Central VPC. The request from the Dev VPC will go to the TGW due to the VPC routing table associated with that subnet The TGW will send the request to the Central VPC via an entry in the Segregated TGW RT The git response will go to the TGW due to the VPC routing table associated with that subnet The Shared TGW RT will direct the response back to the Dev VPC The four TGW RTs exist to serve the following main functions: Segregated TGW RT : Used as the association table for the workload VPCs; prevents east-west traffic, except to shared resources. Core TGW RT : Used for internet/on-premises response traffic, and Endpoint VPC egress. Shared TGW RT : Used to provide Central VPC access east-west for the purposes of response traffic to shared workloads Standalone TGW RT : Reserved for future use. Prevents TGW routing except to the Endpoint VPC. Note that a unique BGP ASN will need to be available for the TGW.","title":"Transit Gateway"},{"location":"architectures/pbmm/network/#endpoint-vpc","text":"DNS functionality for the network architecture is centralized in the Endpoint VPC. It is recommended that the Endpoint VPC use a RFC1918 range - e.g. 10.7.0.0/22 with sufficient capacity to support 60+ AWS services and future endpoint expansion, and inbound and outbound resolvers (all figures per AZ).","title":"Endpoint VPC"},{"location":"architectures/pbmm/network/#endpoint-vpc-interface-endpoints","text":"The endpoint VPC hosts VPC Interface Endpoints (VPCEs) and associated Route 53 private hosted zones for all applicable services in the ca-central-1 region. This permits traffic destined for an eligible AWS service; for example SQS, to remain entirely within the SharedNetwork account rather than transiting via the IPv4 public endpoint for the service: From within an associated workload VPC such as Dev , the service endpoint (e.g. sqs.ca-central-1.amazonaws.com ) will resolve to an IP in the Endpoint VPC: sh-4.2$ nslookup sqs.ca-central-1.amazonaws.com Server: 10.2.0.2 # Dev VPC's .2 resolver. Address: 10.2.0.2#53 Non-authoritative answer: Name: sqs.ca-central-1.amazonaws.com Address: 10.7.1.190 # IP in Endpoint VPC - AZ-a. Name: sqs.ca-central-1.amazonaws.com Address: 10.7.0.135 # IP in Endpoint VPC - AZ-b. This cross-VPC resolution of the service-specific private hosted zone functions via the association of each VPC to each private hosted zone, as depicted above.","title":"Endpoint VPC: Interface Endpoints"},{"location":"architectures/pbmm/network/#endpoint-vpc-hybrid-dns","text":"The Endpoint VPC also hosts the common DNS infrastructure used to resolve DNS queries: within the cloud from the cloud to on-premises from on-premises to the cloud","title":"Endpoint VPC: Hybrid DNS"},{"location":"architectures/pbmm/network/#workload-vpcs","text":"The workload VPCs are where line of business applications ultimately reside, segmented by environment ( Dev , Test , Prod , etc). It is recommended that the Workload VPC use a RFC1918 range (e.g. 10.2.0.0/16 for Dev , 10.3.0.0/16 for Test , etc). Note that security groups are recommended as the primary data-plane isolation mechanism between applications that may coexist in the same VPC. It is anticipated that unrelated applications would coexist in their respective tiers without ever permitting east-west traffic flows. The following subnets are defined by the AWS Secure Environment Architecture : TGW subnet : This subnet hosts the elastic-network interfaces for the TGW attachment. A /27 subnet is sufficient. Web subnet : This subnet hosts front-end or otherwise 'client' facing infrastructure. A /20 or larger subnet is recommended to facilitate auto-scaling. App subnet : This subnet hosts app-tier code (EC2, containers, etc). A /19 or larger subnet is recommended to facilitate auto-scaling. Data subnet : This subnet hosts data-tier code (RDS instances, ElastiCache instances). A /21 or larger subnet is recommended. Mgmt subnet : This subnet hosts bastion or other management instances. A /21 or larger subnet is recommended. Each subnet is associated with a Common VPC Route Table, as depicted above. Gateway Endpoints for relevant services (Amazon S3, Amazon DynamoDB) are installed in the Common route tables of all Workload VPCs. Aside from local traffic or gateway-bound traffic, 0.0.0.0/0 is always destined for the TGW.","title":"Workload VPCs"},{"location":"architectures/pbmm/network/#central-vpc","text":"The Central VPC is a network for localizing operational infrastructure that may be needed across the Organization, such as code repositories, artifact repositories, and notably, the managed Directory Service (Microsoft AD). Instances that are domain joined will connect to this AD domain - a network flow that is made possible from anywhere in the network structure due to the inclusion of the Central VPC in all relevant association TGW RTs. It is recommended that the Central VPC use a RFC1918 range (e.g. 10.1.0.0/16 ) for the purposes of routing from the workload VPCs, and a secondary range from the RFC6598 block (e.g. 100.96.252.0/23 ) to support the Microsoft AD workload. Note that this VPC also contains a peering relationship to the ForSSO VPC in the Organization Management (root) account. This exists purely to support connectivity from an AD-Connector instance in the Organization Management (root) account, which in turn enables AWS SSO for federated login to the AWS control plane.","title":"Central VPC"},{"location":"architectures/pbmm/network/#sandbox-vpc","text":"A sandbox VPC, not depicted, may be included in the AWS Secure Environment Architecture . This is not connected to the Transit Gateway, Perimeter VPC, on-premises network, or other common infrastructure. It contains its own Internet Gateway, and is an entirely separate VPC with respect to the rest of the AWS Secure Environment Architecture . The sandbox VPC should be used exclusively for time-limited experimentation, particularly with out-of-region services, and never used for any line of business workload or data.","title":"Sandbox VPC"},{"location":"developer/","text":"Accelerator Developer Guide \u00b6 This document is a reference document. Instead of reading through it in linear order, you can use it to look up specific issues as needed. It is important to read the Operations Guide before reading this document. If you're interested in actively contributing to the project, you should also review the Governance and Contributing Guide .","title":"Accelerator Developer Guide"},{"location":"developer/#accelerator-developer-guide","text":"This document is a reference document. Instead of reading through it in linear order, you can use it to look up specific issues as needed. It is important to read the Operations Guide before reading this document. If you're interested in actively contributing to the project, you should also review the Governance and Contributing Guide .","title":"Accelerator Developer Guide"},{"location":"developer/best-practices/","text":"Best Practices \u00b6 TypeScript and NodeJS \u00b6 Handle Unhandled Promises \u00b6 Entry point TypeScript files -- files that start execution instead of just defining methods and classes -- should have the following code snippet at the start of the file. process.on('unhandledRejection', (reason, _) => { console.error(reason); process.exit(1); }); This prevents unhandled promise rejection errors by NodeJS. Please read https://medium.com/dailyjs/how-to-prevent-your-node-js-process-from-crashing-5d40247b8ab2 for more information. CloudFormation \u00b6 Cross-Account/Region References \u00b6 When managing multiple AWS accounts, the Accelerator may need permissions to modify resources in the managed accounts. For example, a transit gateway could be created in a shared network account and it need to be shared to the perimeter account to create a VPN connection. In a single-account environment we would could just: create a single stack and use !Ref to refer to the transit gateway; or deploy two stacks one stack that contains the transit gateway and creates a CloudFormation exported output that contains the transit gateway ID; another stack that imports the exported output value from the previous stack and uses it to create a VPN connection. In a multi-account environment this is not possible and we had to find a way to share outputs across accounts and regions. See Passing Outputs Between Phases . Resource Names and Logical IDs \u00b6 Some resources, like AWS::S3::Bucket , can have an explicit name. Setting an explicit name can introduce some possible issues. The first issue that could occur goes as follows: the named resource has a retention policy to retain the resource after deleting; then the named resource is created through a CloudFormation stack; next, an error happens while creating or updating the stack and the stack rolls back; and finally the named resource is deleted from the stack but has a retention policy to retain, so the resource not be deleted; Suppose then that the stack creation issue is resolved and we retry to create the named resource through the CloudFormation stack: the named resource is created through a CloudFormation stack; the named resource will fail to create because a resource with the given name already exists. The best way to prevent this issue from happening is to not explicitly set a name for the resource and let CloudFormation generate the name. Another issue could occur when changing the logical ID of the named resource. This is documented in the following section. Changing Logical IDs \u00b6 When changing the logical ID of a resource CloudFormation assumes the resource is a new resource since it has a logical ID it does not know yet. When updating a stack, CloudFormation will always prioritize resource creation before deletion. The following issue could occur when the resource has an explicit name. CloudFormation will try to create the resource anew and will fail since a resource with the given name already exists. Example of resources where this could happen are AWS::S3::Bucket , AWS::SecretManager::Secret . Changing (Immutable) Properties \u00b6 Not only changing logical IDs could cause CloudFormation to replace resources. Changing immutable properties also cause replacement of resources. See Update behaviors of stack resources . Be especially careful when: changing immutable properties for a named resource. Example of a resource is AWS::Budgets::Budget , AWS::ElasticLoadBalancingV2::LoadBalancer . updating network interfaces for an AWS::EC2::Instance . Not only will this cause the instance to re-create, it will also fail to attach the network interfaces to the new EC2 instance. CloudFormation creates the new EC2 instance first before deleting the old one. It will try to attach the network interfaces to the new instance, but the network interfaces are still attached to the old instance and CloudFormation will fail. For some named resources, like AWS::AutoScaling::LaunchConfiguration and AWS::Budgets::Budget , we append a hash to the name of the resource that is based on its properties. This way when an immutable property is changed, the name will also change, and the resource will be replaced successfully. See for example src/lib/cdk-constructs/src/autoscaling/launch-configuration.ts and src/lib/cdk-constructs/src//billing/budget.ts . export type LaunchConfigurationProps = autoscaling.CfnLaunchConfigurationProps; /** * Wrapper around CfnLaunchConfiguration. The construct adds a hash to the launch configuration name that is based on * the launch configuration properties. The hash makes sure the launch configuration gets replaced correctly by * CloudFormation. */ export class LaunchConfiguration extends autoscaling.CfnLaunchConfiguration { constructor(scope: cdk.Construct, id: string, props: LaunchConfigurationProps) { super(scope, id, props); if (props.launchConfigurationName) { const hash = hashSum({ ...props, path: this.node.path }); this.launchConfigurationName = `${props.launchConfigurationName}-${hash}`; } } } CDK \u00b6 CDK makes heavy use of CloudFormation so all best practices that apply to CloudFormation also apply to CDK. Logical IDs \u00b6 The logical ID of a CDK component is calculated based on its path in the construct tree. Be careful moving around constructs in the construct tree -- e.g. changing the parent of a construct or nesting a construct in another construct -- as this will change the logical ID of the construct. Then you could end up with the issues described in section Changing Logical IDs and section Changing (Immutable) Properties . See Logical ID Stability for more information. Moving Resources between Nested Stacks \u00b6 In some cases we use nested stacks to overcome the limit of 200 CloudFormation resources per stack . In the code snippet below you can see how we generate a dynamic amount of nested stack based on the amount of interface endpoints we construct. The InterfaceEndpoint construct contains CloudFormation resources so we have to be careful to not exceed the limit of 200 CloudFormation resources per nested stack. That is why we limit the amount of interface endpoints to 30 per nested stack. let endpointCount = 0; let endpointStackIndex = 0; let endpointStack; for (const endpoint of endpointConfig.endpoints) { if (!endpointStack || endpointCount >= 30) { endpointStack = new NestedStack(accountStack, `Endpoint${endpointStackIndex++}`); endpointCount = 0; } new InterfaceEndpoint(endpointStack, pascalCase(endpoint), { serviceName: endpoint, }); endpointCount++; } We have to be careful here though. Suppose the configuration file contains 40 interface endpoints. The first 30 interface endpoints will be created in the first nested stack; the next 10 interface endpoints will be created in the second nested stack. Suppose now that we remove the first nested endpoint from the configuration file. This will cause the 31st interface endpoint to become the 30th interface endpoint in the list and it will cause the interface endpoint to be moved from the second nested stack to the first nested stack. This will cause the stack updates to fail since CloudFormation will first try to create the interface endpoint in the first nested stack before removing it from the second nested stack. We do currently not support changes to the interface endpoint configuration because of this behavior. L1 vs. L2 Constructs \u00b6 See AWS Construct library for an explanation on L1 and L2 constructs. The L2 constructs for EC2 and VPC do not map well onto the Accelerator-managed resources. For this reason we mostly use L1 CDK constructs -- such as ec2.CfnVPC , ec2.CfnSubnet -- instead of using L2 CDK constructs -- such as ec2.Vpc and ec2.Subnet . CDK Code Dependency on Lambda Function Code \u00b6 You can read about the distinction between CDK code and runtime code in the introduction of the Development section. CDK code can depend on runtime code. For example when we want to create a Lambda function using CDK, we need the runtime code to define the Lambda function. We use npm scripts , npm dependencies and the NodeJS modules API to define this dependency between CDK code and runtime code. First of all, we create a separate folder that contains the workspace and runtime code for our Lambda function. Throughout the project we've called these workspaces ...-lambda but it could also be named ...-runtime . See src/lib/custom-resources/cdk-acm-import-certificate/runtime/package.json . This workspace's package.json file needs a prepare script that compiles the runtime code. See npm-scripts . The package.json file also needs a name and a main entry that points to the compiled code. runtime/package.json { \"name\": \"lambda-fn-runtime\", \"main\": \"dist/index.js\", \"scripts\": { \"prepare\": \"webpack-cli --config webpack.config.ts\" } } Now when another workspace depends on our Lambda function runtime code workspace, the prepare script will run and it will compile the Lambda function runtime code. Next, we add the dependency to the new workspace to the workspace that contains the CDK code using pnpm or by adding it to package.json . cdk/package.json { \"devDependencies\": { \"lambda-fn-runtime\": \"workspace:^0.0.1\" } } In the CDK code we can now resolve the path to the compiled code using the NodeJS modules API. See NodeJS modules API . cdk/src/index.ts class LambdaFun extends cdk.Construct { constructor(scope: cdk.Construct, id: string) { super(scope, id); // Find the runtime package folder and resolves the `main` entry of `package.json`. // In our case this is `node_modules/lambda-fn-runtime/dist/index.js`. const runtimeMain = resolve.require('lambda-fn-runtime'); // Find the directory containing our `index.js` file. // In our case this is `node_modules/lambda-fn-runtime/dist`. const runtimeDir = path.dirname(lambdaPath); new lambda.Function(this, 'Resource', { runtime: lambda.Runtime.NODEJS_14_X, code: lambda.Code.fromAsset(runtimeDir), handler: 'index.handler', // The `handler` function in `index.js` }); } } You now have a CDK Lambda function that uses the compiled Lambda function runtime code. Note : The runtime code needs to recompile every time it changes since the prepare script only runs when the runtime workspace is installed. Custom Resource \u00b6 We create custom resources for functionality that is not supported natively by CloudFormation. We have two types of custom resources in this project: Custom resource that calls an SDK method; Custom resource that needs additional functionality and is backed by a custom Lambda function. CDK has a helper construct for the first type of custom resources. See CDK AwsCustomResource documentation . This helper construct is for example used in the custom resource ds-log-subscription . The second type of custom resources requires a custom Lambda function runtime as described in the previous section. For example acm-import-certificate is backed by a custom Lambda function. Only a single Lambda function is created per custom resource, account and region. This is achieved by creating only a single Lambda function in the construct tree. src/lib/custom-resources/custom-resource/cdk/index.ts class CustomResource extends cdk.Construct { constructor(scope: cdk.Construct, id: string, props: CustomResourceProps) { super(scope, id); new cdk.CustomResource(this, 'Resource', { resourceType: 'Custom::CustomResource', serviceToken: this.lambdaFunction.functionArn, }); } private get lambdaFunction() { const constructName = `CustomResourceLambda`; const stack = cdk.Stack.of(this); const existing = stack.node.tryFindChild(constructName); if (existing) { return existing as lambda.Function; } // The package '@aws-accelerator/custom-resources/cdk-custom-resource-runtime' contains the runtime code for the custom resource const lambdaPath = require.resolve('@aws-accelerator/custom-resources/cdk-custom-resource-runtime'); const lambdaDir = path.dirname(lambdaPath); return new lambda.Function(stack, constructName, { code: lambda.Code.fromAsset(lambdaDir), }); } } Escape Hatches \u00b6 Sometimes CDK does not support a property on a resource that CloudFormation does support. You can then override the property using the addOverride or addPropertyOverride methods on CDK CloudFormation resources. See CDK escape hatches . AutoScaling Group Metadata \u00b6 An example where we override metadata is when we create a launch configuration.S const launchConfig = new autoscaling.CfnLaunchConfiguration(this, 'LaunchConfig', { ... }); launchConfig.addOverride('Metadata.AWS::CloudFormation::Authentication', { S3AccessCreds: { type: 'S3', roleName, buckets: [bucketName], }, }); launchConfig.addOverride('Metadata.AWS::CloudFormation::Init', { configSets: { config: ['setup'], }, setup: { files: { // Add files here }, services: { // Add services here }, commands: { // Add commands here }, }, }); Secret SecretValue \u00b6 Another example is when we want to use secretsmanager.Secret and set the secret value. function setSecretValue(secret: secrets.Secret, value: string) { const cfnSecret = secret.node.defaultChild as secrets.CfnSecret; // Get the L1 resource that backs this L2 resource cfnSecret.addPropertyOverride('SecretString', value); // Override the property `SecretString` on the L1 resource cfnSecret.addPropertyDeletionOverride('GenerateSecretString'); // Delete the property `GenerateSecretString` from the L1 resource }","title":"Best Practices"},{"location":"developer/best-practices/#best-practices","text":"","title":"Best Practices"},{"location":"developer/best-practices/#typescript-and-nodejs","text":"","title":"TypeScript and NodeJS"},{"location":"developer/best-practices/#handle-unhandled-promises","text":"Entry point TypeScript files -- files that start execution instead of just defining methods and classes -- should have the following code snippet at the start of the file. process.on('unhandledRejection', (reason, _) => { console.error(reason); process.exit(1); }); This prevents unhandled promise rejection errors by NodeJS. Please read https://medium.com/dailyjs/how-to-prevent-your-node-js-process-from-crashing-5d40247b8ab2 for more information.","title":"Handle Unhandled Promises"},{"location":"developer/best-practices/#cloudformation","text":"","title":"CloudFormation"},{"location":"developer/best-practices/#cross-accountregion-references","text":"When managing multiple AWS accounts, the Accelerator may need permissions to modify resources in the managed accounts. For example, a transit gateway could be created in a shared network account and it need to be shared to the perimeter account to create a VPN connection. In a single-account environment we would could just: create a single stack and use !Ref to refer to the transit gateway; or deploy two stacks one stack that contains the transit gateway and creates a CloudFormation exported output that contains the transit gateway ID; another stack that imports the exported output value from the previous stack and uses it to create a VPN connection. In a multi-account environment this is not possible and we had to find a way to share outputs across accounts and regions. See Passing Outputs Between Phases .","title":"Cross-Account/Region References"},{"location":"developer/best-practices/#resource-names-and-logical-ids","text":"Some resources, like AWS::S3::Bucket , can have an explicit name. Setting an explicit name can introduce some possible issues. The first issue that could occur goes as follows: the named resource has a retention policy to retain the resource after deleting; then the named resource is created through a CloudFormation stack; next, an error happens while creating or updating the stack and the stack rolls back; and finally the named resource is deleted from the stack but has a retention policy to retain, so the resource not be deleted; Suppose then that the stack creation issue is resolved and we retry to create the named resource through the CloudFormation stack: the named resource is created through a CloudFormation stack; the named resource will fail to create because a resource with the given name already exists. The best way to prevent this issue from happening is to not explicitly set a name for the resource and let CloudFormation generate the name. Another issue could occur when changing the logical ID of the named resource. This is documented in the following section.","title":"Resource Names and Logical IDs"},{"location":"developer/best-practices/#changing-logical-ids","text":"When changing the logical ID of a resource CloudFormation assumes the resource is a new resource since it has a logical ID it does not know yet. When updating a stack, CloudFormation will always prioritize resource creation before deletion. The following issue could occur when the resource has an explicit name. CloudFormation will try to create the resource anew and will fail since a resource with the given name already exists. Example of resources where this could happen are AWS::S3::Bucket , AWS::SecretManager::Secret .","title":"Changing Logical IDs"},{"location":"developer/best-practices/#changing-immutable-properties","text":"Not only changing logical IDs could cause CloudFormation to replace resources. Changing immutable properties also cause replacement of resources. See Update behaviors of stack resources . Be especially careful when: changing immutable properties for a named resource. Example of a resource is AWS::Budgets::Budget , AWS::ElasticLoadBalancingV2::LoadBalancer . updating network interfaces for an AWS::EC2::Instance . Not only will this cause the instance to re-create, it will also fail to attach the network interfaces to the new EC2 instance. CloudFormation creates the new EC2 instance first before deleting the old one. It will try to attach the network interfaces to the new instance, but the network interfaces are still attached to the old instance and CloudFormation will fail. For some named resources, like AWS::AutoScaling::LaunchConfiguration and AWS::Budgets::Budget , we append a hash to the name of the resource that is based on its properties. This way when an immutable property is changed, the name will also change, and the resource will be replaced successfully. See for example src/lib/cdk-constructs/src/autoscaling/launch-configuration.ts and src/lib/cdk-constructs/src//billing/budget.ts . export type LaunchConfigurationProps = autoscaling.CfnLaunchConfigurationProps; /** * Wrapper around CfnLaunchConfiguration. The construct adds a hash to the launch configuration name that is based on * the launch configuration properties. The hash makes sure the launch configuration gets replaced correctly by * CloudFormation. */ export class LaunchConfiguration extends autoscaling.CfnLaunchConfiguration { constructor(scope: cdk.Construct, id: string, props: LaunchConfigurationProps) { super(scope, id, props); if (props.launchConfigurationName) { const hash = hashSum({ ...props, path: this.node.path }); this.launchConfigurationName = `${props.launchConfigurationName}-${hash}`; } } }","title":"Changing (Immutable) Properties"},{"location":"developer/best-practices/#cdk","text":"CDK makes heavy use of CloudFormation so all best practices that apply to CloudFormation also apply to CDK.","title":"CDK"},{"location":"developer/best-practices/#logical-ids","text":"The logical ID of a CDK component is calculated based on its path in the construct tree. Be careful moving around constructs in the construct tree -- e.g. changing the parent of a construct or nesting a construct in another construct -- as this will change the logical ID of the construct. Then you could end up with the issues described in section Changing Logical IDs and section Changing (Immutable) Properties . See Logical ID Stability for more information.","title":"Logical IDs"},{"location":"developer/best-practices/#moving-resources-between-nested-stacks","text":"In some cases we use nested stacks to overcome the limit of 200 CloudFormation resources per stack . In the code snippet below you can see how we generate a dynamic amount of nested stack based on the amount of interface endpoints we construct. The InterfaceEndpoint construct contains CloudFormation resources so we have to be careful to not exceed the limit of 200 CloudFormation resources per nested stack. That is why we limit the amount of interface endpoints to 30 per nested stack. let endpointCount = 0; let endpointStackIndex = 0; let endpointStack; for (const endpoint of endpointConfig.endpoints) { if (!endpointStack || endpointCount >= 30) { endpointStack = new NestedStack(accountStack, `Endpoint${endpointStackIndex++}`); endpointCount = 0; } new InterfaceEndpoint(endpointStack, pascalCase(endpoint), { serviceName: endpoint, }); endpointCount++; } We have to be careful here though. Suppose the configuration file contains 40 interface endpoints. The first 30 interface endpoints will be created in the first nested stack; the next 10 interface endpoints will be created in the second nested stack. Suppose now that we remove the first nested endpoint from the configuration file. This will cause the 31st interface endpoint to become the 30th interface endpoint in the list and it will cause the interface endpoint to be moved from the second nested stack to the first nested stack. This will cause the stack updates to fail since CloudFormation will first try to create the interface endpoint in the first nested stack before removing it from the second nested stack. We do currently not support changes to the interface endpoint configuration because of this behavior.","title":"Moving Resources between Nested Stacks"},{"location":"developer/best-practices/#l1-vs-l2-constructs","text":"See AWS Construct library for an explanation on L1 and L2 constructs. The L2 constructs for EC2 and VPC do not map well onto the Accelerator-managed resources. For this reason we mostly use L1 CDK constructs -- such as ec2.CfnVPC , ec2.CfnSubnet -- instead of using L2 CDK constructs -- such as ec2.Vpc and ec2.Subnet .","title":"L1 vs. L2 Constructs"},{"location":"developer/best-practices/#cdk-code-dependency-on-lambda-function-code","text":"You can read about the distinction between CDK code and runtime code in the introduction of the Development section. CDK code can depend on runtime code. For example when we want to create a Lambda function using CDK, we need the runtime code to define the Lambda function. We use npm scripts , npm dependencies and the NodeJS modules API to define this dependency between CDK code and runtime code. First of all, we create a separate folder that contains the workspace and runtime code for our Lambda function. Throughout the project we've called these workspaces ...-lambda but it could also be named ...-runtime . See src/lib/custom-resources/cdk-acm-import-certificate/runtime/package.json . This workspace's package.json file needs a prepare script that compiles the runtime code. See npm-scripts . The package.json file also needs a name and a main entry that points to the compiled code. runtime/package.json { \"name\": \"lambda-fn-runtime\", \"main\": \"dist/index.js\", \"scripts\": { \"prepare\": \"webpack-cli --config webpack.config.ts\" } } Now when another workspace depends on our Lambda function runtime code workspace, the prepare script will run and it will compile the Lambda function runtime code. Next, we add the dependency to the new workspace to the workspace that contains the CDK code using pnpm or by adding it to package.json . cdk/package.json { \"devDependencies\": { \"lambda-fn-runtime\": \"workspace:^0.0.1\" } } In the CDK code we can now resolve the path to the compiled code using the NodeJS modules API. See NodeJS modules API . cdk/src/index.ts class LambdaFun extends cdk.Construct { constructor(scope: cdk.Construct, id: string) { super(scope, id); // Find the runtime package folder and resolves the `main` entry of `package.json`. // In our case this is `node_modules/lambda-fn-runtime/dist/index.js`. const runtimeMain = resolve.require('lambda-fn-runtime'); // Find the directory containing our `index.js` file. // In our case this is `node_modules/lambda-fn-runtime/dist`. const runtimeDir = path.dirname(lambdaPath); new lambda.Function(this, 'Resource', { runtime: lambda.Runtime.NODEJS_14_X, code: lambda.Code.fromAsset(runtimeDir), handler: 'index.handler', // The `handler` function in `index.js` }); } } You now have a CDK Lambda function that uses the compiled Lambda function runtime code. Note : The runtime code needs to recompile every time it changes since the prepare script only runs when the runtime workspace is installed.","title":"CDK Code Dependency on Lambda Function Code"},{"location":"developer/best-practices/#custom-resource","text":"We create custom resources for functionality that is not supported natively by CloudFormation. We have two types of custom resources in this project: Custom resource that calls an SDK method; Custom resource that needs additional functionality and is backed by a custom Lambda function. CDK has a helper construct for the first type of custom resources. See CDK AwsCustomResource documentation . This helper construct is for example used in the custom resource ds-log-subscription . The second type of custom resources requires a custom Lambda function runtime as described in the previous section. For example acm-import-certificate is backed by a custom Lambda function. Only a single Lambda function is created per custom resource, account and region. This is achieved by creating only a single Lambda function in the construct tree. src/lib/custom-resources/custom-resource/cdk/index.ts class CustomResource extends cdk.Construct { constructor(scope: cdk.Construct, id: string, props: CustomResourceProps) { super(scope, id); new cdk.CustomResource(this, 'Resource', { resourceType: 'Custom::CustomResource', serviceToken: this.lambdaFunction.functionArn, }); } private get lambdaFunction() { const constructName = `CustomResourceLambda`; const stack = cdk.Stack.of(this); const existing = stack.node.tryFindChild(constructName); if (existing) { return existing as lambda.Function; } // The package '@aws-accelerator/custom-resources/cdk-custom-resource-runtime' contains the runtime code for the custom resource const lambdaPath = require.resolve('@aws-accelerator/custom-resources/cdk-custom-resource-runtime'); const lambdaDir = path.dirname(lambdaPath); return new lambda.Function(stack, constructName, { code: lambda.Code.fromAsset(lambdaDir), }); } }","title":"Custom Resource"},{"location":"developer/best-practices/#escape-hatches","text":"Sometimes CDK does not support a property on a resource that CloudFormation does support. You can then override the property using the addOverride or addPropertyOverride methods on CDK CloudFormation resources. See CDK escape hatches .","title":"Escape Hatches"},{"location":"developer/contributing-guidelines/","text":"How to Contribute \u00b6 How-to \u00b6 Adding New Functionality? \u00b6 Before making a change or adding new functionality you have to verify what kind of functionality is being added. Is it an Accelerator-management change? Is the change related to the Installer stack? Is the change CDK related? Make the change in src/installer/cdk . Is the change runtime related? Make the change in src/installer/cdk/assets . Is the change related to the Initial Setup stack? Is the change CDK related? Make the change in src/core/cdk Is the change runtime related? Make the change in src/core/runtime Is it an Accelerator-managed change? Is the change related to the Phase stacks? Is the change CDK related? Make the change in src/deployments/cdk Is the change runtime related? Make the change in src/deployments/runtime Create a CDK Lambda Function with Lambda Runtime Code \u00b6 See CDK Code Dependency on Lambda Function Code for a short introduction. Create a Custom Resource \u00b6 See Custom Resource and Custom Resources for a short introduction. Create a separate folder that contains the CDK and Lambda function runtime code, e.g. src/lib/custom-resources/my-custom-resource ; Create a folder my-custom-resource that contains the CDK code; Create a package.json file with a dependency to the my-custom-resource/runtime package; Create a cdk folder that contains the source of the CDK code; Create a folder my-custom-resource/runtime that contains the runtime code; Create a runtime/package.json file with a \"name\" , \"prepare\" script and a \"main\" ; Create a runtime/webpack.config.ts file that compiles TypeScript code to a single JavaScript file; Create a runtime/src folder that contains the source of the Lambda function runtime code; You can look at the src/lib/custom-resources/cdk-acm-import-certificate custom resource as an example. It is best practice to add tags to any resources that the custom resource creates using the cfn-tags library. Run All Unit Tests \u00b6 Run in the root of the project. pnpm recursive run test --no-bail --stream -- --silent Accept Unit Test Snapshot Changes \u00b6 Run in src/deployments/cdk . pnpm run test -- -u Validate Code with Prettier \u00b6 Run in the root of the project. pnpx prettier --check **/*.ts Format Code with Prettier \u00b6 Run in the root of the project. pnpx prettier --write **/*.ts Validate Code with tslint \u00b6 Run in the root of the project. pnpm recursive run lint --stream --no-bail","title":"How to Contribute"},{"location":"developer/contributing-guidelines/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"developer/contributing-guidelines/#how-to","text":"","title":"How-to"},{"location":"developer/contributing-guidelines/#adding-new-functionality","text":"Before making a change or adding new functionality you have to verify what kind of functionality is being added. Is it an Accelerator-management change? Is the change related to the Installer stack? Is the change CDK related? Make the change in src/installer/cdk . Is the change runtime related? Make the change in src/installer/cdk/assets . Is the change related to the Initial Setup stack? Is the change CDK related? Make the change in src/core/cdk Is the change runtime related? Make the change in src/core/runtime Is it an Accelerator-managed change? Is the change related to the Phase stacks? Is the change CDK related? Make the change in src/deployments/cdk Is the change runtime related? Make the change in src/deployments/runtime","title":"Adding New Functionality?"},{"location":"developer/contributing-guidelines/#create-a-cdk-lambda-function-with-lambda-runtime-code","text":"See CDK Code Dependency on Lambda Function Code for a short introduction.","title":"Create a CDK Lambda Function with Lambda Runtime Code"},{"location":"developer/contributing-guidelines/#create-a-custom-resource","text":"See Custom Resource and Custom Resources for a short introduction. Create a separate folder that contains the CDK and Lambda function runtime code, e.g. src/lib/custom-resources/my-custom-resource ; Create a folder my-custom-resource that contains the CDK code; Create a package.json file with a dependency to the my-custom-resource/runtime package; Create a cdk folder that contains the source of the CDK code; Create a folder my-custom-resource/runtime that contains the runtime code; Create a runtime/package.json file with a \"name\" , \"prepare\" script and a \"main\" ; Create a runtime/webpack.config.ts file that compiles TypeScript code to a single JavaScript file; Create a runtime/src folder that contains the source of the Lambda function runtime code; You can look at the src/lib/custom-resources/cdk-acm-import-certificate custom resource as an example. It is best practice to add tags to any resources that the custom resource creates using the cfn-tags library.","title":"Create a Custom Resource"},{"location":"developer/contributing-guidelines/#run-all-unit-tests","text":"Run in the root of the project. pnpm recursive run test --no-bail --stream -- --silent","title":"Run All Unit Tests"},{"location":"developer/contributing-guidelines/#accept-unit-test-snapshot-changes","text":"Run in src/deployments/cdk . pnpm run test -- -u","title":"Accept Unit Test Snapshot Changes"},{"location":"developer/contributing-guidelines/#validate-code-with-prettier","text":"Run in the root of the project. pnpx prettier --check **/*.ts","title":"Validate Code with Prettier"},{"location":"developer/contributing-guidelines/#format-code-with-prettier","text":"Run in the root of the project. pnpx prettier --write **/*.ts","title":"Format Code with Prettier"},{"location":"developer/contributing-guidelines/#validate-code-with-tslint","text":"Run in the root of the project. pnpm recursive run lint --stream --no-bail","title":"Validate Code with tslint"},{"location":"developer/development/","text":"Development Guide \u00b6 There are different types of projects in this monorepo. Projects containing CDK code that compiles to CloudFormation templates and deploy to AWS using the CDK toolkit; Projects containing runtime code that is used by the CDK code to deploy Lambda functions; Projects containing reusable code; both for use by the CDK code and/or runtime code. The CDK code either deploys Accelerator-management resources or Accelerator-managed resources. See the Operations Guide for the distinction between Accelerator-management and Accelerator-managed resources. The only language used in the project is TypeScript and exceptionally JavaScript. We do not write CloudFormation templates, only CDK code. When we want to enable functionality in a managed account we try to use native CloudFormation/CDK resource to enable the functionality; create a custom resource to enable the functionality; or lastly create a new step in the Initial Setup state machine to enable the functionality. Project Structure \u00b6 The folder structure of the project is as follows: src/installer/cdk : See Installer Stack ; src/core/cdk : See Initial Setup Stack ; src/core/runtime See Initial Setup Stack and Phase Steps and Phase Stacks ; src/deployments/runtime See Phase Steps and Phase Stacks ; src/deployments/cdk : See Phase Steps and Phase Stacks ; src/lib/accelerator-cdk : See Libraries & Tools ; src/lib/cdk-constructs : See Libraries & Tools ; src/lib/cdk-plugin-assume-role : See CDK Assume Role Plugin . src/lib/common-config : See Libraries & Tools ; src/lib/common-outputs : See Libraries & Tools ; src/lib/common-types : See Libraries & Tools ; src/lib/common : See Libraries & Tools ; src/lib/custom-resources/**/cdk : See Custom Resources ; src/lib/custom-resources/**/runtime : See Custom Resources ; Installer Stack \u00b6 Read the Operations Guide first before reading this section. This section is a technical addition to the section in the Operations Guide. As stated in the Operations Guide, the Installer stack is responsible for installing the Initial Setup stack. It is an Accelerator-management resource. The main resource in the Installer stack is the ASEA-Installer CodePipeline. The CodePipeline uses this GitHub repository as source action and runs CDK in a CodeBuild step to deploy the Initial Setup stack. new codebuild.PipelineProject(stack, 'InstallerProject', { buildSpec: codebuild.BuildSpec.fromObject({ version: '0.2', phases: { install: { 'runtime-versions': { nodejs: 14, }, // The flag '--unsafe-perm' is necessary to run pnpm scripts in Docker commands: ['npm install --global pnpm@6.2.3', 'pnpm install --unsafe-perm --frozen-lockfile'], }, pre_build: { // The flag '--unsafe-perm' is necessary to run pnpm scripts in Docker commands: ['pnpm recursive run build --unsafe-perm'], }, build: { commands: [ 'cd src/core/cdk', // Bootstrap the environment for use by CDK 'pnpx cdk bootstrap --require-approval never', // Deploy the Initial Setup stack 'pnpx cdk deploy --require-approval never', ], }, }, }), }); When the CodePipeline finishes deploying the Initial Setup stack, it starts a Lambda function that starts the execution of the Initial Setup stack's main state machine. The Initial Setup stack deployment receives environment variables from the CodePipeline's CodeBuild step. The most notable environment variables are: ACCELERATOR_STATE_MACHINE_NAME : The Initial Setup will use this name for the main state machine. So it is the Installer stack that decides the name of the main state machine. This way we can confidently start the main state machine of the Initial Setup stack from the CodePipeline; ENABLE_PREBUILT_PROJECT : See Prebuilt Docker Image . Initial Setup Stack \u00b6 Read Operations Guide first before reading this section. This section is a technical addition to the section in the Operations Guide. As stated in the Operations Guide, the Initial Setup stack consists of a state machine, named ASEA-MainStateMachine_sm , which executes steps to create the Accelerator-managed stacks and resources in the managed accounts. It is an Accelerator-management resource. The Initial Setup stack is defined in the src/core/cdk folder. The Initial Setup stack is similar to the Installer stack, as in that it runs a CodeBuild project to deploy others stacks using CDK. In case of the Initial Setup stack we use a AWS Step Functions State Machine to run steps instead of using a CodePipeline; we deploy multiple stacks, called Phase stacks, in Accelerator-managed accounts. These Phase stacks contain Accelerator-managed resources. In order to install these Phase stacks in Accelerator-managed accounts, we need access to those accounts. We create a stack set in the Organization Management (root) account that has instances in all Accelerator-managed accounts. This stack set contains what we call the PipelineRole . The code for the steps in the state machine is in src/core/runtime . All the steps are in different files but are compiled into a single file. We used to compile all the steps separately but we would hit a limit in the amount of parameters in the generated CloudFormation template. Each step would have its own CDK asset that would introduce three new parameters. We quickly reached the limit of 60 parameters in a CloudFormation template and decided to compile the steps into a single file and use it across all different Lambda functions. CodeBuild and Prebuilt Docker Image \u00b6 The CodeBuild project that deploys the different Phase stacks is constructed using the CdkDeployProject or PrebuiltCdkDeployProject based on the value of the environment variable ENABLE_PREBUILT_PROJECT . The first, CdkDeployProject constructs a CodeBuild project that copies this whole Github repository as a ZIP file to S3 using CDK S3 assets . This ZIP file is then used as source for the CodeBuild project. When the CodeBuild project executes, it runs pnpm recursive install which in turn will run all prepare scripts in all package.json files in the project -- as described in section CDK Code Dependency on Lambda Function Code . After installing the dependencies, the CodeBuild project deploys the Phase stacks. cd src/deployments/cdk sh codebuild-deploy.sh We have more than 50 workspace projects in the monorepo with a prepare script, so the pnpm recursive install step can take some time. Also, the CodeBuild project will run for each deployed Phase stack in each Accelerator-managed account. This is where the PrebuiltCdkDeployProject CodeBuild project comes in. The PrebuiltCdkDeployProject contains a Docker image that contains the whole project in the /app directory and has all the dependencies already installed. FROM node:12-alpine3.11 # Install the package manager RUN npm install --global pnpm RUN mkdir /app WORKDIR /app # Copy over the project root to the /app directory ADD . /app/ # Install the dependencies RUN pnpm install --unsafe-perm --frozen-lockfile # Build all Lambda function runtime code RUN pnpm recursive run build --unsafe-perm When this CodeBuild project executes, it uses the Docker image as base -- the dependencies are already installed -- and runs the same commands as the CdkDeployProject to deploy the Phase stacks. Passing Data to Phase Steps and Phase Stacks \u00b6 Some steps in the state machine write data to Amazon DynamoDB. This data is necessary to deploy the Phase stacks later on. At one time this data was written to Secrets Manager and/or S3, these mechanisms were deemed ineffective due to object size limitations or consistency challenges and were all eventually migrated to DynamoDB. Load Accounts step: This step finds the Accelerator-managed accounts in AWS Organizations and stores the account key -- the key of the account in mandatory-account-configs or workload-account-configs object in the Accelerator config -- and account ID and other useful information in the ASEA-Parameters table, accounts/# key and accounts-items-count key; Load Organizations step: More or less the same as the Load Accounts step but for organizational units in AWS Organizations and stores the values in the ASEA-Parameters table, organizations key; Load Limits step: This step requests limit increases for Accelerator-managed accounts and stores the current limits in the the ASEA-Parameters table, limits key. Store Phase X Output : This step loads stack outputs from all existing Phase stacks and stores the outputs in the DynamoDB table ASEA-Outputs . Other data is passed through environment variables: ACCELERATOR_NAME : The name of the Accelerator; ACCELERATOR_PREFIX : The prefix for all named Accelerator-managed resources; ACCELERATOR_EXECUTION_ROLE_NAME : The name of the execution role in the Accelerator-managed accounts. This is the PipelineRole we created using stack sets. Phase Steps and Phase Stacks \u00b6 Read Operations Guide first before reading this section. This section is a technical addition to the Deploy Phase X sections in the Operations Guide. The Phase stacks contain the Accelerator-managed resources. The reason the deployment of Accelerator-managed resources is split into different phases is because there cannot be cross account/region references between CloudFormation stacks. See Cross-Account/Region References . The Phase stacks are deployed by a CodeBuild project in the Initial Setup stack as stated in the previous paragraphs. The CodeBuild project executes the codebuild-deploy.sh script. See initial-setup.ts . The codebuild-deploy.sh script executes the cdk.ts file. The cdk.ts file is meant as a replacement for the cdk CLI command. To deploy a phase stack you would not run pnpx cdk deploy but cdk.sh --phase 1 . See CDK API for more information why we use the CDK API instead of using the CDK CLI. The cdk.ts command parses command line arguments and creates all the cdk.App for all accounts and regions for the given --phase . When you pass the --region or --account-key command, all the cdk.App for all accounts and regions will still be created, except that only the cdk.App s matching the parameters will be deployed. This behavior could be optimized in the future. See Stacks with Same Name in Different Regions for more information why we're creating multiple cdk.App s. Store outputs to SSM Parameter Store \u00b6 Customers need the CloudFormation outputs of resources that are created by the accelerator in order to deploy their own resources in AWS. eg. vpcId in shared-network account to create an ec2 instance, etc. This step loads the stack outputs from our DynamoDB Table ASEA-Outputs and stores as key value pairs in SSM Parameter Store in each account. Example values are /ASEA/network/vpc/1/name => Endpoint /ASEA/network/vpc/1/id => vpc-XXXXXXXXXX ASEA-Outputs-Utils DynamoDB Table is used extensively to maintain same index irrespective of configuration changes. This allows customers to reliably build Infrastructure as Code (IaC) which depends on accelerator deployed objects like VPC's, security groups, subnets, ELB's, KMS keys, IAM users and policies. Rather than making the parameters dependent on object names, we used an indexing scheme, which we maintain and don't update as a customers configuration changes. We have attempted to keep the index values consistent across accounts (based on the config file), such that when code is propoted through the SDLC cycle from Dev to Test to Prod, the input parameters to the IaC scripts do not need to be updated, the App subnet, for example, will have the same index value in all accounts. Phases and Deployments \u00b6 The cdk.ts file calls the deploy method in the apps/app.ts . This deploy method loads the Accelerator configuration, accounts, organizations from DynamoDB; loads the stack outputs from Amazon DynamoDB; and loads required environment variables. /** * Input to the `deploy` method of a phase. */ export interface PhaseInput { // The config.json file acceleratorConfig: AcceleratorConfig; // Auxiliary class to construct stacks accountStacks: AccountStacks; // The list of accounts, their key in the configuration file and their ID accounts: Account[]; // The parsed environment variables context: Context; // The list of stack outputs from previous phases outputs: StackOutput[]; // Auxiliary class to manage limits limiter: Limiter; } It is important to note that no configuration is hard-coded. The CloudFormation templates are generated by CDK and the CDK constructs are created according to the configuration file. Changes to the configuration will change the CDK construct tree and that will result in a different CloudFormation template that is deployed. The different phases are defined in apps/phase-x.ts . Historically we created all CDK constructs in the phase-x.ts files. After a while the phase-x.ts files started to get too big and we moved to separating the logic into separate deployments. Every logical component has a separate folder in the deployments folder. Every deployment consists of so-called steps. Separate steps are put in loaded in phases. For example, take the deployments/defaults deployment. The deployment consists of two steps, i.e. step-1.ts and step-2.ts . deployments/defaults/step-1.ts is created in apps/phase-0.ts and deployments/defaults/step-2.ts is created in apps/phase-1.ts . You can find more details about what happens in each phase in the Operations Guide . apps/phase-0.ts export async function deploy({ acceleratorConfig, accountStacks, accounts, context }: PhaseInput) { // Create defaults, e.g. S3 buckets, EBS encryption keys const defaultsResult = await defaults.step1({ acceleratorPrefix: context.acceleratorPrefix, accountStacks, accounts, config: acceleratorConfig, }); apps/phase-1.ts export async function deploy({ acceleratorConfig, accountStacks, accounts, outputs }: PhaseInput) { // Find the central bucket in the outputs const centralBucket = CentralBucketOutput.getBucket({ accountStacks, config: acceleratorConfig, outputs, }); // Find the log bucket in the outputs const logBucket = LogBucketOutput.getBucket({ accountStacks, config: acceleratorConfig, outputs, }); // Find the account buckets in the outputs const accountBuckets = await defaults.step2({ accounts, accountStacks, centralLogBucket: logBucket, config: acceleratorConfig, }); } Passing Outputs between Phases \u00b6 The CodeBuild step that is responsible for deploying a Phase stack runs in the Organization Management (root) account. We wrote a CDK plugin that allows the CDK deploy step to assume a role in the Accelerator-managed account and create the CloudFormation Phase stack in the managed account. See CDK Assume Role Plugin . After a Phase-X is deployed in all Accelerator-managed accounts, a step in the Initial Setup state machine collects all the Phase-X stack outputs in all Accelerator-managed accounts and regions and stores theses outputs in DynamoDB. Then the next Phase-X+1 deploys using the outputs from the previous Phase-X stacks. See Creating Stack Outputs for helper constructs to create outputs. Decoupling Configuration from Constructs \u00b6 At the start of the project we created constructs that had tight coupling to the Accelerator config structure. The properties to instantiate a construct would sometimes have a reference to an Accelerator-specific interface. An example of this is the Vpc construct in src/deployments/cdk/common/vpc.ts . Later on in the project we started decoupling the Accelerator config from the construct properties. Good examples are in src/lib/cdk-constructs/ . Decoupling the configuration from the constructs improves reusability and robustness of the codebase. Libraries & Tools \u00b6 CDK Assume Role Plugin \u00b6 At the time of writing, CDK does not support cross-account deployments of stacks. It is possible however to write a CDK plugin and implement your own credential loader for cross-account deployment. We wrote a CDK plugin that can assume a role into another account. In our case, the Organization Management (root) account will assume the PipelineRole in an Accelerator-managed account to deploy stacks. CDK API \u00b6 We use the internal CDK API to deploy the Phase stacks instead of the CDK CLI for the following reasons: It allows us to deploy multiple stacks in parallel; Disable stack termination before destroying a stack; Delete a stack after it initially failed to create; Deploy multiple apps at the same time -- see Stacks with Same Name in Different Regions . The helper class CdkToolkit in toolkit.ts wraps around the CDK API. The risk of using the CDK API directly is that the CDK API can change at any time. There is no stable API yet. When upgrading the CDK version, the CdkToolkit wrapper might need to be adapted. AWS SDK Wrappers \u00b6 You can find aws-sdk wrappers in the src/lib/common/src/aws folder. Most of the classes and functions just wrap around aws-sdk classes and implement promises and exponential backoff to retryable errors. Other classes, like Organizations have additional functionality such as listing all the organizational units in an organization in the function listOrganizationalUnits . Please use the aws-sdk wrappers throughout the project or write an additional wrapper when necessary. Configuration File Parsing \u00b6 The configuration file is defined and validated using the io-ts library. See src/lib/common-config/src/index.ts . In case any changes need to be made to the configuration file parsing, this is the place to be. We wrap a class around the AcceleratorConfig type that contains additional helper functions. You can add your own additional helper functions. AcceleratorNameTagger \u00b6 AcceleratorNameTagger is a CDK aspect that sets the name tag on specific resources based on the construct ID of the resource. The following example illustrates its purpose. const stack = new cdk.Stack(); new ec2.CfnVpc(stack, 'SharedNetwork', {}); Aspects.of(stack).add(new AcceleratorNameTagger()); The example above synthesizes to the following CloudFormation template. Resources: SharedNetworkAB7JKF7: Properties: Tags: - Key: Name Value: SharedNetwork_vpc AcceleratorStack \u00b6 AcceleratorStack is a class that extends cdk.Stack and adds the Accelerator tag to all resources in the stack. It also applies the aspect AcceleratorNameTagger . It is also used by the accelerator-name-generator.ts functions to find the name of the Accelerator . Name Generator \u00b6 The accelerator-name-generator.ts file contains methods that create names for resources that are optionally prefixed with the Accelerator name, and optionally suffixed with a hash based on the path of the resource, the account ID and region of the stack. The functions should be used to create pseudo-random names for IAM roles, KMS keys, key pairs and log groups. AccountStacks \u00b6 AccountStacks is a class that manages the creation of an AcceleratorStack based on a given account key and region. If an account with the given account key cannot be found in the accounts object -- which is loaded by apps/app.ts then no stack will be created. This class is used extensively throughout the phases and deployment steps. export async function step1(props: CertificatesStep1Props) { const { accountStacks, centralBucket: centralBucket, config } = props; for (const { accountKey, certificates } of config.getCertificateConfigs()) { if (certificates.length === 0) { continue; } const accountStack = accountStacks.tryGetOrCreateAccountStack(accountKey); if (!accountStack) { console.warn(`Cannot find account stack ${accountKey}`); continue; } for (const certificate of certificates) { createCertificate({ centralBucket, certificate, scope: accountStack, }); } } } Vpc and ImportedVpc \u00b6 Vpc is an interface in the src/lib/cdk-constructs/src/vpc/vpc.ts file that attempts to define an interface for a VPC. The goal of the interface is to be implemented by a cdk.Construct that implements the interface. This CDK issue provides more background [https://github.com/aws/aws-cdk/issues/5927]. Another goal of the interface is to provide an interface on top of imported VPC outputs. This is what the ImportedVpc class implements. The class loads outputs from VPC in a previous phase and implements the Vpc interface on top of those outputs. Limiter \u00b6 So far we haven't talked about limits yet. There is a step in the Initial Setup state machine that requests limit increases according to the desired limits in the configuration file. The step saves the current limits to the limits key in the DynamoDB table ASEA-Parameters . The apps/app.ts file loads the limits and passes them as an input to the phase deployment. The Limiter class helps keeps track of resource we create and prevents exceeding these limits. for (const { ouKey, accountKey, vpcConfig, deployments } of acceleratorConfig.getVpcConfigs()) { if (!limiter.create(accountKey, Limit.VpcPerRegion, region)) { console.log(`Skipping VPC \"${vpcConfig.name}\" deployment.`); console.log(`Reached maximum VPCs per region for account \"${accountKey}\" and region \"${region}\"`); continue; } createVpc({ ouKey, accountKey, vpcConfig }); } Action Item: This functionality could be redesigned to scan all the constructs in a cdk.App and remove resource that are exceeding any limits. Creating Stack Outputs \u00b6 Initially we would create stack outputs like this: new cdk.CfnOutput(stack, 'BucketOutput', { value: bucket.bucketArn, }); But then we'd get a lot of outputs in a stack. We started some outputs together using JSON. This allowed us to store structured data inside the stack outputs. new JsonOutputValue(stack, 'Output', { type: 'FirewallInstanceOutput', value: { instanceId: instance.instanceId, name: firewallConfig.name, az, }, }); Using the solution above, we'd not have type checking when reading or writing outputs. That's what the class StructuredOutputValue has a solution for. It uses the io-ts library to serialize and deserialize structured types. export const FirewallInstanceOutput = t.interface( { id: t.string, name: t.string, az: t.string, }, 'FirewallInstanceOutput', ); export type FirewallInstanceOutput = t.TypeOf<typeof FirewallInstanceOutput>; new StructuredOutputValue<FirewallInstanceOutput>(stack, 'Output', { type: FirewallInstanceOutput, value: { instanceId: instance.instanceId, name: firewallConfig.name, az, }, }); And we can even improve on this a bit more. export const CfnFirewallInstanceOutput = createCfnStructuredOutput(FirewallInstanceOutput); new CfnFirewallInstanceOutput(stack, 'Output', { vpcId: vpc.ref, vpcName: vpcConfig.name, }); export const FirewallInstanceOutputFinder = createStructuredOutputFinder(FirewallInstanceOutput, () => ({})); // Create an OutputFinder const firewallInstances = FirewallInstanceOutputFinder.findAll({ outputs, accountKey, }); // Example usage of the OutputFinder const firewallInstance = firewallInstances.find(i => i.name === target.name && i.az === target.az); Generally you would place the output type definition inside src/lib/common-outputs along with the output finder. Then in the deployment folder in src/deployments/cdk/deployments you would create an output.ts file where you would define the CDK output type with createCfnStructuredOutput . You would not define the CDK output type in src/lib/common-outputs since that project is also used by runtime code that does not need to know about CDK and CloudFormation. Adding Tags to Shared Resources in Destination Account \u00b6 There is another special type of output, AddTagsToResourcesOutput . It can be used to attach tags to resources that are shared into another account. new AddTagsToResourcesOutput(this, 'OutputSharedResourcesSubnets', { dependencies: sharedSubnets.map(o => o.subnet), produceResources: () => sharedSubnets.map(o => ({ resourceId: o.subnet.ref, resourceType: 'subnet', sourceAccountId: o.sourceAccountId, targetAccountIds: o.targetAccountIds, tags: o.subnet.tags.renderTags(), })), }); This will add the outputs to the stack in the account that is initiating the resource share. Next, the state machine step Add Tags to Shared Resources looks for all those outputs. The step will assume the PipelineRole in the targetAccountIds and attach the given tags to the shared resource. Custom Resources \u00b6 There are different ways to create a custom resource using CDK. See the Custom Resource section for more information. All custom resources have a README.md that demonstrates their usage. Externalizing aws-sdk \u00b6 Some custom resources set the aws-sdk as external dependency and some do not. Example of setting aws-sdk as external dependency. src/lib/custom-resources/cdk-kms-grant/runtime/package.json { \"externals\": [\"aws-lambda\", \"aws-sdk\"], \"dependencies\": { \"aws-lambda\": \"1.0.6\", \"aws-sdk\": \"2.631.0\" } } Example of setting aws-sdk as embedded dependency. src/lib/custom-resources/cdk-guardduty-enable-admin/runtime/package.json { \"externals\": [\"aws-lambda\"], \"dependencies\": { \"aws-lambda\": \"1.0.6\", \"aws-sdk\": \"2.711.0\" } } Setting the aws-sdk library as external is sometimes necessary when a newer aws-sdk version is necessary for the Lambda runtime code. At the time of writing the NodeJS 12 runtime uses aws-sdk version 2.631.0 For example the method AWS.GuardDuty.enableOrganizationAdminAccount was only introduced in aws-sdk version 2.660 . That means that Webpack has to embed the aws-sdk version specified in package.json into the compiled JavaScript file. This can be achieved by removing aws-sdk from the external array. src/lib/custom-resources/cdk-kms-grant/runtime/package.json cfn-response \u00b6 This library helps you send a custom resource response to CloudFormation. src/lib/custom-resources/cdk-kms-grant/runtime/src/index.ts export const handler = errorHandler(onEvent); async function onEvent(event: CloudFormationCustomResourceEvent) { console.log(`Creating KMS grant...`); console.log(JSON.stringify(event, null, 2)); // eslint-disable-next-line default-case switch (event.RequestType) { case 'Create': return onCreate(event); case 'Update': return onUpdate(event); case 'Delete': return onDelete(event); } } cfn-tags \u00b6 This library helps you send attaching tags to resource created in a custom resource. webpack-base \u00b6 This library defines the base Webpack template to compile custom resource runtime code. src/lib/custom-resources/cdk-kms-grant/runtime/package.json { \"name\": \"@aws-accelerator/custom-resource-kms-grant-runtime\", \"version\": \"0.0.1\", \"private\": true, \"scripts\": { \"prepare\": \"webpack-cli --config webpack.config.ts\" }, \"source\": \"src/index.ts\", \"main\": \"dist/index.js\", \"types\": \"dist/index.d.ts\", \"externals\": [\"aws-lambda\", \"aws-sdk\"], \"devDependencies\": { \"@aws-accelerator/custom-resource-runtime-webpack-base\": \"workspace:^0.0.1\", \"@types/aws-lambda\": \"8.10.46\", \"@types/node\": \"14.14.31\", \"ts-loader\": \"7.0.5\", \"typescript\": \"3.8.3\", \"webpack\": \"4.42.1\", \"webpack-cli\": \"3.3.11\" }, \"dependencies\": { \"@aws-accelerator/custom-resource-runtime-cfn-response\": \"workspace:^0.0.1\", \"aws-lambda\": \"1.0.6\", \"aws-sdk\": \"2.668.0\" } } src/lib/custom-resources/cdk-ec2-image-finder/runtime/webpack.config.ts import { webpackConfigurationForPackage } from '@aws-accelerator/custom-resource-runtime-webpack-base'; import pkg from './package.json'; export default webpackConfigurationForPackage(pkg); Workarounds \u00b6 Stacks with Same Name in Different Regions \u00b6 The reason we're creating a cdk.App per account and per region and per phase is because stack names across environments might overlap, and at the time of writing, the CDK CLI does not handle stacks with the same name well. For example, when there is a stack Phase1 in us-east-1 and another stack Phase1 in ca-central-1 , the stacks will both be synthesized by CDK to the cdk.out/Phase1.template.json file and one stack will overwrite another's output. Using multiple cdk.App s overcomes this issues as a different outdir can be set on each cdk.App . These cdk.App s are managed by the AccountStacks abstraction. Local Development \u00b6 Local Installer Stack \u00b6 Use CDK to synthesize the CloudFormation template. cd src/installer/cdk pnpx cdk synth The installer template file is now in cdk.out/AcceleratorInstaller.template.json . This file can be used to install the installer stack. You can also deploy the installer stack directly from the command line but then you'd have to pass some stack parameters. See CDK documentation: Deploying with parameters . cd accelerator/installer pnpx cdk deploy --parameters GithubBranch=main --parameters ConfigS3Bucket=ASEA-myconfigbucket Local Initial Setup Stack \u00b6 There is a script called cdk.sh in src/core/cdk that allows you to deploy the Initial Setup stack. The script sets the required environment variables and makes sure all workspace projects are built before deploying the CDK stack. Phase Stacks \u00b6 There is a script called cdk.sh in src/deployments/cdk that allows you to deploy a phase stack straight from the command-line without having to deploy the Initial Setup stack first. The script enables development mode which means that accounts, organizations, configuration, limits and outputs will be loaded from the local environment instead of loading the values from DynamoDB. The local files that need to be available in the src/deployments/cdk folder are the following. accounts.json based on accelerator/accounts (-Parameters table) [ { \"key\": \"shared-network\", \"id\": \"000000000001\", \"arn\": \"arn:aws:organizations::000000000000:account/o-0123456789/000000000001\", \"name\": \"myacct-ASEA-shared-network\", \"email\": \"myacct+ASEA-mandatory-shared-network@example.com\", \"ou\": \"core\" }, { \"key\": \"operations\", \"id\": \"000000000002\", \"arn\": \"arn:aws:organizations::000000000000:account/o-0123456789/000000000002\", \"name\": \"myacct-ASEA-operations\", \"email\": \"myacct+ASEA-mandatory-operations@example.com\", \"ou\": \"core\" } ] organizations.json based on accelerator/organizations (-Parameters table) [ { \"ouId\": \"ou-0000-00000000\", \"ouArn\": \"arn:aws:organizations::000000000000:ou/o-0123456789/ou-0000-00000000\", \"ouName\": \"core\", \"ouPath\": \"core\" }, { \"ouId\": \"ou-0000-00000001\", \"ouArn\": \"arn:aws:organizations::000000000000:ou/o-0123456789/ou-0000-00000001\", \"ouName\": \"prod\", \"ouPath\": \"prod\" } ] limits.json based on accelerator/limits (-Parameters table) [ { \"accountKey\": \"shared-network\", \"limitKey\": \"Amazon VPC/VPCs per Region\", \"serviceCode\": \"vpc\", \"quotaCode\": \"L-F678F1CE\", \"value\": 15, \"region\": \"ca-central-1\" }, { \"accountKey\": \"shared-network\", \"limitKey\": \"Amazon VPC/Interface VPC endpoints per VPC\", \"serviceCode\": \"vpc\", \"quotaCode\": \"L-29B6F2EB\", \"value\": 50, \"region\": \"ca-central-1\" } ] outputs.json based on the -Outputs table [ { \"accountKey\": \"shared-network\", \"outputKey\": \"DefaultBucketOutputC7CE5936\", \"outputValue\": \"{\\\"type\\\":\\\"AccountBucket\\\",\\\"value\\\":{\\\"bucketArn\\\":\\\"arn:aws:s3:::ASEA-sharednetwork-phase1-cacentral1-18vq0emthri3h\\\",\\\"bucketName\\\":\\\"ASEA-sharednetwork-phase1-cacentral1-18vq0emthri3h\\\",\\\"encryptionKeyArn\\\":\\\"arn:aws:kms:ca-central-1:0000000000001:key/d54a8acb-694c-4fc5-9afe-ca2b263cd0b3\\\",\\\"region\\\":\\\"ca-central-1\\\"}}\" } ] context.json that contains the default values for values that are otherwise passed as environment variables. { \"acceleratorName\": \"ASEA\", \"acceleratorPrefix\": \"ASEA-\", \"acceleratorExecutionRoleName\": \"ASEA-PipelineRole\", \"defaultRegion\": \"ca-central-1\" } config.json that contains the Accelerator configuration. The script also sets the default execution role to allow CDK to assume a role in subaccounts to deploy the phase stacks. Now that you have all the required local files you can deploy the phase stacks using cdk.sh . cd src/deployments/cdk ./cdk.sh deploy --phase 1 # deploy all phase 1 stacks ./cdk.sh deploy --phase 1 --parallel # deploy all phase 1 stacks in parallel ./cdk.sh deploy --phase 1 --account shared-network # deploy phase 1 stacks for account shared-network in all regions ./cdk.sh deploy --phase 1 --region ca-central-1 # deploy phase 1 stacks for region ca-central-1 for all accounts ./cdk.sh deploy --phase 1 --account shared-network --region ca-central-1 # deploy phase 1 stacks for account shared-network and region ca-central Other CDK commands are also available. cd src/deployments/cdk ./cdk.sh bootstrap --phase 1 ./cdk.sh synth --phase 1 Testing \u00b6 We use jest for unit testing. There are no integration tests but this could be set-up by configuring the Installer CodePipeline to have a webhook on the repository and deploying changes automatically. To run unit tests locally you can run the following command in the monorepo. pnpx recursive run test -- --pass-with-no-tests --silent See CDK's documentation on Testing constructs for more information on how to tests CDK constructs. Validating Immutable Property Changes and Logical ID Changes \u00b6 The most important unit test in this project is one that validates that logical IDs and immutable properties do not change unexpectedly. To avoid the issues described in section Resource Names and Logical IDs , Changing Logical IDs and Changing (Immutable) Properties . This test can be found in the src/deployments/cdk/test/apps/unsupported-changes.spec.ts file. It synthesizes the Phase stacks using mocked outputs and uses jest snapshots to compare against future changes. The test will fail when changing immutable properties or changing logical IDs of existing resources. In case the changes are expected then the snapshots will need to be updated. You can update the snapshots by running the following command. pnpx run test -- -u See Accept Unit Test Snapshot Changes . Upgrade CDK \u00b6 There's a test in the file src/deployments/cdk/test/apps/unsupported-changes.spec.ts that is currently commented out. The test takes a snapshot of the whole Phase stack and compares the snapshot to changes in the code. test('templates should stay exactly the same', () => { for (const [stackName, resources] of Object.entries(stackResources)) { // Compare the relevant properties to the snapshot expect(resources).toMatchSnapshot(stackName); } }); Before upgrading CDK we uncomment this test. We run the test to update all the snapshots. Then we update all CDK versions and run the test again to compare the snapshots with the code using the new CDK version. If the test passes, then the upgrade should be stable. Action Item: Automate this process.","title":"Development Guide"},{"location":"developer/development/#development-guide","text":"There are different types of projects in this monorepo. Projects containing CDK code that compiles to CloudFormation templates and deploy to AWS using the CDK toolkit; Projects containing runtime code that is used by the CDK code to deploy Lambda functions; Projects containing reusable code; both for use by the CDK code and/or runtime code. The CDK code either deploys Accelerator-management resources or Accelerator-managed resources. See the Operations Guide for the distinction between Accelerator-management and Accelerator-managed resources. The only language used in the project is TypeScript and exceptionally JavaScript. We do not write CloudFormation templates, only CDK code. When we want to enable functionality in a managed account we try to use native CloudFormation/CDK resource to enable the functionality; create a custom resource to enable the functionality; or lastly create a new step in the Initial Setup state machine to enable the functionality.","title":"Development Guide"},{"location":"developer/development/#project-structure","text":"The folder structure of the project is as follows: src/installer/cdk : See Installer Stack ; src/core/cdk : See Initial Setup Stack ; src/core/runtime See Initial Setup Stack and Phase Steps and Phase Stacks ; src/deployments/runtime See Phase Steps and Phase Stacks ; src/deployments/cdk : See Phase Steps and Phase Stacks ; src/lib/accelerator-cdk : See Libraries & Tools ; src/lib/cdk-constructs : See Libraries & Tools ; src/lib/cdk-plugin-assume-role : See CDK Assume Role Plugin . src/lib/common-config : See Libraries & Tools ; src/lib/common-outputs : See Libraries & Tools ; src/lib/common-types : See Libraries & Tools ; src/lib/common : See Libraries & Tools ; src/lib/custom-resources/**/cdk : See Custom Resources ; src/lib/custom-resources/**/runtime : See Custom Resources ;","title":"Project Structure"},{"location":"developer/development/#installer-stack","text":"Read the Operations Guide first before reading this section. This section is a technical addition to the section in the Operations Guide. As stated in the Operations Guide, the Installer stack is responsible for installing the Initial Setup stack. It is an Accelerator-management resource. The main resource in the Installer stack is the ASEA-Installer CodePipeline. The CodePipeline uses this GitHub repository as source action and runs CDK in a CodeBuild step to deploy the Initial Setup stack. new codebuild.PipelineProject(stack, 'InstallerProject', { buildSpec: codebuild.BuildSpec.fromObject({ version: '0.2', phases: { install: { 'runtime-versions': { nodejs: 14, }, // The flag '--unsafe-perm' is necessary to run pnpm scripts in Docker commands: ['npm install --global pnpm@6.2.3', 'pnpm install --unsafe-perm --frozen-lockfile'], }, pre_build: { // The flag '--unsafe-perm' is necessary to run pnpm scripts in Docker commands: ['pnpm recursive run build --unsafe-perm'], }, build: { commands: [ 'cd src/core/cdk', // Bootstrap the environment for use by CDK 'pnpx cdk bootstrap --require-approval never', // Deploy the Initial Setup stack 'pnpx cdk deploy --require-approval never', ], }, }, }), }); When the CodePipeline finishes deploying the Initial Setup stack, it starts a Lambda function that starts the execution of the Initial Setup stack's main state machine. The Initial Setup stack deployment receives environment variables from the CodePipeline's CodeBuild step. The most notable environment variables are: ACCELERATOR_STATE_MACHINE_NAME : The Initial Setup will use this name for the main state machine. So it is the Installer stack that decides the name of the main state machine. This way we can confidently start the main state machine of the Initial Setup stack from the CodePipeline; ENABLE_PREBUILT_PROJECT : See Prebuilt Docker Image .","title":"Installer Stack"},{"location":"developer/development/#initial-setup-stack","text":"Read Operations Guide first before reading this section. This section is a technical addition to the section in the Operations Guide. As stated in the Operations Guide, the Initial Setup stack consists of a state machine, named ASEA-MainStateMachine_sm , which executes steps to create the Accelerator-managed stacks and resources in the managed accounts. It is an Accelerator-management resource. The Initial Setup stack is defined in the src/core/cdk folder. The Initial Setup stack is similar to the Installer stack, as in that it runs a CodeBuild project to deploy others stacks using CDK. In case of the Initial Setup stack we use a AWS Step Functions State Machine to run steps instead of using a CodePipeline; we deploy multiple stacks, called Phase stacks, in Accelerator-managed accounts. These Phase stacks contain Accelerator-managed resources. In order to install these Phase stacks in Accelerator-managed accounts, we need access to those accounts. We create a stack set in the Organization Management (root) account that has instances in all Accelerator-managed accounts. This stack set contains what we call the PipelineRole . The code for the steps in the state machine is in src/core/runtime . All the steps are in different files but are compiled into a single file. We used to compile all the steps separately but we would hit a limit in the amount of parameters in the generated CloudFormation template. Each step would have its own CDK asset that would introduce three new parameters. We quickly reached the limit of 60 parameters in a CloudFormation template and decided to compile the steps into a single file and use it across all different Lambda functions.","title":"Initial Setup Stack"},{"location":"developer/development/#codebuild-and-prebuilt-docker-image","text":"The CodeBuild project that deploys the different Phase stacks is constructed using the CdkDeployProject or PrebuiltCdkDeployProject based on the value of the environment variable ENABLE_PREBUILT_PROJECT . The first, CdkDeployProject constructs a CodeBuild project that copies this whole Github repository as a ZIP file to S3 using CDK S3 assets . This ZIP file is then used as source for the CodeBuild project. When the CodeBuild project executes, it runs pnpm recursive install which in turn will run all prepare scripts in all package.json files in the project -- as described in section CDK Code Dependency on Lambda Function Code . After installing the dependencies, the CodeBuild project deploys the Phase stacks. cd src/deployments/cdk sh codebuild-deploy.sh We have more than 50 workspace projects in the monorepo with a prepare script, so the pnpm recursive install step can take some time. Also, the CodeBuild project will run for each deployed Phase stack in each Accelerator-managed account. This is where the PrebuiltCdkDeployProject CodeBuild project comes in. The PrebuiltCdkDeployProject contains a Docker image that contains the whole project in the /app directory and has all the dependencies already installed. FROM node:12-alpine3.11 # Install the package manager RUN npm install --global pnpm RUN mkdir /app WORKDIR /app # Copy over the project root to the /app directory ADD . /app/ # Install the dependencies RUN pnpm install --unsafe-perm --frozen-lockfile # Build all Lambda function runtime code RUN pnpm recursive run build --unsafe-perm When this CodeBuild project executes, it uses the Docker image as base -- the dependencies are already installed -- and runs the same commands as the CdkDeployProject to deploy the Phase stacks.","title":"CodeBuild and Prebuilt Docker Image"},{"location":"developer/development/#passing-data-to-phase-steps-and-phase-stacks","text":"Some steps in the state machine write data to Amazon DynamoDB. This data is necessary to deploy the Phase stacks later on. At one time this data was written to Secrets Manager and/or S3, these mechanisms were deemed ineffective due to object size limitations or consistency challenges and were all eventually migrated to DynamoDB. Load Accounts step: This step finds the Accelerator-managed accounts in AWS Organizations and stores the account key -- the key of the account in mandatory-account-configs or workload-account-configs object in the Accelerator config -- and account ID and other useful information in the ASEA-Parameters table, accounts/# key and accounts-items-count key; Load Organizations step: More or less the same as the Load Accounts step but for organizational units in AWS Organizations and stores the values in the ASEA-Parameters table, organizations key; Load Limits step: This step requests limit increases for Accelerator-managed accounts and stores the current limits in the the ASEA-Parameters table, limits key. Store Phase X Output : This step loads stack outputs from all existing Phase stacks and stores the outputs in the DynamoDB table ASEA-Outputs . Other data is passed through environment variables: ACCELERATOR_NAME : The name of the Accelerator; ACCELERATOR_PREFIX : The prefix for all named Accelerator-managed resources; ACCELERATOR_EXECUTION_ROLE_NAME : The name of the execution role in the Accelerator-managed accounts. This is the PipelineRole we created using stack sets.","title":"Passing Data to Phase Steps and Phase Stacks"},{"location":"developer/development/#phase-steps-and-phase-stacks","text":"Read Operations Guide first before reading this section. This section is a technical addition to the Deploy Phase X sections in the Operations Guide. The Phase stacks contain the Accelerator-managed resources. The reason the deployment of Accelerator-managed resources is split into different phases is because there cannot be cross account/region references between CloudFormation stacks. See Cross-Account/Region References . The Phase stacks are deployed by a CodeBuild project in the Initial Setup stack as stated in the previous paragraphs. The CodeBuild project executes the codebuild-deploy.sh script. See initial-setup.ts . The codebuild-deploy.sh script executes the cdk.ts file. The cdk.ts file is meant as a replacement for the cdk CLI command. To deploy a phase stack you would not run pnpx cdk deploy but cdk.sh --phase 1 . See CDK API for more information why we use the CDK API instead of using the CDK CLI. The cdk.ts command parses command line arguments and creates all the cdk.App for all accounts and regions for the given --phase . When you pass the --region or --account-key command, all the cdk.App for all accounts and regions will still be created, except that only the cdk.App s matching the parameters will be deployed. This behavior could be optimized in the future. See Stacks with Same Name in Different Regions for more information why we're creating multiple cdk.App s.","title":"Phase Steps and Phase Stacks"},{"location":"developer/development/#store-outputs-to-ssm-parameter-store","text":"Customers need the CloudFormation outputs of resources that are created by the accelerator in order to deploy their own resources in AWS. eg. vpcId in shared-network account to create an ec2 instance, etc. This step loads the stack outputs from our DynamoDB Table ASEA-Outputs and stores as key value pairs in SSM Parameter Store in each account. Example values are /ASEA/network/vpc/1/name => Endpoint /ASEA/network/vpc/1/id => vpc-XXXXXXXXXX ASEA-Outputs-Utils DynamoDB Table is used extensively to maintain same index irrespective of configuration changes. This allows customers to reliably build Infrastructure as Code (IaC) which depends on accelerator deployed objects like VPC's, security groups, subnets, ELB's, KMS keys, IAM users and policies. Rather than making the parameters dependent on object names, we used an indexing scheme, which we maintain and don't update as a customers configuration changes. We have attempted to keep the index values consistent across accounts (based on the config file), such that when code is propoted through the SDLC cycle from Dev to Test to Prod, the input parameters to the IaC scripts do not need to be updated, the App subnet, for example, will have the same index value in all accounts.","title":"Store outputs to SSM Parameter Store"},{"location":"developer/development/#phases-and-deployments","text":"The cdk.ts file calls the deploy method in the apps/app.ts . This deploy method loads the Accelerator configuration, accounts, organizations from DynamoDB; loads the stack outputs from Amazon DynamoDB; and loads required environment variables. /** * Input to the `deploy` method of a phase. */ export interface PhaseInput { // The config.json file acceleratorConfig: AcceleratorConfig; // Auxiliary class to construct stacks accountStacks: AccountStacks; // The list of accounts, their key in the configuration file and their ID accounts: Account[]; // The parsed environment variables context: Context; // The list of stack outputs from previous phases outputs: StackOutput[]; // Auxiliary class to manage limits limiter: Limiter; } It is important to note that no configuration is hard-coded. The CloudFormation templates are generated by CDK and the CDK constructs are created according to the configuration file. Changes to the configuration will change the CDK construct tree and that will result in a different CloudFormation template that is deployed. The different phases are defined in apps/phase-x.ts . Historically we created all CDK constructs in the phase-x.ts files. After a while the phase-x.ts files started to get too big and we moved to separating the logic into separate deployments. Every logical component has a separate folder in the deployments folder. Every deployment consists of so-called steps. Separate steps are put in loaded in phases. For example, take the deployments/defaults deployment. The deployment consists of two steps, i.e. step-1.ts and step-2.ts . deployments/defaults/step-1.ts is created in apps/phase-0.ts and deployments/defaults/step-2.ts is created in apps/phase-1.ts . You can find more details about what happens in each phase in the Operations Guide . apps/phase-0.ts export async function deploy({ acceleratorConfig, accountStacks, accounts, context }: PhaseInput) { // Create defaults, e.g. S3 buckets, EBS encryption keys const defaultsResult = await defaults.step1({ acceleratorPrefix: context.acceleratorPrefix, accountStacks, accounts, config: acceleratorConfig, }); apps/phase-1.ts export async function deploy({ acceleratorConfig, accountStacks, accounts, outputs }: PhaseInput) { // Find the central bucket in the outputs const centralBucket = CentralBucketOutput.getBucket({ accountStacks, config: acceleratorConfig, outputs, }); // Find the log bucket in the outputs const logBucket = LogBucketOutput.getBucket({ accountStacks, config: acceleratorConfig, outputs, }); // Find the account buckets in the outputs const accountBuckets = await defaults.step2({ accounts, accountStacks, centralLogBucket: logBucket, config: acceleratorConfig, }); }","title":"Phases and Deployments"},{"location":"developer/development/#passing-outputs-between-phases","text":"The CodeBuild step that is responsible for deploying a Phase stack runs in the Organization Management (root) account. We wrote a CDK plugin that allows the CDK deploy step to assume a role in the Accelerator-managed account and create the CloudFormation Phase stack in the managed account. See CDK Assume Role Plugin . After a Phase-X is deployed in all Accelerator-managed accounts, a step in the Initial Setup state machine collects all the Phase-X stack outputs in all Accelerator-managed accounts and regions and stores theses outputs in DynamoDB. Then the next Phase-X+1 deploys using the outputs from the previous Phase-X stacks. See Creating Stack Outputs for helper constructs to create outputs.","title":"Passing Outputs between Phases"},{"location":"developer/development/#decoupling-configuration-from-constructs","text":"At the start of the project we created constructs that had tight coupling to the Accelerator config structure. The properties to instantiate a construct would sometimes have a reference to an Accelerator-specific interface. An example of this is the Vpc construct in src/deployments/cdk/common/vpc.ts . Later on in the project we started decoupling the Accelerator config from the construct properties. Good examples are in src/lib/cdk-constructs/ . Decoupling the configuration from the constructs improves reusability and robustness of the codebase.","title":"Decoupling Configuration from Constructs"},{"location":"developer/development/#libraries-tools","text":"","title":"Libraries &amp; Tools"},{"location":"developer/development/#cdk-assume-role-plugin","text":"At the time of writing, CDK does not support cross-account deployments of stacks. It is possible however to write a CDK plugin and implement your own credential loader for cross-account deployment. We wrote a CDK plugin that can assume a role into another account. In our case, the Organization Management (root) account will assume the PipelineRole in an Accelerator-managed account to deploy stacks.","title":"CDK Assume Role Plugin"},{"location":"developer/development/#cdk-api","text":"We use the internal CDK API to deploy the Phase stacks instead of the CDK CLI for the following reasons: It allows us to deploy multiple stacks in parallel; Disable stack termination before destroying a stack; Delete a stack after it initially failed to create; Deploy multiple apps at the same time -- see Stacks with Same Name in Different Regions . The helper class CdkToolkit in toolkit.ts wraps around the CDK API. The risk of using the CDK API directly is that the CDK API can change at any time. There is no stable API yet. When upgrading the CDK version, the CdkToolkit wrapper might need to be adapted.","title":"CDK API"},{"location":"developer/development/#aws-sdk-wrappers","text":"You can find aws-sdk wrappers in the src/lib/common/src/aws folder. Most of the classes and functions just wrap around aws-sdk classes and implement promises and exponential backoff to retryable errors. Other classes, like Organizations have additional functionality such as listing all the organizational units in an organization in the function listOrganizationalUnits . Please use the aws-sdk wrappers throughout the project or write an additional wrapper when necessary.","title":"AWS SDK Wrappers"},{"location":"developer/development/#configuration-file-parsing","text":"The configuration file is defined and validated using the io-ts library. See src/lib/common-config/src/index.ts . In case any changes need to be made to the configuration file parsing, this is the place to be. We wrap a class around the AcceleratorConfig type that contains additional helper functions. You can add your own additional helper functions.","title":"Configuration File Parsing"},{"location":"developer/development/#creating-stack-outputs","text":"Initially we would create stack outputs like this: new cdk.CfnOutput(stack, 'BucketOutput', { value: bucket.bucketArn, }); But then we'd get a lot of outputs in a stack. We started some outputs together using JSON. This allowed us to store structured data inside the stack outputs. new JsonOutputValue(stack, 'Output', { type: 'FirewallInstanceOutput', value: { instanceId: instance.instanceId, name: firewallConfig.name, az, }, }); Using the solution above, we'd not have type checking when reading or writing outputs. That's what the class StructuredOutputValue has a solution for. It uses the io-ts library to serialize and deserialize structured types. export const FirewallInstanceOutput = t.interface( { id: t.string, name: t.string, az: t.string, }, 'FirewallInstanceOutput', ); export type FirewallInstanceOutput = t.TypeOf<typeof FirewallInstanceOutput>; new StructuredOutputValue<FirewallInstanceOutput>(stack, 'Output', { type: FirewallInstanceOutput, value: { instanceId: instance.instanceId, name: firewallConfig.name, az, }, }); And we can even improve on this a bit more. export const CfnFirewallInstanceOutput = createCfnStructuredOutput(FirewallInstanceOutput); new CfnFirewallInstanceOutput(stack, 'Output', { vpcId: vpc.ref, vpcName: vpcConfig.name, }); export const FirewallInstanceOutputFinder = createStructuredOutputFinder(FirewallInstanceOutput, () => ({})); // Create an OutputFinder const firewallInstances = FirewallInstanceOutputFinder.findAll({ outputs, accountKey, }); // Example usage of the OutputFinder const firewallInstance = firewallInstances.find(i => i.name === target.name && i.az === target.az); Generally you would place the output type definition inside src/lib/common-outputs along with the output finder. Then in the deployment folder in src/deployments/cdk/deployments you would create an output.ts file where you would define the CDK output type with createCfnStructuredOutput . You would not define the CDK output type in src/lib/common-outputs since that project is also used by runtime code that does not need to know about CDK and CloudFormation.","title":"Creating Stack Outputs"},{"location":"developer/development/#custom-resources","text":"There are different ways to create a custom resource using CDK. See the Custom Resource section for more information. All custom resources have a README.md that demonstrates their usage.","title":"Custom Resources"},{"location":"developer/development/#workarounds","text":"","title":"Workarounds"},{"location":"developer/development/#stacks-with-same-name-in-different-regions","text":"The reason we're creating a cdk.App per account and per region and per phase is because stack names across environments might overlap, and at the time of writing, the CDK CLI does not handle stacks with the same name well. For example, when there is a stack Phase1 in us-east-1 and another stack Phase1 in ca-central-1 , the stacks will both be synthesized by CDK to the cdk.out/Phase1.template.json file and one stack will overwrite another's output. Using multiple cdk.App s overcomes this issues as a different outdir can be set on each cdk.App . These cdk.App s are managed by the AccountStacks abstraction.","title":"Stacks with Same Name in Different Regions"},{"location":"developer/development/#local-development","text":"","title":"Local Development"},{"location":"developer/development/#local-installer-stack","text":"Use CDK to synthesize the CloudFormation template. cd src/installer/cdk pnpx cdk synth The installer template file is now in cdk.out/AcceleratorInstaller.template.json . This file can be used to install the installer stack. You can also deploy the installer stack directly from the command line but then you'd have to pass some stack parameters. See CDK documentation: Deploying with parameters . cd accelerator/installer pnpx cdk deploy --parameters GithubBranch=main --parameters ConfigS3Bucket=ASEA-myconfigbucket","title":"Local Installer Stack"},{"location":"developer/development/#local-initial-setup-stack","text":"There is a script called cdk.sh in src/core/cdk that allows you to deploy the Initial Setup stack. The script sets the required environment variables and makes sure all workspace projects are built before deploying the CDK stack.","title":"Local Initial Setup Stack"},{"location":"developer/development/#phase-stacks","text":"There is a script called cdk.sh in src/deployments/cdk that allows you to deploy a phase stack straight from the command-line without having to deploy the Initial Setup stack first. The script enables development mode which means that accounts, organizations, configuration, limits and outputs will be loaded from the local environment instead of loading the values from DynamoDB. The local files that need to be available in the src/deployments/cdk folder are the following. accounts.json based on accelerator/accounts (-Parameters table) [ { \"key\": \"shared-network\", \"id\": \"000000000001\", \"arn\": \"arn:aws:organizations::000000000000:account/o-0123456789/000000000001\", \"name\": \"myacct-ASEA-shared-network\", \"email\": \"myacct+ASEA-mandatory-shared-network@example.com\", \"ou\": \"core\" }, { \"key\": \"operations\", \"id\": \"000000000002\", \"arn\": \"arn:aws:organizations::000000000000:account/o-0123456789/000000000002\", \"name\": \"myacct-ASEA-operations\", \"email\": \"myacct+ASEA-mandatory-operations@example.com\", \"ou\": \"core\" } ] organizations.json based on accelerator/organizations (-Parameters table) [ { \"ouId\": \"ou-0000-00000000\", \"ouArn\": \"arn:aws:organizations::000000000000:ou/o-0123456789/ou-0000-00000000\", \"ouName\": \"core\", \"ouPath\": \"core\" }, { \"ouId\": \"ou-0000-00000001\", \"ouArn\": \"arn:aws:organizations::000000000000:ou/o-0123456789/ou-0000-00000001\", \"ouName\": \"prod\", \"ouPath\": \"prod\" } ] limits.json based on accelerator/limits (-Parameters table) [ { \"accountKey\": \"shared-network\", \"limitKey\": \"Amazon VPC/VPCs per Region\", \"serviceCode\": \"vpc\", \"quotaCode\": \"L-F678F1CE\", \"value\": 15, \"region\": \"ca-central-1\" }, { \"accountKey\": \"shared-network\", \"limitKey\": \"Amazon VPC/Interface VPC endpoints per VPC\", \"serviceCode\": \"vpc\", \"quotaCode\": \"L-29B6F2EB\", \"value\": 50, \"region\": \"ca-central-1\" } ] outputs.json based on the -Outputs table [ { \"accountKey\": \"shared-network\", \"outputKey\": \"DefaultBucketOutputC7CE5936\", \"outputValue\": \"{\\\"type\\\":\\\"AccountBucket\\\",\\\"value\\\":{\\\"bucketArn\\\":\\\"arn:aws:s3:::ASEA-sharednetwork-phase1-cacentral1-18vq0emthri3h\\\",\\\"bucketName\\\":\\\"ASEA-sharednetwork-phase1-cacentral1-18vq0emthri3h\\\",\\\"encryptionKeyArn\\\":\\\"arn:aws:kms:ca-central-1:0000000000001:key/d54a8acb-694c-4fc5-9afe-ca2b263cd0b3\\\",\\\"region\\\":\\\"ca-central-1\\\"}}\" } ] context.json that contains the default values for values that are otherwise passed as environment variables. { \"acceleratorName\": \"ASEA\", \"acceleratorPrefix\": \"ASEA-\", \"acceleratorExecutionRoleName\": \"ASEA-PipelineRole\", \"defaultRegion\": \"ca-central-1\" } config.json that contains the Accelerator configuration. The script also sets the default execution role to allow CDK to assume a role in subaccounts to deploy the phase stacks. Now that you have all the required local files you can deploy the phase stacks using cdk.sh . cd src/deployments/cdk ./cdk.sh deploy --phase 1 # deploy all phase 1 stacks ./cdk.sh deploy --phase 1 --parallel # deploy all phase 1 stacks in parallel ./cdk.sh deploy --phase 1 --account shared-network # deploy phase 1 stacks for account shared-network in all regions ./cdk.sh deploy --phase 1 --region ca-central-1 # deploy phase 1 stacks for region ca-central-1 for all accounts ./cdk.sh deploy --phase 1 --account shared-network --region ca-central-1 # deploy phase 1 stacks for account shared-network and region ca-central Other CDK commands are also available. cd src/deployments/cdk ./cdk.sh bootstrap --phase 1 ./cdk.sh synth --phase 1","title":"Phase Stacks"},{"location":"developer/development/#testing","text":"We use jest for unit testing. There are no integration tests but this could be set-up by configuring the Installer CodePipeline to have a webhook on the repository and deploying changes automatically. To run unit tests locally you can run the following command in the monorepo. pnpx recursive run test -- --pass-with-no-tests --silent See CDK's documentation on Testing constructs for more information on how to tests CDK constructs.","title":"Testing"},{"location":"developer/development/#validating-immutable-property-changes-and-logical-id-changes","text":"The most important unit test in this project is one that validates that logical IDs and immutable properties do not change unexpectedly. To avoid the issues described in section Resource Names and Logical IDs , Changing Logical IDs and Changing (Immutable) Properties . This test can be found in the src/deployments/cdk/test/apps/unsupported-changes.spec.ts file. It synthesizes the Phase stacks using mocked outputs and uses jest snapshots to compare against future changes. The test will fail when changing immutable properties or changing logical IDs of existing resources. In case the changes are expected then the snapshots will need to be updated. You can update the snapshots by running the following command. pnpx run test -- -u See Accept Unit Test Snapshot Changes .","title":"Validating Immutable Property Changes and Logical ID Changes"},{"location":"developer/development/#upgrade-cdk","text":"There's a test in the file src/deployments/cdk/test/apps/unsupported-changes.spec.ts that is currently commented out. The test takes a snapshot of the whole Phase stack and compares the snapshot to changes in the code. test('templates should stay exactly the same', () => { for (const [stackName, resources] of Object.entries(stackResources)) { // Compare the relevant properties to the snapshot expect(resources).toMatchSnapshot(stackName); } }); Before upgrading CDK we uncomment this test. We run the test to update all the snapshots. Then we update all CDK versions and run the test again to compare the snapshots with the code using the new CDK version. If the test passes, then the upgrade should be stable. Action Item: Automate this process.","title":"Upgrade CDK"},{"location":"developer/release-process/","text":"AWS Internal - Accelerator Release Process \u00b6 Creating a new Accelerator Code Release \u00b6 Ensure main branch is in a suitable state Disable branch protection for both the main branch and for the release/ branches Create a version branch with SemVer semantics and a release/ prefix: e.g. release/v1.0.5 or release/v1.0.5-b On latest main , run: git checkout -b release/vX.Y.Z Important: Certain git operations are ambiguous if tags and branches have the same name. Using the release/ prefix reserves the actual version name for the tag itself; i.e. every release/vX.Y.Z branch will have a corresponding vX.Y.Z tag. Push that branch to GitHub (if created locally) git push origin release/vX.Y.Z The release workflow will run, and create a DRAFT release if successful with all commits since the last tagged release. Prune the commits that have been added to the release notes (e.g. remove any low-information commits) Publish the release - this creates the git tag in the repo and marks the release as latest. It also bumps the version key in several project package.json files. Re-enable branch protection for both the main branch and for the release/ branches Note: The Publish operation will run the following GitHub Action , which merges the release/vX.Y.Z branch to main . Branch Protection in GitHub will cause this to fail , and why we are momentarily disabling branch protection. A successful run of this workflow will automatically kick off the \"Generate Documentation\" workflow. This workflow may also be initiated at any time manually via the GitHub Actions UI (since it is configured as a workflow_dispatch action). once the documentation is generated, add the ZIP file to the release assets, named AWS-SEA-Documentation-vXXX.zip Rename the AcceleratorInstaller.template.json to AcceleratorInstaller XXX .template.json replacing XXX with the version number without punctuation (i.e. AcceleratorInstaller121b.template.json ). Repeat for AcceleratorInstaller-CodeCommit.template-vXXX.json . Add the Accelerator configuration file schema documentation ZIP to the release assets, named AWS-SEA-Config-Schema-vXXX-DRAFT.zip . Add the Accelerator Alpha GUI ZIP to the release assets, named AWS-SEA-GUI-mockup-DoNotUse-vXXX-alpha.zip . ...Return to Accelerator Table of Contents","title":"Release Process"},{"location":"developer/release-process/#aws-internal-accelerator-release-process","text":"","title":"AWS Internal - Accelerator Release Process"},{"location":"developer/release-process/#creating-a-new-accelerator-code-release","text":"Ensure main branch is in a suitable state Disable branch protection for both the main branch and for the release/ branches Create a version branch with SemVer semantics and a release/ prefix: e.g. release/v1.0.5 or release/v1.0.5-b On latest main , run: git checkout -b release/vX.Y.Z Important: Certain git operations are ambiguous if tags and branches have the same name. Using the release/ prefix reserves the actual version name for the tag itself; i.e. every release/vX.Y.Z branch will have a corresponding vX.Y.Z tag. Push that branch to GitHub (if created locally) git push origin release/vX.Y.Z The release workflow will run, and create a DRAFT release if successful with all commits since the last tagged release. Prune the commits that have been added to the release notes (e.g. remove any low-information commits) Publish the release - this creates the git tag in the repo and marks the release as latest. It also bumps the version key in several project package.json files. Re-enable branch protection for both the main branch and for the release/ branches Note: The Publish operation will run the following GitHub Action , which merges the release/vX.Y.Z branch to main . Branch Protection in GitHub will cause this to fail , and why we are momentarily disabling branch protection. A successful run of this workflow will automatically kick off the \"Generate Documentation\" workflow. This workflow may also be initiated at any time manually via the GitHub Actions UI (since it is configured as a workflow_dispatch action). once the documentation is generated, add the ZIP file to the release assets, named AWS-SEA-Documentation-vXXX.zip Rename the AcceleratorInstaller.template.json to AcceleratorInstaller XXX .template.json replacing XXX with the version number without punctuation (i.e. AcceleratorInstaller121b.template.json ). Repeat for AcceleratorInstaller-CodeCommit.template-vXXX.json . Add the Accelerator configuration file schema documentation ZIP to the release assets, named AWS-SEA-Config-Schema-vXXX-DRAFT.zip . Add the Accelerator Alpha GUI ZIP to the release assets, named AWS-SEA-GUI-mockup-DoNotUse-vXXX-alpha.zip . ...Return to Accelerator Table of Contents","title":"Creating a new Accelerator Code Release"},{"location":"developer/tech-stack/","text":"Technology Stack \u00b6 We use TypeScript, NodeJS, CDK and CloudFormation. You can find some more information in the sections below. TypeScript and NodeJS \u00b6 In the following sections we describe the tools and libraries used along with TypeScript. pnpm \u00b6 We use the pnpm package manager along with pnpm workspaces to manage all the packages in this monorepo. https://pnpm.js.org https://pnpm.js.org/en/workspaces The binary pnpx runs binaries that belong to pnpm packages in the workspace. https://pnpm.js.org/en/pnpx-cli prettier \u00b6 We use prettier to format code in this repository. A GitHub action makes sure that all the code in a pull requests adheres to the configured prettier rules. See Github Actions . eslint \u00b6 We use eslint as a static analysis tool that checks our TypeScript code. A GitHub action makes sure that all the code in a pull requests adheres to the configured eslint rules. See Github Actions . CloudFormation \u00b6 CloudFormation deploys both the Accelerator stacks and resources and the deployed stacks and resources. See Operations Guide: System Overview for the distinction between Accelerator resources and deployed resources. CDK \u00b6 AWS CDK defines the cloud resources in a familiar programming language. While AWS CDK supports TypeScript, JavaScript, Python, Java, and C#/.Net, the contributions should be made in Typescript, as outlined in the Accelerator Development First Principles . Developers can use programming languages to define reusable cloud components known as Constructs. You compose these together into Stacks and Apps. Learn more at https://docs.aws.amazon.com/cdk/latest/guide/home.html","title":"Tech Stack"},{"location":"developer/tech-stack/#technology-stack","text":"We use TypeScript, NodeJS, CDK and CloudFormation. You can find some more information in the sections below.","title":"Technology Stack"},{"location":"developer/tech-stack/#typescript-and-nodejs","text":"In the following sections we describe the tools and libraries used along with TypeScript.","title":"TypeScript and NodeJS"},{"location":"developer/tech-stack/#pnpm","text":"We use the pnpm package manager along with pnpm workspaces to manage all the packages in this monorepo. https://pnpm.js.org https://pnpm.js.org/en/workspaces The binary pnpx runs binaries that belong to pnpm packages in the workspace. https://pnpm.js.org/en/pnpx-cli","title":"pnpm"},{"location":"developer/tech-stack/#prettier","text":"We use prettier to format code in this repository. A GitHub action makes sure that all the code in a pull requests adheres to the configured prettier rules. See Github Actions .","title":"prettier"},{"location":"developer/tech-stack/#eslint","text":"We use eslint as a static analysis tool that checks our TypeScript code. A GitHub action makes sure that all the code in a pull requests adheres to the configured eslint rules. See Github Actions .","title":"eslint"},{"location":"developer/tech-stack/#cloudformation","text":"CloudFormation deploys both the Accelerator stacks and resources and the deployed stacks and resources. See Operations Guide: System Overview for the distinction between Accelerator resources and deployed resources.","title":"CloudFormation"},{"location":"developer/tech-stack/#cdk","text":"AWS CDK defines the cloud resources in a familiar programming language. While AWS CDK supports TypeScript, JavaScript, Python, Java, and C#/.Net, the contributions should be made in Typescript, as outlined in the Accelerator Development First Principles . Developers can use programming languages to define reusable cloud components known as Constructs. You compose these together into Stacks and Apps. Learn more at https://docs.aws.amazon.com/cdk/latest/guide/home.html","title":"CDK"},{"location":"faq/","text":"Accelerator Basic Operation and Frequently asked Questions \u00b6 Operational Activities \u00b6 How do I add new AWS accounts to my AWS Organization? \u00b6 We offer four options and all can be used in the same Accelerator deployment. All options work with AWS Control Tower, ensuring the account is both ingested into Control Tower and all Accelerator guardrails are automatically applied: Users can simply add the following five lines to the configuration file workload-account-configs section and rerun the state machine. The majority of the account configuration will be picked up from the ou the AWS account has been assigned. You can also add additional account specific configuration, or override items like the default ou budget with an account specific budget. This mechanism is often used by customers that wish to programmatically create AWS accounts using the Accelerator and allows for adding many new accounts at one time. json \"fun-acct\": { \"account-name\": \"TheFunAccount\", \"email\": \"myemail+aseaT-funacct@example.com\", \"src-filename\": \"config.json\", \"ou\": \"Sandbox\" } We've heard consistent feedback that our customers wish to use native AWS services and do not want to do things differently once security controls, guardrails, or accelerators are applied to their environment. In this regard, simply create your new AWS account in AWS Organizations as you did before**, either by a) using the AWS Console or b) by using standard AWS account creation API's, CLI or 3rd party tools like Terraform. ** IMPORTANT: When creating the new AWS account using AWS Organizations, you need to specify the role name provided in the Accelerator configuration file global-options\\organization-admin-role , otherwise we cannot bootstrap the account. In Control Tower installations, this MUST be set to AWSControlTowerExecution , for customers who installed prior to v1.2.5 this value is AWSCloudFormationStackSetExecutionRole and after v1.2.5 we were recommending using the role OrganizationAccountAccessRole as this role is used by default by AWS Organizations if no role name is specified when creating AWS accounts through the AWS console or cli. On account creation we will apply a quarantine SCP which prevents the account from being used by anyone until the Accelerator has applied the appropriate guardrails Moving the account into the appropriate OU triggers the state machine and the application of the guardrails to the account, once complete, we will remove the quarantine SCP. NOTE: Accounts CANNOT be moved between OU's to maintain compliance, so select the proper top-level OU with care In AWS Organizations, select ALL the newly created AWS accounts and move them all (preferably at once) to the correct destination OU (assuming the same OU for all accounts) In case you need to move accounts to multiple OU's we have added a 2 minute delay before triggering the State Machine Any accounts moved after the 2 minute window will NOT be properly ingested, and will need to be ingested on a subsequent State Machine Execution. Create your account using Account Factory in the AWS Control Tower console. No matter the mechanism you choose, new accounts will automatically be blocked from use until fully guardrailed, the Accelerator will automatically execute, and accounts will automatically be ingested into AWS Control Tower. I tried to enroll a new account via Control Tower but it failed? The state machine failed during the \"Load Organization Configuration\" step with the error \"The Control Tower account: ACCOUNT_NAME is in a failed state ERROR\"? \u00b6 If account enrollment fails within Control Tower, you will need to follow the troubleshooting steps here . A common reason for this is not having the ControlTowerExectution role created in the account you are trying to enroll. Even after you successfully enroll the account, it is possible the state machine will fail at Load Organization Configuration . If you look at the cloudwatch logs you will see the error message: There were errors while loading the configuration: The Control Tower account: ACCOUNT_NAME is in a failed state ERROR. This is because the Accelerator checks that there are no errors with Control Tower before continuing. In some cases Control Tower can leave an orphaned Service Catalog product in an Error state. You need to cleanup Control Towers Service Catalogs Provisioned Products so there are no products remaining in an error or tainted state before you can successfully re-run the state machine. Can I use AWS Organizations for all tasks I currently use AWS Organizations for? \u00b6 In AWS Organizations you can continue to: create and rename AWS accounts move AWS accounts between ou's create, delete and rename ou's, including support for nested ou's create, rename, modify, apply and remove SCP's What can't I do: modify Accelerator or Control Tower controlled SCP's add/remove SCP's on top-level OU's (these are Accelerator and/or Control Tower controlled) users can change SCP's on non-top-level ou's and non-Accelerator controlled accounts as they please add/remove SCP's on specific accounts that have Accelerator controlled SCPs move an AWS account between top-level ou's (i.e. Sandbox to Prod is a security violation) moving between Prod/sub-ou-1 to Prod/sub-ou2 or Prod/sub-ou2/sub-ou2a/sub-ou2ab is fully supported create a top-level ou (need to validate, as they require config file entries) remove quarantine SCP from newly created accounts we do not support forward slashes ( / ) in ou names, even though the AWS platform does More details: If an AWS account is renamed, an account email is changed, or an OU is renamed, on the next state machine execution, the config file will automatically be updated. If you edit an Accelerator controlled SCP through Organizations, we will reset it per what is defined in the Accelerator configuration files. If you add/remove an SCP from a top-level ou or Accelerator controlled account, we will put them back as defined in the Accelerator configuration file. If you move an account between top-level ou's, we will put it back to its original designated top-level ou. The Accelerator fully supports nested ou's, customers can create any depth ou structure in AWS Organizations and add/remove/change SCP's below the top-level as they desire or move accounts between these ou's without restriction. Users can create ou's to the full AWS ou structure/depth Except for the Quarantine SCP applied to specific accounts, we do not 'control' SCP's below the top level, customers can add/create/customize SCP's as of v1.3.3 customers can optionally control account level SCP's through the configuration file How do I make changes to items I defined in the Accelerator configuration file during installation? \u00b6 Simply update your configuration file in CodeCommit and rerun the state machine! In most cases, it is that simple. If you ask the Accelerator to do something that is not supported by the AWS platform, the state machine will fail, so it needs to be a supported capability. For example, the platform does not allow you to change the CIDR block on a VPC, but you can accomplish this as you would today by using the Accelerator to deploy a new second VPC, manually migrating workloads, and then removing the deprecated VPC from the Accelerator configuration. Below we have also documented additional considerations when creating or updating the configuration file. It should be noted that we have added code to the Accelerator to block customers from making many 'breaking' or impactful changes to their configuration files. If someone is positive they want to make these changes, we also provide override switches to allow these changes to be attempted forcefully. Can I update the config file while the State Machine is running? When will those changes be applied? \u00b6 Yes. The state machine captures a consistent input state of the requested configuration when it starts. The running Accelerator instance does not see or consider any configuration changes that occur after it has started. All configuration changes occurring after the state machine is running will only be leveraged on the next state machine execution. What if I really mess up the configuration file? \u00b6 The Accelerator is designed with checks to compare your current configuration file with the version of the config file from the previous successful execution of the state machine. If we believe you are making major or breaking changes to the config file, we will purposefully fail the state machine. See 1.4. Config file and Deployment Protections for more details. With the release of v1.3.0 we introduced state machine scoping capabilities to further protect customers, detailed here What if my State Machine fails? Why? Previous solutions had complex recovery processes, what's involved? \u00b6 If your main state machine fails, review the error(s), resolve the problem and simply re-run the state machine. We've put a huge focus on ensuring the solution is idempotent and to ensure recovery is a smooth and easy process. Ensuring the integrity of deployed guardrails is critical in operating and maintaining an environment hosting protected data. Based on customer feedback and security best practices, we purposely fail the state machine if we cannot successfully deploy guardrails. Additionally, with millions of active customers each supporting different and diverse use cases and with the rapid rate of evolution of the AWS platform, sometimes we will encounter unexpected circumstances and the state machine might fail. We've spent a lot of time over the course of the Accelerator development process ensuring the solution can roll forward, roll backward, be stopped, restarted, and rerun without issues. A huge focus was placed on dealing with and writing custom code to manage and deal with non-idempotent resources (like S3 buckets, log groups, KMS keys, etc.). We've spent a lot of time ensuring that any failed artifacts are automatically cleaned up and don't cause subsequent executions to fail. We've put a strong focus on ensuring you do not need to go into your various AWS sub-accounts and manually remove or cleanup resources or deployment failures. We've also tried to provide usable error messages that are easy to understand and troubleshoot. As new scenario's are brought to our attention, we continue to adjust the codebase to better handle these situations. Will your state machine fail at some point in time, likely. Will you be able to easily recover and move forward without extensive time and effort, YES! How do I update some of the supplied sample configuration items found in reference-artifact, like SCPs and IAM policies? \u00b6 To override items like SCP's or IAM policies, customers simply need to provide the identically named file in there input bucket. As long as the file exists in the correct folder in the customers input bucket, the Accelerator will use the customers supplied version of the configuration item, rather than the Accelerator version. Customer SCP's need to be placed into a folder named scp and iam policies in a folder named iam-policy (case sensitive). The Accelerator was designed to allow customers complete customization capabilities without any requirement to update code or fork the GitHub repo. Additionally, rather than forcing customers to provide a multitude of config files for a standard or prescriptive installation, we provide and auto-deploy with Accelerator versions of most required configuration items from the reference-artifacts folder of the repo. If a customer provides the required configuration file in their Accelerator S3 input bucket, we will use the customer supplied version of the configuration file rather than the Accelerator version. At any time, either before initial installation, or in future, a customer can place new or updated SCPs, policies, or other supported file types into their input bucket and we will use those instead of or in addition to Accelerator supplied versions. Customer only need to provide the specific files they wish to override, not all files. Customers can also define additional SCPs (or modify existing SCPs) using the name, description and filename of their choosing, and deploy them by referencing them on the appropriate organizational unit in the config file. Prior to v1.2.5, if we updated the default files, we overwrote customers customizations during upgrade. Simply updating the timestamp after upgrade on the customized versions and then rerunning the state machine re-instates customer customizations. In v1.2.5 we always use the customer customized version from the S3 bucket. It's important customers assess newly provided defaults during an upgrade process to ensure they are incorporating all the latest fixes and improvements. If a customer wants to revert to Accelerator provided default files, they will need to manually copy it from the repo into their input bucket. NOTE: Most of the provided SCPs are designed to protect the Accelerator deployed resources from modification and ensure the integrity of the Accelerator. Extreme caution must be exercised if the provided SCPs are modified. In v1.5.0 we restructured the SCPs based on a) customer requests, and b) the addition of Control Tower support for new installs. we reorganized and optimized our SCP's from 4 SCP files down to 3 SCP files, without removing any protections or guardrails; these optimizations have resulted in minor enhancements to the SCP protections and in some cases better scoping; the first two SCP files (Part-0 and Part-1) contain the controls which protect the integrity of the Accelerator itself; the third file (Sensitive, Unclass, Sandbox) contains customer data protection specific guardrails, which may change based on workload data classification or customer profiles and requirements; this freed the fourth SCP for use by Control Tower. As Control Tower leverages 2 SCP files on the Security OU, we have moved some of our SCP's to the account level. I deployed AWS Managed Active Directory (MAD) as part of my deployment, how do I manage Active Directory domain users, groups, and domain policies after deployment? \u00b6 Customers have clearly indicated they do NOT want to use the Accelerator to manage their Active Directory domain or change the way they manage Active Directory on an ongoing basis. Customer have also indicated, they need help getting up and running quickly. For these reasons, the Accelerator only sets the domain password policy, and creates AD users and groups on the initial installation of MAD. After the initial installation, customers must manage Windows users and groups using their traditional tools. A bastion Windows host is deployed as a mechanism to support these capabilities. Passwords for all newly created MAD users have been stored, encrypted, in AWS Secrets Manager in the Management (root) Organization AWS account. To create new users and groups: RDP into the ASEA-RDGW bastion host in the Ops account Run ADUC and create users and groups as you please under the NETBIOSDOMAIN (example) tree Or run the appropriate powershell command Go to AWS SSO and map the Active Directory group to the appropriate AWS account and permission set The Accelerator will not create/update/delete new AD users or groups, nor will it update the domain password policy after the initial installation of Managed Active Directory. It is your responsibility to rotate these passwords on a regular basis per your organizations password policy. (NOTE: After updating the admin password it needs to be stored back in secrets manager). How do I suspend an AWS account? \u00b6 Suspending accounts is blocked via SCP and purposely difficult, two options exist: Modify SCP method (not desired) Leverage the UnManaged OU validate your config file contains the value: \"ignored-ous\": [\"UnManaged\"] the state machine must be executed at least once after this value is added to the config file In AWS Organizations create an OU named UnManaged in the root of the OU tree, if it does not exist Change to the us-east-1 region and open CloudWatch and navigate to Rules Select the PBMMAccel-MoveAccount_rule , select actions, select Disable In Organizations move the account to be suspended to the UnManaged OU Change to the us-east-1 region and open CloudWatch and navigate to Rules Select the PBMMAccel-MoveAccount_rule , select actions, select Enable login to the account to be suspended as the account root user suspend the account through My Account Run the state machine (from the Organization management account), the account will: have a deleted=true value added to the config file be moved to the suspended OU (OU value and path stays the same in the config file) deleted=true causes OU validation to be skipped on this account on subsequent SM executions If the AWS account was listed in the mandatory-accounts section of the config file the SM will fail (expected) after the above tasks have been completed, remove all references to the suspended mandatory account from the config file rerun the state machine, specifying: { \"overrideComparison\": true } Deleted accounts will continue to appear under the Suspended OU for 90-days I need a new VPC, where shall I define it? \u00b6 You can define a VPC in one of four major sections of the Accelerator configuration file: within an organization unit (this is the recommended and preferred method); within an account in mandatory-account-configs; within an account in workload-account-configs; defined within an organization unit, but opted-in within the account config. We generally recommend most items be defined within organizational units, such that all workload accounts pickup their persona from the OU they are associated and minimize per account configuration. Both a local account based VPC (as deployed in the Sandbox OU accounts), or a central shared VPC (as deployed in the Dev/Test/Prod OU accounts in many of the example configs) can be defined at the OU level. As mandatory accounts often have unique configuration requirements, for example the centralized Endpoint VPC, they must be configured within the accounts configuration. Customers can define VPC's or other account specific settings within any accounts configuration, but this requires editing the configuration file for each account configuration. Prior to v1.5.0, local VPC's defined at the OU level were each deployed with the same CIDR ranges and therefor could not be connected to a TGW. Local VPC's requiring centralized networking (i.e. TGW connectivity) were required to be defined in each account config, adding manual effort and bloating the configuration file. The addition of dynamic and lookup CIDR sources in v1.5.0 resolves this problem. Local VPCs can be defined in an OU, and each VPC will be dynamically assigned a unique CIDR range from the assigned CIDR pool, or looked up from the DynamoDB database. Customers can now ensure connected, templated VPCs are consistently deployed to every account in an OU, each with unique IP addresses. v1.5.0 also added a new opt-in VPC capability. A VPC is defined in an OU and a new config file variable is added to this VPC opt-in: true . When opt-in is set to true, the state machine does NOT create the VPC for the accounts in the OU, essentially ignoring the VPC definition. Select accounts in the OU can then be opted-in to the VPC(s) definition, by adding the value accountname\\opt-in-vpcs: [\u201copt-in-vpc-name1\u201d, \u201copt-in-vpc-name2\u201d, \u201copt-in-vpc-nameN\u201d] to the specific accounts which need the VPC(s). A vpc definition with the specified name (i.e. opt-in-vpc-name1 ) and the value opt-in: true , must exist in the ou config for the specified account. When these conditions apply, the VPC will be created in the account per the OU definition. Additional opt-in VPCs can be added to an account, but VPC's cannot be removed from the opt-in-vpcs array. VPC's can be TGW attached, assuming dynamic cidr-src is utilized, or DynamoDB is prepopulated with the required CIDR ranges using lookup mode. cidr-src provided is suitable for disconnected Sandbox type accounts. The Future: While Opt-In VPCs are powerful, we want to take this further. Why not deploy an AWS Service Catalog template which contains the names of all the available opt-in VPCs for the accounts OU, inside each account. An account end user could then request a new VPC for their account from the list of available opt-in patterns. A users selection would be sent to a centralized queue for approval (w/auto-approval options), which would result in the opt-in-vpc entry in that account being updated with the end users requested vpc pattern and the personalized VPC being created in the account and attached to the centralized TGW (if part of the pattern). This would ensure all VPC's conformed to a set of desirable design patterns, but also allow the end-user community choices based on their desired development and app patterns. If you like this idea, please +1 this feature request. How do I modify and extend the Accelerator or execute my own code after the Accelerator provisions a new AWS account or the state machine executes? \u00b6 Flexibility: The AWS Secure Environment Accelerator was developed to enable extreme flexibility without requiring a single line of code to be changed. One of our primary goals throughout the development process was to avoid making any decisions that would result in users needing to fork or branch the Accelerator codebase. This would help ensure we had a sustainable and upgradable solution for a broad customer base over time. Functionality provided by the Accelerator can generally be controlled by modifying the main Accelerator configuration file. Items like SCP's, rsyslog config, Powershell scripts, and iam-policies have config files provided and auto-deployed as part of the Accelerator to deliver on the prescriptive architecture (these are located in the \\reference-artifacts folder of the Github repo for reference). If you want to alter the functionality delivered by any of these additional config files, you can simply provide your own by placing it in your specified Accelerator bucket in the appropriate sub-folder. The Accelerator will use your provided version instead of the supplied repo reference version. As SCP's and IAM policies are defined in the main config file, you can simply define new policies, pointing to new policy files, and provide these new files in your bucket, and they will be used. While a sample firewall config file is provided in the \\reference-artifacts folder, it must be manually placed in your s3 bucket/folder on new Accelerator deployments Any/all of these files can be updated at any time and will be used on the next execution of the state machine Over time, we predict we will provide several sample or reference architectures and not just the current single PBMM architecture (all located in the \\reference-artifacts\\SAMPLE_CONFIGS folder). Extensibility: Every execution of the state machine sends a state machine status event to a state machine SNS topic These status events include the Success/Failure status of the state machine, and on success, a list of all successfully processed AWS accounts While this SNS topic is automatically subscribed to a user provided email address for user notification, users can also create additional SNS subscriptions to enable triggering their own subsequent workflows, state machines, or custom code using any supported SNS subscription type (Lambda, SQS, Email, HTTPS, HTTPS) Additionally, objects deployed within an account have been populated in Parameter Store, see answer 1.3.2 for details Example: One of our early adopter customers has developed a custom user interface which allows their clients to request new AWS environments. Clients provide items like cost center, budget, and select their environment requirements (i.e. Sandbox, Unclass or full sensitive SDLC account set). On appropriate approval, this pushes the changes to the Accelerator configuration file and triggers the state machine. Once the state machine completes, the SNS topic triggers their follow-up workflow, validates the requested accounts were provisioned, updates the customer's account database, and then executes a collection of customer specific follow-up workflow actions on any newly provisioned accounts. How can I easily access my virtual machines or EC2 instances? \u00b6 The preferred and recommended method to connect to instances within the Accelerator is by using AWS Systems Manager Session Manager. Session Manager allows access to instances without the need to have any open firewall ports. Session Manager allows for Command line access to instances (both Windows and Linux) directly through the AWS console, with all activity logged to CloudWatch Logs. Session Manager enables customers to connect to Windows instances with a native RDP client and Linux instances with a native SSH client, if desired. Customers can gain quick access to instances through the AWS consolve, or using their preferred clients. General Both the RDGW and rsyslog instances deployed in the Ops account are properly configured for Systems Manager Session Manager We have implemented automation such that all instances are also automatically configured for Session Manager (i.e. configured with the appropriate launch role, has a recent session manager agent installed (most amazon ami's do), has access to an SSM endpoint) Connecting to an Instance From the AWS Console Go to: EC2, Instances, select the instance (i.e. ASEA-RDGW), click \u201cConnect\u201d, select Session Manager, Connect Ideal for Linux or Windows Powershell users Everything is fully logged for future reference Directly through local RDP client using Session Managers tunnel capability: Provides direct access to your instances/host without any open internet ports on the RDGW instance, using a local/fat client tool Install AWS CLI v2 on your PC - available here (uninstall CLIv1 first, if installed) Install the SSM plugin on your PC - available here Get AWS temporary credentials from AWS SSO for the account your workload resides (i.e. Ops account when accessing the ASEA-RDGW instance) by selecting \u201cCommand line or programmatic access\u201d instead of \u201cManagement Console\u201d and paste them into a command prompt i.e. via logging in here: https://xxxxxxx.awsapps.com/start or This blog describes the process to use SSO to get credentials for the AWS CLI directly without the GUI Then enter: aws ssm start-session --target \"i-12345678901234567\" --document-name AWS-StartPortForwardingSession --parameters portNumber=\"3389\",localPortNumber=\"56789\"--region ca-central-1 Command syntax is slightly different on Linux/Mac Replace i-1111adddce582b23c with the instance id of your RDGW instance A tunnel will open As these are tunnels to proprietary protocols (i.e. RDP/screen scraping) session content is not logged. Run mstsc/rdp client and connect to 127.0.0.1:56789 By replacing 3389 with a new port for another applications (i.e. SSH running on a Linux instance), you can connect to a different application type You can change the local port by changing 56789 to any other valid port number (i.e. connecting to multiple instances at the same time) Login with the windows credentials discussed above in the format NETBIOSDOMAIN\\User1 (i.e. example\\user1) Your netbios domain is found here in your config file: \"netbios-domain\": \"example\", Connect to your desktop command line to command line interface of remote Windows or Linux servers, instead of through console (i.e. no tunnel): aws ssm start-session --target \"i-090c25e64c2d9d276\"\"--region ca-central-1 Replace i-xxx with your instance ID Everything is fully logged for future reference If you want to remove the region from your command line, you can: Type: \u201caws configure\u201d from command prompt, hit {enter} (key), {enter} (secret), enter: ca-central-1, {enter} I ran the state machine but it failed when it tried to delete the default VPC? The state machine cannot delete the default VPC (Error: VPC has dependencies and cannot be deleted) \u00b6 You need to ensure that resources don\u2019t exist in the default VPC or else the state machine won't be able to delete it. If you encounter this error, you can either delete the resources within the VPC or delete the default VPC manually and run the state machine again. Existing Accounts/Organizations \u00b6 How do I import an existing AWS account into my Accelerator managed AWS Organization (or what if I created a new AWS account with a different Organization trust role)? \u00b6 Ensure you have valid administrative privileges for the account to be invited/added Add the account to your AWS Organization using standard processes (i.e. Invite/Accept) this process does NOT create an organization trust role imported accounts do NOT have the quarantine SCP applied as we don't want to break existing workloads Login to the account using the existing administrative credentials Execute the Accelerator provided CloudFormation template to create the required Accelerator bootstrapping role - in the Github repo here: reference-artifacts\\Custom-Scripts\\Import-Account-CFN-Role-Template.yml add the account to the Accelerator config file and run the state machine If you simply created the account with an incorrect role name, you likely need to take extra steps: Update the Accelerator config file to add the parameter: global-options\\ignored-ous = [\"UnManagedAccounts\"] In AWS Organizations, create a new OU named UnManagedAccounts (case sensitive) Move the account to the UnManagedAccounts ou You can now remove the Quarantine SCP from the account Assume an administrative role into the account Execute the Accelerator provided CloudFormation template to create the required Accelerator bootstrapping role Is it possible to deploy the Accelerator on top of an AWS Organization that I have already installed the AWS Landing Zone (ALZ) solution into? \u00b6 Existing ALZ customers are required to uninstall their ALZ deployment before deploying the Accelerator. Please work with your AWS account team to find the best mechanism to uninstall the ALZ solution (procedures and scripts exist). Additionally, please reference section 4 of the Instation and Upgrade Guide. It may be easier to migrate AWS accounts to a new Accelerator Organization, per the process detailed in FAQ #1.2.3. What if I want to move an account from an AWS Organization that has the ALZ deployed into an AWS Organization running the Accelerator? \u00b6 Before removing the AWS account from the source organization, terminate the AWS Service Catalog product associated with the member account that you're interested in moving. Ensuring the product terminates successfully and that there aren't any remaining CloudFormation stacks in the account that were deployed by the ALZ. You can then remove the account from the existing Organization and invite it into the new organization. Accounts invited into the Organization do NOT get the Deny All SCP applied, as we do not want to break existing running workloads. Moving the newly invited account into its destination OU will trigger the state machine and result in the account being ingested into the Accelerator and having the guardrails applied per the target OU persona. For a detailed procedure, please review this document . End User Environment \u00b6 Is there anything my end users need to be aware of? Why do some of my end users struggle with CloudWatch Log groups errors? \u00b6 CloudWatch Log group deletion is prevented for security purposes and bypassing this rule would be a fundamental violation of security best practices. This protection does NOT exist solely to protect ASEA logs, but ALL log groups. Users of the Accelerator environment will need to ensure they set CloudFormation stack Log group retention type to RETAIN, or stack deletes will fail when attempting to delete a stack (as deleting the log group will be blocked) and users will encounter errors. As repeated stack deployments will be prevented from recreating the same log group name (as it already exists), end users will either need to check for the existence of the log group before attempting creation, or include a random hash in the log group name. The Accelerator also sets log group retention for all log groups to value(s) specified by customers in the config file and prevents end users from setting or changing Log group retentions. When creating new log groups, end users must either not configure a retention period, or set it to the default NEVER expire or they will also be blocked from creating the CloudWatch Log group. If applied by bypassing the guardrails, customer specified retention periods on log group creation will be overridden with the Accelerator specified retention period. While a security best practice, some end users continue to request this be changed, but you need to ask: Are end users allowed to go in and clean out logs from Windows Event Viewer (locally or on domain controllers) after testing? Clean out Linux kernel logs? Apache log histories? The fundamental principal is that all and as many logs as possible will be retained for a defined retention period (some longer). In the \"old days\", logs were hidden deep within OS directory structures or access restricted by IT from developers - now that we make them all centralized, visible, and accessible, end users seem to think they suddenly need to clean them up. Customers need to establish a usable and scalable log group naming standard/convention as the first step in moving past this concern, such that they can always find their active logs easily. As stated, to enable repeated install and removal of stacks during test cycles, end user CloudFormation stacks need to set log groups to RETAIN and leverage a random hash in log group naming (or check for existence, before creating). The Accelerator provided SCPs (guardrails/protections) are our recommendations, yet designed to be fully customizable, enabling any customer to carefully override these defaults to meet their individual requirements. If insistent, we'd suggest only bypassing the policy on the Sandbox OU, and only for log groups that start with a very specific prefix (not all log groups). When a customer wants to use the delete capability, they would need to name their log group with the designated prefix - i.e. opt-in to allow CloudWatch log group deletes. How can I leverage Accelerator deployed objects in my IaC? Do I need to manually determine the arn's and object id's of Accelerator deployed objects to leverage them in my IaC? \u00b6 Objects deployed by the Accelerator which customers may need to leverage in their own IaC have been populated in parameters in AWS parameter store for use by the IaC tooling of choice. The Accelerator ensures parameters are deployed consistently across accounts and OUs, such that a customers code does not need to be updated when it is moved between accounts or promoted from Dev to Test to Prod. Objects of the following types and their associated values are stored in parameter store: vpc, subnet, security group, elb (alb/nlb w/DNS address), IAM policy, IAM role, KMS key, ACM cert, SNS topic, and the firewall replacement variables. Additionally, setting \"populate-all-elbs-in-param-store\": true for an account will populates all Accelerator wide ELB information into parameter store within that account. The sample PBMM configuration files set this value on the perimeter account, such that ELB information is available to configure centralized ingress capabilities. Upgrades \u00b6 Can I upgrade directly to the latest release, or must I perform upgrades sequentially? \u00b6 Yes, currently customers can upgrade from whatever version they have deployed to the latest Accelerator version. There is no requirement to perform sequential upgrades. In fact, we strongly discourage sequential upgrades. Given the magnitude of the v1.5.0 release, we have added a one-time requirement that all customers upgrade to a minimum of v1.3.8 before attempting to upgrade to v1.5.0. Why do I get the error \"There were errors while comparing the configuration changes:\" when I update the config file? \u00b6 In v1.3.0 we added protections to allow customers to verify the scope of impact of their intended changes to the configuration file. In v1.3.0 and above, the state machine does not allow changes to the config file (other than new accounts) without providing the scope parameter. Please refer to section 1.1 of the State Machine behavior and inputs Guide for more details. Support Concerns \u00b6 The Accelerator is written in CDK and deploys CloudFormation, does this restrict the Infrastructure as Code (IaC) tools that I can use? \u00b6 No. Customers can choose the IaC framework or tooling of their choice. The tooling used to deploy the Accelerator has no impact on the automation framework customers use to deploy their applications within the Accelerator environment. It should be noted that the functionality deployed by the Accelerator is extremely platform specific and would not benefit from multi-platform IaC frameworks or tooling. What happens if AWS stops enhancing the Accelerator? \u00b6 The Accelerator is an open source project, should AWS stop enhancing the solution for any reason, the community has access to the full codebase, its roadmap and history. The community can enhance, update, fork and take ownership of the project, as appropriate. The Accelerator is an AWS CDK based project and synthesizes to native AWS CloudFormation. AWS sub-accounts simply contain native CloudFormation stacks and associated custom resources, when required. The Accelerator architecture is such that all CloudFormation stacks are native to each AWS account with no links or ties to code in other AWS accounts or even other stacks within the same AWS account. This was an important initial design decision. The Accelerator codebase can be completely uninstalled from the organization management (root) account, without any impact to the deployed functionality or guardrails. In this situation, guardrail updates and new account provisioning reverts to a manual process. Should a customer decide they no longer wish to utilize the solution, they can remove the Accelerator codebase without any impact to deployed resources and go back to doing things natively in AWS as they did before they deployed the Accelerator. By adopting the Accelerator, customers are not locking themselves in or making a one-way door decision. What level of Support will the ASEA have from AWS Support? \u00b6 The majority of the solution leverages native AWS services which are fully supported by AWS Support. Additionally, the Accelerator is an AWS CDK based project and synthesizes to native AWS CloudFormation. AWS sub-accounts simply contain native CloudFormation stacks and associated custom resources (when required). The Accelerator architecture is such that all CloudFormation stacks are native to each AWS account with no direct links or ties to code in other AWS accounts (no stacksets, no local CDK). This was an important project design decision, keeping deployed functionality in independent local CloudFormation stacks and decoupled from solution code, which allows AWS support to effectively troubleshoot and diagnose issues local to the sub-account. As the Accelerator also includes code, anything specifically related to the Accelerator codebase will be only supported on a \"best effort\" basis by AWS support, as AWS support does not support custom code. The first line of support for the codebase is typically your local AWS team (your SA, TAM, Proserve and/or AWS Partner). As an open source project, customers can file requests using GitHub Issues against the Accelerator repository or open a discussion in GitHub discussions. Most customer issues arise during installation and are related to configuration customization or during the upgrade process. What does it take to support the Accelerator? \u00b6 We advise customers to allocate a 1/2 day per quarter to upgrade to the latest Accelerator release. Customers have indicated that deploying the Accelerator reduces their ongoing operational burden over operating in native AWS, saving hours of effort every time a new account is provisioned by automating the deployment of the persona associated with new accounts (guardrails, networking and security). The Accelerator does NOT alleviate a customers requirement to learn to effectively operate in the cloud (like monitoring security tooling/carrying out Security Operation Center (SOC) duties). This effort exists regardless of the existence of the Accelerator. Is the Accelerator only designed and suitable for Government of Canada or PBMM customers? \u00b6 No. The Accelerator is targeted at any AWS customer that is looking to automate the deployment and management of a comprehensive end-to-end multi-account environment in AWS. It is ideally suited for customers interested in achieving a high security posture in AWS. The Accelerator is a sophisticated deployment framework that allows for the deployment and management of virtually any AWS multi-account \"Landing Zone\" architecture without any code modifications. The Accelerator is actually delivering two separate and distinct products which can each be used on their own: the Accelerator the tool, which can deploy virtually any architecture based on a provided config file (no code changes), and; the Government of Canada (GC) prescriptive PBMM architecture which is delivered as a sample configuration file and documentation. The tooling was purposely built to be extremely flexible, as we realized that some customers may not like some of the opinionated and prescriptive design decisions we made in the GC architecture. Virtually every feature being deployed can be turned on/off, not be used or can have its configuration adjusted to meet your specific design requirements. We are working on building a library of sample config files to support additional customer needs and better demonstrate product capabilities and different architecture patterns. In no way is it required that the prescriptive GC architecture be used or deployed. Just because we can deploy, for example, an AWS Managed Active Directory, does not mean you need to use that feature of the solution. Disabling or changing these capabilities also requires zero code changes. While the prescriptive sample configuration files were originally developed based on GC requirements, they were also developed following AWS Best Practices. Additionally, many security frameworks around the world have similar and overlapping security requirements (you can only do security so many ways). The provided architecture is applicable to many security compliance regimes around the world and not just the GC. Deployed Functionality \u00b6 I wish to be in compliance with the 12 GC TBS Guardrails, what don't you cover with the provided sample architecture? \u00b6 The AWS SEA allows for a lot of flexibility in deployed architectures. If used, the provided PBMM sample architecture was designed to help deliver on the technical portion of all 12 of the GC guardrails, when automation was possible. What don't we cover? Assigning MFA to users is a manual process. Specifically you need to procure Yubikeys for your root/break glass users, and enable a suitable form of MFA for all other users (i.e. virtual, email, other). The guardrails also include some organizational processes (i.e. break glass procedures, or signing an MOU with CCCS) which customers will need to work through independently. While AWS is providing the tools to help customer be compliant with the 12 PBMM guardrails (which were developed in collaboration with the GC) - it's up to each customers ITSec organization to assess and determine if the deployed controls actually meet their security requirements. Finally, while we started with a goal of delivering on the 12 guardrails, we believe we have extended well beyond those security controls, to further help customers move towards meeting the full PBMM technical control profile (official documentation is weak in this area at this time). Does the ALB perform SSL offloading? \u00b6 As configured - the perimeter ALB decrypts incoming traffic using its certificate and then re-encrypts it with the certificate for the back-end ALB. The front-end and back-end ALB's can use the same or different certs. If the Firewall needs to inspect the traffic, it also needs the backend certificate be manually installed. What is the recommended approach to manage the ALB certificates deployed by the Accelerator? \u00b6 The Accelerator installation process allows customers to provide their own certificates (either self-signed or generated by a CA), to enable quick and easy installation and allowing customers to test end-to-end traffic flows. After the initial installation, we recommend customers leverage AWS Certificate Manager (ACM) to easily provision, manage, and deploy public and private SSL/TLS certificates. ACM helps manage the challenges of maintaining certificates, including certificate rotation and renewal, so you don\u2019t have to worry about expiring certificates. The Accelerator provides 3 mechanisms to enable utilizing certificates with ALB's: Method 1 - IMPORT a certificate into AWS Certificate Manager from a 3rd party product When using a certificate that does not have a certificate chain (usually this is the case with Self-Signed) json \"certificates\": [ { \"name\": \"My-Cert\", \"type\": \"import\", \"priv-key\": \"certs/example1-cert.key\", \"cert\": \"certs/example1-cert.crt\" } ] When using a certificate that has a certificate chain (usually this is the case when signed by a Certificate Authority with a CA Bundle) json \"certificates\": [ { \"name\": \"My-Cert\", \"type\": \"import\", \"priv-key\": \"certs/example1-cert.key\", \"cert\": \"certs/example1-cert.crt\", \"chain\": \"certs/example1-cert.chain\" } ] this mechanism allows a customer to generate certificates using their existing tools and processes and import 3rd party certificates into AWS Certificate Manager for use in AWS Self-Signed certificates should NOT be used for production (samples were provided simply to demonstrate functionality) both a .key and a .crt file must be supplied in the customers S3 input bucket \"cert\" must contain only the certificate and not the full chain \"chain\" is an optional attribute that contains the certificate chain. This is generally used when importing a CA signed certificate this will create a certificate in ACM and a secret in secrets manager named accelerator/certificates/My-Cert in the specified AWS account(s), which points to the newly imported certificates ARN Method 2 - REQUEST AWS Certificate Manager generate a certificate json \"certificates\": [ { \"name\": \"My-Cert\", \"type\": \"request\", \"domain\": \"*.example.com\", \"validation\": \"DNS\", \"san\": [\"www.example.com\"] } ] this mechanism allows a customer to generate new public certificates directly in ACM both DNS and EMAIL validation mechanisms are supported (DNS recommended) this requires a Public DNS zone be properly configured to validate you are legally entitled to issue certificates for the domain this will also create a certificate in ACM and a secret in secrets manager named accelerator/certificates/My-Cert in the specified AWS account(s), which points to the newly imported certificates ARN this mechanism should NOT be used on new installs, skip certificate and ALB deployment during initial deployment (removing them from the config file) and simply add on a subsequent state machine execution Process: you need a public DNS domain properly registered and configured to publicly resolve the domain(s) you will be generating certificates for (i.e. example.com) domains can be purchased and configured in Amazon Route53 or through any 3rd party registrar and DNS service provider in Accelerator phase 1, the cert is generated, but the stack does NOT complete deploying (i.e. it waits) until certificate validation is complete during deployment, go to the AWS account in question, open ACM and the newly requested certificate. Document the authorization CNAME record required to validate certificate generation add the CNAME record to the zone in bullet 1 (in Route53 or 3rd party DNS provider) (documented here ) after a few minutes the certificate will validate and switch to Issued status Accelerator phase 1 will finish (as long as the certificate is validated before the Phase 1 credentials time-out after 60-minutes) the ALB will deploy in a later phase with the specified certificate Method 3 - Manually generate a certificate in ACM this mechanism allows a customer to manually generate certificates directly in the ACM interface for use by the Accelerator this mechanism should NOT be used on new installs, skip certificate and ALB deployment during initial deployment (removing them from the config file) and simply add on a subsequent state machine execution Process: go to the AWS account for which you plan to deploy an ALB and open ACM generate a certificate, documenting the certificates ARN open Secrets manager and generate a new secret of the format accelerator/certificates/My-Cert (of type Plaintext under Other type of secrets ), where My-Cert is the unique name you will use to reference this certificate In all three mechanisms a secret will exist in Secrets Manager named accelerator/certificates/My-Cert which contains the ARN of the certificate to be used. In the Accelerator config file, find the definition of the ALB for that AWS account and specify My-Cert for the ALB cert-name \"alb\": [ { \"cert-name\": \"My-Cert\" } ] The state machine will fail if you specify a certificate in any ALB which is not defined in Secrets Manager in the local account. We suggest the most effective mechanism for leveraging ACM is by adding CNAME authorization records to the relevant DNS domains using Method 2, but may not appropriate right for all customers. Why do we have rsyslog servers? I thought everything was sent to CloudWatch? \u00b6 The rsyslog servers are included to accept logs for appliances and third party applications that do not natively support the CloudWatch Agent from any account within a customers Organization. These logs are then immediately forwarded to CloudWatch Logs within the account the rsyslog servers are deployed (Operations) and are also copied to the S3 immutable bucket in the log-archive account. Logs are only persisted on the rsyslog hosts for 24 hours. The rsyslog servers are required to centralize the 3rd party firewall logs (Fortinet Fortigate). Can you deploy the solution without Fortinet Firewall Licenses? \u00b6 Yes, if license files are not provided, the firewalls will come up configured and route traffic, but customers will have no mechanism to manage the firewalls/change the configuration until a valid license file is added. If invalid licence files are provided, the firewalls will fail to load the provided configuration, will not enable routing, will not bring up the VPN tunnels and will not be manageable. Customers will need to either remove and redeploy the firewalls, or manually configure them. If performing a test deployment, please work with your local Fortinet account team to discuss any options for temporary evaluation licenses. Additionally, several additional firewall options are now available, including using AWS Network Firewall, a native AWS service. I installed additional software on my Accelerator deployed RDGW / rsyslog host, where did it go? \u00b6 The RDGW and rsyslog hosts are members of auto-scaling groups. These auto-scaling groups have been configured to refresh instances in the pool on a regular basis (7-days in the current sample config files). This ensures these instances are always clean. Additionally, on every execution of the Accelerator state machine the ASG are updated to the latest AWS AMI for the instances. When the auto-scaling group refreshes its instances, they will be redeployed with the latest patch release of the AMI/OS. It is recommended that the state machine be executed monthly to ensure the latest AMI's are always in use. Customers wanting to install additional software on these instances should either a) update the automated deployment scripts to install the new software on new instance launch, or b) create and specify a custom AMI in the Accelerator configuration file which has the software pre-installed ensuring they are also managing patch compliance on the instance through some other mechanism. At any time, customers can terminate the RDGW or rsyslog hosts and they will automatically be re-created from the base images with the latest patch available at the time of the last Accelerator State Machine execution. Some sample configurations provide NACLs and Security Groups. Is that enough? \u00b6 Security group egress rules are often used in 'allow all' mode ( 0.0.0.0/0 ), with the focus primarily being on consistently allow listing required ingress traffic (centralized ingress/egress controls are in-place using the perimeter firewalls). This ensures day to day activities like patching, access to DNS, or to directory services access can function on instances without friction. The Accelerator provided sample security groups in the workload accounts offer a good balance that considers both security, ease of operations, and frictionless development. They allow developers to focus on developing, enabling them to simply use the pre-created security constructs for their workloads, and avoid the creation of wide-open security groups. Developers can equally choose to create more appropriate least-privilege security groups more suitable for their application, if they are skilled in this area. It is expected as an application is promoted through the SDLC cycle from Dev through Test to Prod, these security groups will be further refined by the extended customers teams to further reduce privilege, as appropriate. It is expected that each customer will review and tailor their Security Groups based on their own security requirements. The provided security groups ensures day to day activities like patching, access to DNS, or to directory services access can function on instances without friction, with the understanding further protections are providing by the central ingress/egress firewalls. The use of NACLs are general discouraged, but leveraged in this architecture as a defense-in-depth mechanism. Security groups should be used as the primary access control mechanism. As with security groups, we encourage customers to review and tailor their NACLs based on their own security requirements. Can I deploy the solution as the account root user? \u00b6 No, you cannot install as the root user. The root user has no ability to assume roles which is a requirement to configure the sub-accounts and will prevent the deployment. As per the installation instructions , you require an IAM user with the AdministratorAccess policy attached. Is the Organizational Management root account monitored similarly to the other accounts in the organization? \u00b6 Yes, all accounts including the Organization Management or root account have the same monitoring and logging services enabled. When supported, AWS security services like GuardDuty, Macie, and Security Hub have their delegated administrator account configured as the \"security\" account. These tools can be used within each local account (including the Organization Management account) within the organization to gain account level visibility or within the Security account for Organization wide visibility. For more information about monitoring and logging refer to architecture documentation . How are the perimeter firewall configurations and licensing managed after deployment? \u00b6 While you deploy the perimeter firewalls with the Accelerator you will continue to manage firewall updates, configuration changes, and license renewals from the respective firewall management interface and not from the Accelerator config file. As these changes are not managed by the Accelerator you do not need to rerun the state machine to implement or track any of these changes. You can update the AMI of the 3rd party firewalls using the Accelerator, you must first remove the existing firewalls and redeploy them (as the Elastic IP's (EIP's) will block a parallel deployment) or deploy a second parallel firewall cluster and de-provision the first cluster when ready. Can the Fortinet Firewall deployments use static private IP address assignments? \u00b6 Yes, the \"port\" stanza in the configuration file can support a private static IP address assignment from the az and subnet. Care must be exercised to assure the assigned IP address is within the correct subnet and availability zone. Consideration must also be given to the Amazon reserved IP addresses (first three addresses, and the last) within subnets when choosing an IP Address to assign. Using the config.example.json as a reference, static IP Assignments would look like this in the ports: stanza of the firewall deployment. \"ports\": [ { \"name\": \"Public\", \"subnet\": \"Public\", \"create-eip\": true, \"create-cgw\": true, \"private-ips\": [ { \"az\": \"a\", \"ip\": \"100.96.250.4\" }, { \"az\": \"b\", \"ip\": \"100.96.250.132\" } ] }, { \"name\": \"OnPremise\", \"subnet\": \"OnPremise\", \"create-eip\": false, \"create-cgw\": false, \"private-ips\": [ { \"az\": \"a\", \"ip\": \"100.96.250.68\" }, { \"az\": \"b\", \"ip\": \"100.96.250.196\" } ] } ... ], Where private-ips are not present for the subnet or availability zone an address will be assigned automatically from available addresses when the firewall instance is created. I've noticed CloudTrail logs and in certain situation VPC flow logs are stored in the centralized log-archive account logging bucket twice? \u00b6 Yes. Cloudtrail is configured to send its logs directly to S3 for centralized immutable log retention. CloudTrail is also configured to send it's logs to a centralized Organizational CloudWatch Log group such that the trail can be a) easily queried online using CloudWatch Insights across all AWS accounts in the organization, and b) to enable alerting based on undesirable API activity using CloudWatch Metrics and Alarms. All CloudWatch Log groups are also configured to be sent, using Amazon Kinesis, to S3 for centralized immutable log retention. VPC flow log destinations can be configured in the config file. The example config files are set to send the VPC flow logs to both S3 and CloudWatch Logs by default for the same reasons as CloudTrail. To reduce the duplicate long-term storage of these two specific CloudWatch Log types, customers can set cwl-glbl-exclusions under central-log-services to: [\"/${ACCELERATOR_PREFIX_ND}/flowlogs/*\", \"/${ACCELERATOR_PREFIX_ND}/CloudTrail*\"] to prevent these specifically named log groups from being stored on S3. This setting also prevents the Accelerator from setting the customer desired log group retention period defined in the config file, once implemented, for those log groups. Therfore, we do not recommend this exception be applied during the initial installation, as the retention setting on these CWL groups will remain the default (infinite). If cwl-glbl-exclusions is set after initial install, the defined retention will be configured during install and will remain set to the value present when the exception was applied to those log groups. This allows logs to be stored in CloudWatch Logs for quick and easy online access (short-retention only), and stored in S3 for long-term retention and access. Side note: CloudTrail S3 data plane logs are enabled at the Organizational level, meaning all S3 bucket access is logged. As CloudTrail is writing to a bucket within the Organization, Cloudtrail itself is accessing the bucket, seemingly creating a cyclical loop. As CloudTrail writes to S3 in 5-10min batches, Cloudtrail will actually only cause one extra log 'entry' every 5-10minutes and not per s3 event, mitigating major concerns. Today, with an Organization trail logging data plane events for all buckets - there is no way to exclude any one bucket. But - having clear view of who accessed/changed logs, including AWS services, is important.","title":"FAQ"},{"location":"faq/#accelerator-basic-operation-and-frequently-asked-questions","text":"","title":"Accelerator Basic Operation and Frequently asked Questions"},{"location":"faq/#operational-activities","text":"","title":"Operational Activities"},{"location":"faq/#how-do-i-add-new-aws-accounts-to-my-aws-organization","text":"We offer four options and all can be used in the same Accelerator deployment. All options work with AWS Control Tower, ensuring the account is both ingested into Control Tower and all Accelerator guardrails are automatically applied: Users can simply add the following five lines to the configuration file workload-account-configs section and rerun the state machine. The majority of the account configuration will be picked up from the ou the AWS account has been assigned. You can also add additional account specific configuration, or override items like the default ou budget with an account specific budget. This mechanism is often used by customers that wish to programmatically create AWS accounts using the Accelerator and allows for adding many new accounts at one time. json \"fun-acct\": { \"account-name\": \"TheFunAccount\", \"email\": \"myemail+aseaT-funacct@example.com\", \"src-filename\": \"config.json\", \"ou\": \"Sandbox\" } We've heard consistent feedback that our customers wish to use native AWS services and do not want to do things differently once security controls, guardrails, or accelerators are applied to their environment. In this regard, simply create your new AWS account in AWS Organizations as you did before**, either by a) using the AWS Console or b) by using standard AWS account creation API's, CLI or 3rd party tools like Terraform. ** IMPORTANT: When creating the new AWS account using AWS Organizations, you need to specify the role name provided in the Accelerator configuration file global-options\\organization-admin-role , otherwise we cannot bootstrap the account. In Control Tower installations, this MUST be set to AWSControlTowerExecution , for customers who installed prior to v1.2.5 this value is AWSCloudFormationStackSetExecutionRole and after v1.2.5 we were recommending using the role OrganizationAccountAccessRole as this role is used by default by AWS Organizations if no role name is specified when creating AWS accounts through the AWS console or cli. On account creation we will apply a quarantine SCP which prevents the account from being used by anyone until the Accelerator has applied the appropriate guardrails Moving the account into the appropriate OU triggers the state machine and the application of the guardrails to the account, once complete, we will remove the quarantine SCP. NOTE: Accounts CANNOT be moved between OU's to maintain compliance, so select the proper top-level OU with care In AWS Organizations, select ALL the newly created AWS accounts and move them all (preferably at once) to the correct destination OU (assuming the same OU for all accounts) In case you need to move accounts to multiple OU's we have added a 2 minute delay before triggering the State Machine Any accounts moved after the 2 minute window will NOT be properly ingested, and will need to be ingested on a subsequent State Machine Execution. Create your account using Account Factory in the AWS Control Tower console. No matter the mechanism you choose, new accounts will automatically be blocked from use until fully guardrailed, the Accelerator will automatically execute, and accounts will automatically be ingested into AWS Control Tower.","title":"How do I add new AWS accounts to my AWS Organization?"},{"location":"faq/#i-tried-to-enroll-a-new-account-via-control-tower-but-it-failed-the-state-machine-failed-during-the-load-organization-configuration-step-with-the-error-the-control-tower-account-account_name-is-in-a-failed-state-error","text":"If account enrollment fails within Control Tower, you will need to follow the troubleshooting steps here . A common reason for this is not having the ControlTowerExectution role created in the account you are trying to enroll. Even after you successfully enroll the account, it is possible the state machine will fail at Load Organization Configuration . If you look at the cloudwatch logs you will see the error message: There were errors while loading the configuration: The Control Tower account: ACCOUNT_NAME is in a failed state ERROR. This is because the Accelerator checks that there are no errors with Control Tower before continuing. In some cases Control Tower can leave an orphaned Service Catalog product in an Error state. You need to cleanup Control Towers Service Catalogs Provisioned Products so there are no products remaining in an error or tainted state before you can successfully re-run the state machine.","title":"I tried to enroll a new account via Control Tower but it failed? The state machine failed during the \"Load Organization Configuration\" step with the error \"The Control Tower account: ACCOUNT_NAME is in a failed state ERROR\"?"},{"location":"faq/#can-i-use-aws-organizations-for-all-tasks-i-currently-use-aws-organizations-for","text":"In AWS Organizations you can continue to: create and rename AWS accounts move AWS accounts between ou's create, delete and rename ou's, including support for nested ou's create, rename, modify, apply and remove SCP's What can't I do: modify Accelerator or Control Tower controlled SCP's add/remove SCP's on top-level OU's (these are Accelerator and/or Control Tower controlled) users can change SCP's on non-top-level ou's and non-Accelerator controlled accounts as they please add/remove SCP's on specific accounts that have Accelerator controlled SCPs move an AWS account between top-level ou's (i.e. Sandbox to Prod is a security violation) moving between Prod/sub-ou-1 to Prod/sub-ou2 or Prod/sub-ou2/sub-ou2a/sub-ou2ab is fully supported create a top-level ou (need to validate, as they require config file entries) remove quarantine SCP from newly created accounts we do not support forward slashes ( / ) in ou names, even though the AWS platform does More details: If an AWS account is renamed, an account email is changed, or an OU is renamed, on the next state machine execution, the config file will automatically be updated. If you edit an Accelerator controlled SCP through Organizations, we will reset it per what is defined in the Accelerator configuration files. If you add/remove an SCP from a top-level ou or Accelerator controlled account, we will put them back as defined in the Accelerator configuration file. If you move an account between top-level ou's, we will put it back to its original designated top-level ou. The Accelerator fully supports nested ou's, customers can create any depth ou structure in AWS Organizations and add/remove/change SCP's below the top-level as they desire or move accounts between these ou's without restriction. Users can create ou's to the full AWS ou structure/depth Except for the Quarantine SCP applied to specific accounts, we do not 'control' SCP's below the top level, customers can add/create/customize SCP's as of v1.3.3 customers can optionally control account level SCP's through the configuration file","title":"Can I use AWS Organizations for all tasks I currently use AWS Organizations for?"},{"location":"faq/#how-do-i-make-changes-to-items-i-defined-in-the-accelerator-configuration-file-during-installation","text":"Simply update your configuration file in CodeCommit and rerun the state machine! In most cases, it is that simple. If you ask the Accelerator to do something that is not supported by the AWS platform, the state machine will fail, so it needs to be a supported capability. For example, the platform does not allow you to change the CIDR block on a VPC, but you can accomplish this as you would today by using the Accelerator to deploy a new second VPC, manually migrating workloads, and then removing the deprecated VPC from the Accelerator configuration. Below we have also documented additional considerations when creating or updating the configuration file. It should be noted that we have added code to the Accelerator to block customers from making many 'breaking' or impactful changes to their configuration files. If someone is positive they want to make these changes, we also provide override switches to allow these changes to be attempted forcefully.","title":"How do I make changes to items I defined in the Accelerator configuration file during installation?"},{"location":"faq/#can-i-update-the-config-file-while-the-state-machine-is-running-when-will-those-changes-be-applied","text":"Yes. The state machine captures a consistent input state of the requested configuration when it starts. The running Accelerator instance does not see or consider any configuration changes that occur after it has started. All configuration changes occurring after the state machine is running will only be leveraged on the next state machine execution.","title":"Can I update the config file while the State Machine is running? When will those changes be applied?"},{"location":"faq/#what-if-i-really-mess-up-the-configuration-file","text":"The Accelerator is designed with checks to compare your current configuration file with the version of the config file from the previous successful execution of the state machine. If we believe you are making major or breaking changes to the config file, we will purposefully fail the state machine. See 1.4. Config file and Deployment Protections for more details. With the release of v1.3.0 we introduced state machine scoping capabilities to further protect customers, detailed here","title":"What if I really mess up the configuration file?"},{"location":"faq/#what-if-my-state-machine-fails-why-previous-solutions-had-complex-recovery-processes-whats-involved","text":"If your main state machine fails, review the error(s), resolve the problem and simply re-run the state machine. We've put a huge focus on ensuring the solution is idempotent and to ensure recovery is a smooth and easy process. Ensuring the integrity of deployed guardrails is critical in operating and maintaining an environment hosting protected data. Based on customer feedback and security best practices, we purposely fail the state machine if we cannot successfully deploy guardrails. Additionally, with millions of active customers each supporting different and diverse use cases and with the rapid rate of evolution of the AWS platform, sometimes we will encounter unexpected circumstances and the state machine might fail. We've spent a lot of time over the course of the Accelerator development process ensuring the solution can roll forward, roll backward, be stopped, restarted, and rerun without issues. A huge focus was placed on dealing with and writing custom code to manage and deal with non-idempotent resources (like S3 buckets, log groups, KMS keys, etc.). We've spent a lot of time ensuring that any failed artifacts are automatically cleaned up and don't cause subsequent executions to fail. We've put a strong focus on ensuring you do not need to go into your various AWS sub-accounts and manually remove or cleanup resources or deployment failures. We've also tried to provide usable error messages that are easy to understand and troubleshoot. As new scenario's are brought to our attention, we continue to adjust the codebase to better handle these situations. Will your state machine fail at some point in time, likely. Will you be able to easily recover and move forward without extensive time and effort, YES!","title":"What if my State Machine fails? Why? Previous solutions had complex recovery processes, what's involved?"},{"location":"faq/#how-do-i-update-some-of-the-supplied-sample-configuration-items-found-in-reference-artifact-like-scps-and-iam-policies","text":"To override items like SCP's or IAM policies, customers simply need to provide the identically named file in there input bucket. As long as the file exists in the correct folder in the customers input bucket, the Accelerator will use the customers supplied version of the configuration item, rather than the Accelerator version. Customer SCP's need to be placed into a folder named scp and iam policies in a folder named iam-policy (case sensitive). The Accelerator was designed to allow customers complete customization capabilities without any requirement to update code or fork the GitHub repo. Additionally, rather than forcing customers to provide a multitude of config files for a standard or prescriptive installation, we provide and auto-deploy with Accelerator versions of most required configuration items from the reference-artifacts folder of the repo. If a customer provides the required configuration file in their Accelerator S3 input bucket, we will use the customer supplied version of the configuration file rather than the Accelerator version. At any time, either before initial installation, or in future, a customer can place new or updated SCPs, policies, or other supported file types into their input bucket and we will use those instead of or in addition to Accelerator supplied versions. Customer only need to provide the specific files they wish to override, not all files. Customers can also define additional SCPs (or modify existing SCPs) using the name, description and filename of their choosing, and deploy them by referencing them on the appropriate organizational unit in the config file. Prior to v1.2.5, if we updated the default files, we overwrote customers customizations during upgrade. Simply updating the timestamp after upgrade on the customized versions and then rerunning the state machine re-instates customer customizations. In v1.2.5 we always use the customer customized version from the S3 bucket. It's important customers assess newly provided defaults during an upgrade process to ensure they are incorporating all the latest fixes and improvements. If a customer wants to revert to Accelerator provided default files, they will need to manually copy it from the repo into their input bucket. NOTE: Most of the provided SCPs are designed to protect the Accelerator deployed resources from modification and ensure the integrity of the Accelerator. Extreme caution must be exercised if the provided SCPs are modified. In v1.5.0 we restructured the SCPs based on a) customer requests, and b) the addition of Control Tower support for new installs. we reorganized and optimized our SCP's from 4 SCP files down to 3 SCP files, without removing any protections or guardrails; these optimizations have resulted in minor enhancements to the SCP protections and in some cases better scoping; the first two SCP files (Part-0 and Part-1) contain the controls which protect the integrity of the Accelerator itself; the third file (Sensitive, Unclass, Sandbox) contains customer data protection specific guardrails, which may change based on workload data classification or customer profiles and requirements; this freed the fourth SCP for use by Control Tower. As Control Tower leverages 2 SCP files on the Security OU, we have moved some of our SCP's to the account level.","title":"How do I update some of the supplied sample configuration items found in reference-artifact, like SCPs and IAM policies?"},{"location":"faq/#i-deployed-aws-managed-active-directory-mad-as-part-of-my-deployment-how-do-i-manage-active-directory-domain-users-groups-and-domain-policies-after-deployment","text":"Customers have clearly indicated they do NOT want to use the Accelerator to manage their Active Directory domain or change the way they manage Active Directory on an ongoing basis. Customer have also indicated, they need help getting up and running quickly. For these reasons, the Accelerator only sets the domain password policy, and creates AD users and groups on the initial installation of MAD. After the initial installation, customers must manage Windows users and groups using their traditional tools. A bastion Windows host is deployed as a mechanism to support these capabilities. Passwords for all newly created MAD users have been stored, encrypted, in AWS Secrets Manager in the Management (root) Organization AWS account. To create new users and groups: RDP into the ASEA-RDGW bastion host in the Ops account Run ADUC and create users and groups as you please under the NETBIOSDOMAIN (example) tree Or run the appropriate powershell command Go to AWS SSO and map the Active Directory group to the appropriate AWS account and permission set The Accelerator will not create/update/delete new AD users or groups, nor will it update the domain password policy after the initial installation of Managed Active Directory. It is your responsibility to rotate these passwords on a regular basis per your organizations password policy. (NOTE: After updating the admin password it needs to be stored back in secrets manager).","title":"I deployed AWS Managed Active Directory (MAD) as part of my deployment, how do I manage Active Directory domain users, groups, and domain policies after deployment?"},{"location":"faq/#how-do-i-suspend-an-aws-account","text":"Suspending accounts is blocked via SCP and purposely difficult, two options exist: Modify SCP method (not desired) Leverage the UnManaged OU validate your config file contains the value: \"ignored-ous\": [\"UnManaged\"] the state machine must be executed at least once after this value is added to the config file In AWS Organizations create an OU named UnManaged in the root of the OU tree, if it does not exist Change to the us-east-1 region and open CloudWatch and navigate to Rules Select the PBMMAccel-MoveAccount_rule , select actions, select Disable In Organizations move the account to be suspended to the UnManaged OU Change to the us-east-1 region and open CloudWatch and navigate to Rules Select the PBMMAccel-MoveAccount_rule , select actions, select Enable login to the account to be suspended as the account root user suspend the account through My Account Run the state machine (from the Organization management account), the account will: have a deleted=true value added to the config file be moved to the suspended OU (OU value and path stays the same in the config file) deleted=true causes OU validation to be skipped on this account on subsequent SM executions If the AWS account was listed in the mandatory-accounts section of the config file the SM will fail (expected) after the above tasks have been completed, remove all references to the suspended mandatory account from the config file rerun the state machine, specifying: { \"overrideComparison\": true } Deleted accounts will continue to appear under the Suspended OU for 90-days","title":"How do I suspend an AWS account?"},{"location":"faq/#i-need-a-new-vpc-where-shall-i-define-it","text":"You can define a VPC in one of four major sections of the Accelerator configuration file: within an organization unit (this is the recommended and preferred method); within an account in mandatory-account-configs; within an account in workload-account-configs; defined within an organization unit, but opted-in within the account config. We generally recommend most items be defined within organizational units, such that all workload accounts pickup their persona from the OU they are associated and minimize per account configuration. Both a local account based VPC (as deployed in the Sandbox OU accounts), or a central shared VPC (as deployed in the Dev/Test/Prod OU accounts in many of the example configs) can be defined at the OU level. As mandatory accounts often have unique configuration requirements, for example the centralized Endpoint VPC, they must be configured within the accounts configuration. Customers can define VPC's or other account specific settings within any accounts configuration, but this requires editing the configuration file for each account configuration. Prior to v1.5.0, local VPC's defined at the OU level were each deployed with the same CIDR ranges and therefor could not be connected to a TGW. Local VPC's requiring centralized networking (i.e. TGW connectivity) were required to be defined in each account config, adding manual effort and bloating the configuration file. The addition of dynamic and lookup CIDR sources in v1.5.0 resolves this problem. Local VPCs can be defined in an OU, and each VPC will be dynamically assigned a unique CIDR range from the assigned CIDR pool, or looked up from the DynamoDB database. Customers can now ensure connected, templated VPCs are consistently deployed to every account in an OU, each with unique IP addresses. v1.5.0 also added a new opt-in VPC capability. A VPC is defined in an OU and a new config file variable is added to this VPC opt-in: true . When opt-in is set to true, the state machine does NOT create the VPC for the accounts in the OU, essentially ignoring the VPC definition. Select accounts in the OU can then be opted-in to the VPC(s) definition, by adding the value accountname\\opt-in-vpcs: [\u201copt-in-vpc-name1\u201d, \u201copt-in-vpc-name2\u201d, \u201copt-in-vpc-nameN\u201d] to the specific accounts which need the VPC(s). A vpc definition with the specified name (i.e. opt-in-vpc-name1 ) and the value opt-in: true , must exist in the ou config for the specified account. When these conditions apply, the VPC will be created in the account per the OU definition. Additional opt-in VPCs can be added to an account, but VPC's cannot be removed from the opt-in-vpcs array. VPC's can be TGW attached, assuming dynamic cidr-src is utilized, or DynamoDB is prepopulated with the required CIDR ranges using lookup mode. cidr-src provided is suitable for disconnected Sandbox type accounts. The Future: While Opt-In VPCs are powerful, we want to take this further. Why not deploy an AWS Service Catalog template which contains the names of all the available opt-in VPCs for the accounts OU, inside each account. An account end user could then request a new VPC for their account from the list of available opt-in patterns. A users selection would be sent to a centralized queue for approval (w/auto-approval options), which would result in the opt-in-vpc entry in that account being updated with the end users requested vpc pattern and the personalized VPC being created in the account and attached to the centralized TGW (if part of the pattern). This would ensure all VPC's conformed to a set of desirable design patterns, but also allow the end-user community choices based on their desired development and app patterns. If you like this idea, please +1 this feature request.","title":"I need a new VPC, where shall I define it?"},{"location":"faq/#how-do-i-modify-and-extend-the-accelerator-or-execute-my-own-code-after-the-accelerator-provisions-a-new-aws-account-or-the-state-machine-executes","text":"Flexibility: The AWS Secure Environment Accelerator was developed to enable extreme flexibility without requiring a single line of code to be changed. One of our primary goals throughout the development process was to avoid making any decisions that would result in users needing to fork or branch the Accelerator codebase. This would help ensure we had a sustainable and upgradable solution for a broad customer base over time. Functionality provided by the Accelerator can generally be controlled by modifying the main Accelerator configuration file. Items like SCP's, rsyslog config, Powershell scripts, and iam-policies have config files provided and auto-deployed as part of the Accelerator to deliver on the prescriptive architecture (these are located in the \\reference-artifacts folder of the Github repo for reference). If you want to alter the functionality delivered by any of these additional config files, you can simply provide your own by placing it in your specified Accelerator bucket in the appropriate sub-folder. The Accelerator will use your provided version instead of the supplied repo reference version. As SCP's and IAM policies are defined in the main config file, you can simply define new policies, pointing to new policy files, and provide these new files in your bucket, and they will be used. While a sample firewall config file is provided in the \\reference-artifacts folder, it must be manually placed in your s3 bucket/folder on new Accelerator deployments Any/all of these files can be updated at any time and will be used on the next execution of the state machine Over time, we predict we will provide several sample or reference architectures and not just the current single PBMM architecture (all located in the \\reference-artifacts\\SAMPLE_CONFIGS folder). Extensibility: Every execution of the state machine sends a state machine status event to a state machine SNS topic These status events include the Success/Failure status of the state machine, and on success, a list of all successfully processed AWS accounts While this SNS topic is automatically subscribed to a user provided email address for user notification, users can also create additional SNS subscriptions to enable triggering their own subsequent workflows, state machines, or custom code using any supported SNS subscription type (Lambda, SQS, Email, HTTPS, HTTPS) Additionally, objects deployed within an account have been populated in Parameter Store, see answer 1.3.2 for details Example: One of our early adopter customers has developed a custom user interface which allows their clients to request new AWS environments. Clients provide items like cost center, budget, and select their environment requirements (i.e. Sandbox, Unclass or full sensitive SDLC account set). On appropriate approval, this pushes the changes to the Accelerator configuration file and triggers the state machine. Once the state machine completes, the SNS topic triggers their follow-up workflow, validates the requested accounts were provisioned, updates the customer's account database, and then executes a collection of customer specific follow-up workflow actions on any newly provisioned accounts.","title":"How do I modify and extend the Accelerator or execute my own code after the Accelerator provisions a new AWS account or the state machine executes?"},{"location":"faq/#how-can-i-easily-access-my-virtual-machines-or-ec2-instances","text":"The preferred and recommended method to connect to instances within the Accelerator is by using AWS Systems Manager Session Manager. Session Manager allows access to instances without the need to have any open firewall ports. Session Manager allows for Command line access to instances (both Windows and Linux) directly through the AWS console, with all activity logged to CloudWatch Logs. Session Manager enables customers to connect to Windows instances with a native RDP client and Linux instances with a native SSH client, if desired. Customers can gain quick access to instances through the AWS consolve, or using their preferred clients. General Both the RDGW and rsyslog instances deployed in the Ops account are properly configured for Systems Manager Session Manager We have implemented automation such that all instances are also automatically configured for Session Manager (i.e. configured with the appropriate launch role, has a recent session manager agent installed (most amazon ami's do), has access to an SSM endpoint) Connecting to an Instance From the AWS Console Go to: EC2, Instances, select the instance (i.e. ASEA-RDGW), click \u201cConnect\u201d, select Session Manager, Connect Ideal for Linux or Windows Powershell users Everything is fully logged for future reference Directly through local RDP client using Session Managers tunnel capability: Provides direct access to your instances/host without any open internet ports on the RDGW instance, using a local/fat client tool Install AWS CLI v2 on your PC - available here (uninstall CLIv1 first, if installed) Install the SSM plugin on your PC - available here Get AWS temporary credentials from AWS SSO for the account your workload resides (i.e. Ops account when accessing the ASEA-RDGW instance) by selecting \u201cCommand line or programmatic access\u201d instead of \u201cManagement Console\u201d and paste them into a command prompt i.e. via logging in here: https://xxxxxxx.awsapps.com/start or This blog describes the process to use SSO to get credentials for the AWS CLI directly without the GUI Then enter: aws ssm start-session --target \"i-12345678901234567\" --document-name AWS-StartPortForwardingSession --parameters portNumber=\"3389\",localPortNumber=\"56789\"--region ca-central-1 Command syntax is slightly different on Linux/Mac Replace i-1111adddce582b23c with the instance id of your RDGW instance A tunnel will open As these are tunnels to proprietary protocols (i.e. RDP/screen scraping) session content is not logged. Run mstsc/rdp client and connect to 127.0.0.1:56789 By replacing 3389 with a new port for another applications (i.e. SSH running on a Linux instance), you can connect to a different application type You can change the local port by changing 56789 to any other valid port number (i.e. connecting to multiple instances at the same time) Login with the windows credentials discussed above in the format NETBIOSDOMAIN\\User1 (i.e. example\\user1) Your netbios domain is found here in your config file: \"netbios-domain\": \"example\", Connect to your desktop command line to command line interface of remote Windows or Linux servers, instead of through console (i.e. no tunnel): aws ssm start-session --target \"i-090c25e64c2d9d276\"\"--region ca-central-1 Replace i-xxx with your instance ID Everything is fully logged for future reference If you want to remove the region from your command line, you can: Type: \u201caws configure\u201d from command prompt, hit {enter} (key), {enter} (secret), enter: ca-central-1, {enter}","title":"How can I easily access my virtual machines or EC2 instances?"},{"location":"faq/#i-ran-the-state-machine-but-it-failed-when-it-tried-to-delete-the-default-vpc-the-state-machine-cannot-delete-the-default-vpc-error-vpc-has-dependencies-and-cannot-be-deleted","text":"You need to ensure that resources don\u2019t exist in the default VPC or else the state machine won't be able to delete it. If you encounter this error, you can either delete the resources within the VPC or delete the default VPC manually and run the state machine again.","title":"I ran the state machine but it failed when it tried to delete the default VPC? The state machine cannot delete the default VPC (Error: VPC has dependencies and cannot be deleted)"},{"location":"faq/#existing-accountsorganizations","text":"","title":"Existing Accounts/Organizations"},{"location":"faq/#how-do-i-import-an-existing-aws-account-into-my-accelerator-managed-aws-organization-or-what-if-i-created-a-new-aws-account-with-a-different-organization-trust-role","text":"Ensure you have valid administrative privileges for the account to be invited/added Add the account to your AWS Organization using standard processes (i.e. Invite/Accept) this process does NOT create an organization trust role imported accounts do NOT have the quarantine SCP applied as we don't want to break existing workloads Login to the account using the existing administrative credentials Execute the Accelerator provided CloudFormation template to create the required Accelerator bootstrapping role - in the Github repo here: reference-artifacts\\Custom-Scripts\\Import-Account-CFN-Role-Template.yml add the account to the Accelerator config file and run the state machine If you simply created the account with an incorrect role name, you likely need to take extra steps: Update the Accelerator config file to add the parameter: global-options\\ignored-ous = [\"UnManagedAccounts\"] In AWS Organizations, create a new OU named UnManagedAccounts (case sensitive) Move the account to the UnManagedAccounts ou You can now remove the Quarantine SCP from the account Assume an administrative role into the account Execute the Accelerator provided CloudFormation template to create the required Accelerator bootstrapping role","title":"How do I import an existing AWS account into my Accelerator managed AWS Organization (or what if I created a new AWS account with a different Organization trust role)?"},{"location":"faq/#is-it-possible-to-deploy-the-accelerator-on-top-of-an-aws-organization-that-i-have-already-installed-the-aws-landing-zone-alz-solution-into","text":"Existing ALZ customers are required to uninstall their ALZ deployment before deploying the Accelerator. Please work with your AWS account team to find the best mechanism to uninstall the ALZ solution (procedures and scripts exist). Additionally, please reference section 4 of the Instation and Upgrade Guide. It may be easier to migrate AWS accounts to a new Accelerator Organization, per the process detailed in FAQ #1.2.3.","title":"Is it possible to deploy the Accelerator on top of an AWS Organization that I have already installed the AWS Landing Zone (ALZ) solution into?"},{"location":"faq/#what-if-i-want-to-move-an-account-from-an-aws-organization-that-has-the-alz-deployed-into-an-aws-organization-running-the-accelerator","text":"Before removing the AWS account from the source organization, terminate the AWS Service Catalog product associated with the member account that you're interested in moving. Ensuring the product terminates successfully and that there aren't any remaining CloudFormation stacks in the account that were deployed by the ALZ. You can then remove the account from the existing Organization and invite it into the new organization. Accounts invited into the Organization do NOT get the Deny All SCP applied, as we do not want to break existing running workloads. Moving the newly invited account into its destination OU will trigger the state machine and result in the account being ingested into the Accelerator and having the guardrails applied per the target OU persona. For a detailed procedure, please review this document .","title":"What if I want to move an account from an AWS Organization that has the ALZ deployed into an AWS Organization running the Accelerator?"},{"location":"faq/#end-user-environment","text":"","title":"End User Environment"},{"location":"faq/#is-there-anything-my-end-users-need-to-be-aware-of-why-do-some-of-my-end-users-struggle-with-cloudwatch-log-groups-errors","text":"CloudWatch Log group deletion is prevented for security purposes and bypassing this rule would be a fundamental violation of security best practices. This protection does NOT exist solely to protect ASEA logs, but ALL log groups. Users of the Accelerator environment will need to ensure they set CloudFormation stack Log group retention type to RETAIN, or stack deletes will fail when attempting to delete a stack (as deleting the log group will be blocked) and users will encounter errors. As repeated stack deployments will be prevented from recreating the same log group name (as it already exists), end users will either need to check for the existence of the log group before attempting creation, or include a random hash in the log group name. The Accelerator also sets log group retention for all log groups to value(s) specified by customers in the config file and prevents end users from setting or changing Log group retentions. When creating new log groups, end users must either not configure a retention period, or set it to the default NEVER expire or they will also be blocked from creating the CloudWatch Log group. If applied by bypassing the guardrails, customer specified retention periods on log group creation will be overridden with the Accelerator specified retention period. While a security best practice, some end users continue to request this be changed, but you need to ask: Are end users allowed to go in and clean out logs from Windows Event Viewer (locally or on domain controllers) after testing? Clean out Linux kernel logs? Apache log histories? The fundamental principal is that all and as many logs as possible will be retained for a defined retention period (some longer). In the \"old days\", logs were hidden deep within OS directory structures or access restricted by IT from developers - now that we make them all centralized, visible, and accessible, end users seem to think they suddenly need to clean them up. Customers need to establish a usable and scalable log group naming standard/convention as the first step in moving past this concern, such that they can always find their active logs easily. As stated, to enable repeated install and removal of stacks during test cycles, end user CloudFormation stacks need to set log groups to RETAIN and leverage a random hash in log group naming (or check for existence, before creating). The Accelerator provided SCPs (guardrails/protections) are our recommendations, yet designed to be fully customizable, enabling any customer to carefully override these defaults to meet their individual requirements. If insistent, we'd suggest only bypassing the policy on the Sandbox OU, and only for log groups that start with a very specific prefix (not all log groups). When a customer wants to use the delete capability, they would need to name their log group with the designated prefix - i.e. opt-in to allow CloudWatch log group deletes.","title":"Is there anything my end users need to be aware of? Why do some of my end users struggle with CloudWatch Log groups errors?"},{"location":"faq/#how-can-i-leverage-accelerator-deployed-objects-in-my-iac-do-i-need-to-manually-determine-the-arns-and-object-ids-of-accelerator-deployed-objects-to-leverage-them-in-my-iac","text":"Objects deployed by the Accelerator which customers may need to leverage in their own IaC have been populated in parameters in AWS parameter store for use by the IaC tooling of choice. The Accelerator ensures parameters are deployed consistently across accounts and OUs, such that a customers code does not need to be updated when it is moved between accounts or promoted from Dev to Test to Prod. Objects of the following types and their associated values are stored in parameter store: vpc, subnet, security group, elb (alb/nlb w/DNS address), IAM policy, IAM role, KMS key, ACM cert, SNS topic, and the firewall replacement variables. Additionally, setting \"populate-all-elbs-in-param-store\": true for an account will populates all Accelerator wide ELB information into parameter store within that account. The sample PBMM configuration files set this value on the perimeter account, such that ELB information is available to configure centralized ingress capabilities.","title":"How can I leverage Accelerator deployed objects in my IaC? Do I need to manually determine the arn's and object id's of Accelerator deployed objects to leverage them in my IaC?"},{"location":"faq/#upgrades","text":"","title":"Upgrades"},{"location":"faq/#can-i-upgrade-directly-to-the-latest-release-or-must-i-perform-upgrades-sequentially","text":"Yes, currently customers can upgrade from whatever version they have deployed to the latest Accelerator version. There is no requirement to perform sequential upgrades. In fact, we strongly discourage sequential upgrades. Given the magnitude of the v1.5.0 release, we have added a one-time requirement that all customers upgrade to a minimum of v1.3.8 before attempting to upgrade to v1.5.0.","title":"Can I upgrade directly to the latest release, or must I perform upgrades sequentially?"},{"location":"faq/#why-do-i-get-the-error-there-were-errors-while-comparing-the-configuration-changes-when-i-update-the-config-file","text":"In v1.3.0 we added protections to allow customers to verify the scope of impact of their intended changes to the configuration file. In v1.3.0 and above, the state machine does not allow changes to the config file (other than new accounts) without providing the scope parameter. Please refer to section 1.1 of the State Machine behavior and inputs Guide for more details.","title":"Why do I get the error \"There were errors while comparing the configuration changes:\" when I update the config file?"},{"location":"faq/#support-concerns","text":"","title":"Support Concerns"},{"location":"faq/#the-accelerator-is-written-in-cdk-and-deploys-cloudformation-does-this-restrict-the-infrastructure-as-code-iac-tools-that-i-can-use","text":"No. Customers can choose the IaC framework or tooling of their choice. The tooling used to deploy the Accelerator has no impact on the automation framework customers use to deploy their applications within the Accelerator environment. It should be noted that the functionality deployed by the Accelerator is extremely platform specific and would not benefit from multi-platform IaC frameworks or tooling.","title":"The Accelerator is written in CDK and deploys CloudFormation, does this restrict the Infrastructure as Code (IaC) tools that I can use?"},{"location":"faq/#what-happens-if-aws-stops-enhancing-the-accelerator","text":"The Accelerator is an open source project, should AWS stop enhancing the solution for any reason, the community has access to the full codebase, its roadmap and history. The community can enhance, update, fork and take ownership of the project, as appropriate. The Accelerator is an AWS CDK based project and synthesizes to native AWS CloudFormation. AWS sub-accounts simply contain native CloudFormation stacks and associated custom resources, when required. The Accelerator architecture is such that all CloudFormation stacks are native to each AWS account with no links or ties to code in other AWS accounts or even other stacks within the same AWS account. This was an important initial design decision. The Accelerator codebase can be completely uninstalled from the organization management (root) account, without any impact to the deployed functionality or guardrails. In this situation, guardrail updates and new account provisioning reverts to a manual process. Should a customer decide they no longer wish to utilize the solution, they can remove the Accelerator codebase without any impact to deployed resources and go back to doing things natively in AWS as they did before they deployed the Accelerator. By adopting the Accelerator, customers are not locking themselves in or making a one-way door decision.","title":"What happens if AWS stops enhancing the Accelerator?"},{"location":"faq/#what-level-of-support-will-the-asea-have-from-aws-support","text":"The majority of the solution leverages native AWS services which are fully supported by AWS Support. Additionally, the Accelerator is an AWS CDK based project and synthesizes to native AWS CloudFormation. AWS sub-accounts simply contain native CloudFormation stacks and associated custom resources (when required). The Accelerator architecture is such that all CloudFormation stacks are native to each AWS account with no direct links or ties to code in other AWS accounts (no stacksets, no local CDK). This was an important project design decision, keeping deployed functionality in independent local CloudFormation stacks and decoupled from solution code, which allows AWS support to effectively troubleshoot and diagnose issues local to the sub-account. As the Accelerator also includes code, anything specifically related to the Accelerator codebase will be only supported on a \"best effort\" basis by AWS support, as AWS support does not support custom code. The first line of support for the codebase is typically your local AWS team (your SA, TAM, Proserve and/or AWS Partner). As an open source project, customers can file requests using GitHub Issues against the Accelerator repository or open a discussion in GitHub discussions. Most customer issues arise during installation and are related to configuration customization or during the upgrade process.","title":"What level of Support will the ASEA have from AWS Support?"},{"location":"faq/#what-does-it-take-to-support-the-accelerator","text":"We advise customers to allocate a 1/2 day per quarter to upgrade to the latest Accelerator release. Customers have indicated that deploying the Accelerator reduces their ongoing operational burden over operating in native AWS, saving hours of effort every time a new account is provisioned by automating the deployment of the persona associated with new accounts (guardrails, networking and security). The Accelerator does NOT alleviate a customers requirement to learn to effectively operate in the cloud (like monitoring security tooling/carrying out Security Operation Center (SOC) duties). This effort exists regardless of the existence of the Accelerator.","title":"What does it take to support the Accelerator?"},{"location":"faq/#is-the-accelerator-only-designed-and-suitable-for-government-of-canada-or-pbmm-customers","text":"No. The Accelerator is targeted at any AWS customer that is looking to automate the deployment and management of a comprehensive end-to-end multi-account environment in AWS. It is ideally suited for customers interested in achieving a high security posture in AWS. The Accelerator is a sophisticated deployment framework that allows for the deployment and management of virtually any AWS multi-account \"Landing Zone\" architecture without any code modifications. The Accelerator is actually delivering two separate and distinct products which can each be used on their own: the Accelerator the tool, which can deploy virtually any architecture based on a provided config file (no code changes), and; the Government of Canada (GC) prescriptive PBMM architecture which is delivered as a sample configuration file and documentation. The tooling was purposely built to be extremely flexible, as we realized that some customers may not like some of the opinionated and prescriptive design decisions we made in the GC architecture. Virtually every feature being deployed can be turned on/off, not be used or can have its configuration adjusted to meet your specific design requirements. We are working on building a library of sample config files to support additional customer needs and better demonstrate product capabilities and different architecture patterns. In no way is it required that the prescriptive GC architecture be used or deployed. Just because we can deploy, for example, an AWS Managed Active Directory, does not mean you need to use that feature of the solution. Disabling or changing these capabilities also requires zero code changes. While the prescriptive sample configuration files were originally developed based on GC requirements, they were also developed following AWS Best Practices. Additionally, many security frameworks around the world have similar and overlapping security requirements (you can only do security so many ways). The provided architecture is applicable to many security compliance regimes around the world and not just the GC.","title":"Is the Accelerator only designed and suitable for Government of Canada or PBMM customers?"},{"location":"faq/#deployed-functionality","text":"","title":"Deployed Functionality"},{"location":"faq/#i-wish-to-be-in-compliance-with-the-12-gc-tbs-guardrails-what-dont-you-cover-with-the-provided-sample-architecture","text":"The AWS SEA allows for a lot of flexibility in deployed architectures. If used, the provided PBMM sample architecture was designed to help deliver on the technical portion of all 12 of the GC guardrails, when automation was possible. What don't we cover? Assigning MFA to users is a manual process. Specifically you need to procure Yubikeys for your root/break glass users, and enable a suitable form of MFA for all other users (i.e. virtual, email, other). The guardrails also include some organizational processes (i.e. break glass procedures, or signing an MOU with CCCS) which customers will need to work through independently. While AWS is providing the tools to help customer be compliant with the 12 PBMM guardrails (which were developed in collaboration with the GC) - it's up to each customers ITSec organization to assess and determine if the deployed controls actually meet their security requirements. Finally, while we started with a goal of delivering on the 12 guardrails, we believe we have extended well beyond those security controls, to further help customers move towards meeting the full PBMM technical control profile (official documentation is weak in this area at this time).","title":"I wish to be in compliance with the 12 GC TBS Guardrails, what don't you cover with the provided sample architecture?"},{"location":"faq/#does-the-alb-perform-ssl-offloading","text":"As configured - the perimeter ALB decrypts incoming traffic using its certificate and then re-encrypts it with the certificate for the back-end ALB. The front-end and back-end ALB's can use the same or different certs. If the Firewall needs to inspect the traffic, it also needs the backend certificate be manually installed.","title":"Does the ALB perform SSL offloading?"},{"location":"faq/#what-is-the-recommended-approach-to-manage-the-alb-certificates-deployed-by-the-accelerator","text":"The Accelerator installation process allows customers to provide their own certificates (either self-signed or generated by a CA), to enable quick and easy installation and allowing customers to test end-to-end traffic flows. After the initial installation, we recommend customers leverage AWS Certificate Manager (ACM) to easily provision, manage, and deploy public and private SSL/TLS certificates. ACM helps manage the challenges of maintaining certificates, including certificate rotation and renewal, so you don\u2019t have to worry about expiring certificates. The Accelerator provides 3 mechanisms to enable utilizing certificates with ALB's: Method 1 - IMPORT a certificate into AWS Certificate Manager from a 3rd party product When using a certificate that does not have a certificate chain (usually this is the case with Self-Signed) json \"certificates\": [ { \"name\": \"My-Cert\", \"type\": \"import\", \"priv-key\": \"certs/example1-cert.key\", \"cert\": \"certs/example1-cert.crt\" } ] When using a certificate that has a certificate chain (usually this is the case when signed by a Certificate Authority with a CA Bundle) json \"certificates\": [ { \"name\": \"My-Cert\", \"type\": \"import\", \"priv-key\": \"certs/example1-cert.key\", \"cert\": \"certs/example1-cert.crt\", \"chain\": \"certs/example1-cert.chain\" } ] this mechanism allows a customer to generate certificates using their existing tools and processes and import 3rd party certificates into AWS Certificate Manager for use in AWS Self-Signed certificates should NOT be used for production (samples were provided simply to demonstrate functionality) both a .key and a .crt file must be supplied in the customers S3 input bucket \"cert\" must contain only the certificate and not the full chain \"chain\" is an optional attribute that contains the certificate chain. This is generally used when importing a CA signed certificate this will create a certificate in ACM and a secret in secrets manager named accelerator/certificates/My-Cert in the specified AWS account(s), which points to the newly imported certificates ARN Method 2 - REQUEST AWS Certificate Manager generate a certificate json \"certificates\": [ { \"name\": \"My-Cert\", \"type\": \"request\", \"domain\": \"*.example.com\", \"validation\": \"DNS\", \"san\": [\"www.example.com\"] } ] this mechanism allows a customer to generate new public certificates directly in ACM both DNS and EMAIL validation mechanisms are supported (DNS recommended) this requires a Public DNS zone be properly configured to validate you are legally entitled to issue certificates for the domain this will also create a certificate in ACM and a secret in secrets manager named accelerator/certificates/My-Cert in the specified AWS account(s), which points to the newly imported certificates ARN this mechanism should NOT be used on new installs, skip certificate and ALB deployment during initial deployment (removing them from the config file) and simply add on a subsequent state machine execution Process: you need a public DNS domain properly registered and configured to publicly resolve the domain(s) you will be generating certificates for (i.e. example.com) domains can be purchased and configured in Amazon Route53 or through any 3rd party registrar and DNS service provider in Accelerator phase 1, the cert is generated, but the stack does NOT complete deploying (i.e. it waits) until certificate validation is complete during deployment, go to the AWS account in question, open ACM and the newly requested certificate. Document the authorization CNAME record required to validate certificate generation add the CNAME record to the zone in bullet 1 (in Route53 or 3rd party DNS provider) (documented here ) after a few minutes the certificate will validate and switch to Issued status Accelerator phase 1 will finish (as long as the certificate is validated before the Phase 1 credentials time-out after 60-minutes) the ALB will deploy in a later phase with the specified certificate Method 3 - Manually generate a certificate in ACM this mechanism allows a customer to manually generate certificates directly in the ACM interface for use by the Accelerator this mechanism should NOT be used on new installs, skip certificate and ALB deployment during initial deployment (removing them from the config file) and simply add on a subsequent state machine execution Process: go to the AWS account for which you plan to deploy an ALB and open ACM generate a certificate, documenting the certificates ARN open Secrets manager and generate a new secret of the format accelerator/certificates/My-Cert (of type Plaintext under Other type of secrets ), where My-Cert is the unique name you will use to reference this certificate In all three mechanisms a secret will exist in Secrets Manager named accelerator/certificates/My-Cert which contains the ARN of the certificate to be used. In the Accelerator config file, find the definition of the ALB for that AWS account and specify My-Cert for the ALB cert-name \"alb\": [ { \"cert-name\": \"My-Cert\" } ] The state machine will fail if you specify a certificate in any ALB which is not defined in Secrets Manager in the local account. We suggest the most effective mechanism for leveraging ACM is by adding CNAME authorization records to the relevant DNS domains using Method 2, but may not appropriate right for all customers.","title":"What is the recommended approach to manage the ALB certificates deployed by the Accelerator?"},{"location":"faq/#why-do-we-have-rsyslog-servers-i-thought-everything-was-sent-to-cloudwatch","text":"The rsyslog servers are included to accept logs for appliances and third party applications that do not natively support the CloudWatch Agent from any account within a customers Organization. These logs are then immediately forwarded to CloudWatch Logs within the account the rsyslog servers are deployed (Operations) and are also copied to the S3 immutable bucket in the log-archive account. Logs are only persisted on the rsyslog hosts for 24 hours. The rsyslog servers are required to centralize the 3rd party firewall logs (Fortinet Fortigate).","title":"Why do we have rsyslog servers? I thought everything was sent to CloudWatch?"},{"location":"faq/#can-you-deploy-the-solution-without-fortinet-firewall-licenses","text":"Yes, if license files are not provided, the firewalls will come up configured and route traffic, but customers will have no mechanism to manage the firewalls/change the configuration until a valid license file is added. If invalid licence files are provided, the firewalls will fail to load the provided configuration, will not enable routing, will not bring up the VPN tunnels and will not be manageable. Customers will need to either remove and redeploy the firewalls, or manually configure them. If performing a test deployment, please work with your local Fortinet account team to discuss any options for temporary evaluation licenses. Additionally, several additional firewall options are now available, including using AWS Network Firewall, a native AWS service.","title":"Can you deploy the solution without Fortinet Firewall Licenses?"},{"location":"faq/#i-installed-additional-software-on-my-accelerator-deployed-rdgw-rsyslog-host-where-did-it-go","text":"The RDGW and rsyslog hosts are members of auto-scaling groups. These auto-scaling groups have been configured to refresh instances in the pool on a regular basis (7-days in the current sample config files). This ensures these instances are always clean. Additionally, on every execution of the Accelerator state machine the ASG are updated to the latest AWS AMI for the instances. When the auto-scaling group refreshes its instances, they will be redeployed with the latest patch release of the AMI/OS. It is recommended that the state machine be executed monthly to ensure the latest AMI's are always in use. Customers wanting to install additional software on these instances should either a) update the automated deployment scripts to install the new software on new instance launch, or b) create and specify a custom AMI in the Accelerator configuration file which has the software pre-installed ensuring they are also managing patch compliance on the instance through some other mechanism. At any time, customers can terminate the RDGW or rsyslog hosts and they will automatically be re-created from the base images with the latest patch available at the time of the last Accelerator State Machine execution.","title":"I installed additional software on my Accelerator deployed RDGW / rsyslog host, where did it go?"},{"location":"faq/#some-sample-configurations-provide-nacls-and-security-groups-is-that-enough","text":"Security group egress rules are often used in 'allow all' mode ( 0.0.0.0/0 ), with the focus primarily being on consistently allow listing required ingress traffic (centralized ingress/egress controls are in-place using the perimeter firewalls). This ensures day to day activities like patching, access to DNS, or to directory services access can function on instances without friction. The Accelerator provided sample security groups in the workload accounts offer a good balance that considers both security, ease of operations, and frictionless development. They allow developers to focus on developing, enabling them to simply use the pre-created security constructs for their workloads, and avoid the creation of wide-open security groups. Developers can equally choose to create more appropriate least-privilege security groups more suitable for their application, if they are skilled in this area. It is expected as an application is promoted through the SDLC cycle from Dev through Test to Prod, these security groups will be further refined by the extended customers teams to further reduce privilege, as appropriate. It is expected that each customer will review and tailor their Security Groups based on their own security requirements. The provided security groups ensures day to day activities like patching, access to DNS, or to directory services access can function on instances without friction, with the understanding further protections are providing by the central ingress/egress firewalls. The use of NACLs are general discouraged, but leveraged in this architecture as a defense-in-depth mechanism. Security groups should be used as the primary access control mechanism. As with security groups, we encourage customers to review and tailor their NACLs based on their own security requirements.","title":"Some sample configurations provide NACLs and Security Groups. Is that enough?"},{"location":"faq/#can-i-deploy-the-solution-as-the-account-root-user","text":"No, you cannot install as the root user. The root user has no ability to assume roles which is a requirement to configure the sub-accounts and will prevent the deployment. As per the installation instructions , you require an IAM user with the AdministratorAccess policy attached.","title":"Can I deploy the solution as the account root user?"},{"location":"faq/#is-the-organizational-management-root-account-monitored-similarly-to-the-other-accounts-in-the-organization","text":"Yes, all accounts including the Organization Management or root account have the same monitoring and logging services enabled. When supported, AWS security services like GuardDuty, Macie, and Security Hub have their delegated administrator account configured as the \"security\" account. These tools can be used within each local account (including the Organization Management account) within the organization to gain account level visibility or within the Security account for Organization wide visibility. For more information about monitoring and logging refer to architecture documentation .","title":"Is the Organizational Management root account monitored similarly to the other accounts in the organization?"},{"location":"faq/#how-are-the-perimeter-firewall-configurations-and-licensing-managed-after-deployment","text":"While you deploy the perimeter firewalls with the Accelerator you will continue to manage firewall updates, configuration changes, and license renewals from the respective firewall management interface and not from the Accelerator config file. As these changes are not managed by the Accelerator you do not need to rerun the state machine to implement or track any of these changes. You can update the AMI of the 3rd party firewalls using the Accelerator, you must first remove the existing firewalls and redeploy them (as the Elastic IP's (EIP's) will block a parallel deployment) or deploy a second parallel firewall cluster and de-provision the first cluster when ready.","title":"How are the perimeter firewall configurations and licensing managed after deployment?"},{"location":"faq/#can-the-fortinet-firewall-deployments-use-static-private-ip-address-assignments","text":"Yes, the \"port\" stanza in the configuration file can support a private static IP address assignment from the az and subnet. Care must be exercised to assure the assigned IP address is within the correct subnet and availability zone. Consideration must also be given to the Amazon reserved IP addresses (first three addresses, and the last) within subnets when choosing an IP Address to assign. Using the config.example.json as a reference, static IP Assignments would look like this in the ports: stanza of the firewall deployment. \"ports\": [ { \"name\": \"Public\", \"subnet\": \"Public\", \"create-eip\": true, \"create-cgw\": true, \"private-ips\": [ { \"az\": \"a\", \"ip\": \"100.96.250.4\" }, { \"az\": \"b\", \"ip\": \"100.96.250.132\" } ] }, { \"name\": \"OnPremise\", \"subnet\": \"OnPremise\", \"create-eip\": false, \"create-cgw\": false, \"private-ips\": [ { \"az\": \"a\", \"ip\": \"100.96.250.68\" }, { \"az\": \"b\", \"ip\": \"100.96.250.196\" } ] } ... ], Where private-ips are not present for the subnet or availability zone an address will be assigned automatically from available addresses when the firewall instance is created.","title":"Can the Fortinet Firewall deployments use static private IP address assignments?"},{"location":"faq/#ive-noticed-cloudtrail-logs-and-in-certain-situation-vpc-flow-logs-are-stored-in-the-centralized-log-archive-account-logging-bucket-twice","text":"Yes. Cloudtrail is configured to send its logs directly to S3 for centralized immutable log retention. CloudTrail is also configured to send it's logs to a centralized Organizational CloudWatch Log group such that the trail can be a) easily queried online using CloudWatch Insights across all AWS accounts in the organization, and b) to enable alerting based on undesirable API activity using CloudWatch Metrics and Alarms. All CloudWatch Log groups are also configured to be sent, using Amazon Kinesis, to S3 for centralized immutable log retention. VPC flow log destinations can be configured in the config file. The example config files are set to send the VPC flow logs to both S3 and CloudWatch Logs by default for the same reasons as CloudTrail. To reduce the duplicate long-term storage of these two specific CloudWatch Log types, customers can set cwl-glbl-exclusions under central-log-services to: [\"/${ACCELERATOR_PREFIX_ND}/flowlogs/*\", \"/${ACCELERATOR_PREFIX_ND}/CloudTrail*\"] to prevent these specifically named log groups from being stored on S3. This setting also prevents the Accelerator from setting the customer desired log group retention period defined in the config file, once implemented, for those log groups. Therfore, we do not recommend this exception be applied during the initial installation, as the retention setting on these CWL groups will remain the default (infinite). If cwl-glbl-exclusions is set after initial install, the defined retention will be configured during install and will remain set to the value present when the exception was applied to those log groups. This allows logs to be stored in CloudWatch Logs for quick and easy online access (short-retention only), and stored in S3 for long-term retention and access. Side note: CloudTrail S3 data plane logs are enabled at the Organizational level, meaning all S3 bucket access is logged. As CloudTrail is writing to a bucket within the Organization, Cloudtrail itself is accessing the bucket, seemingly creating a cyclical loop. As CloudTrail writes to S3 in 5-10min batches, Cloudtrail will actually only cause one extra log 'entry' every 5-10minutes and not per s3 event, mitigating major concerns. Today, with an Organization trail logging data plane events for all buckets - there is no way to exclude any one bucket. But - having clear view of who accessed/changed logs, including AWS services, is important.","title":"I've noticed CloudTrail logs and in certain situation VPC flow logs are stored in the centralized log-archive account logging bucket twice?"},{"location":"guides/fortigate/","text":"ASEA - Fortinet Guides \u00b6 Expose Public Facing Workload via Fortigate","title":"ASEA - Fortinet Guides"},{"location":"guides/fortigate/#asea-fortinet-guides","text":"Expose Public Facing Workload via Fortigate","title":"ASEA - Fortinet Guides"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/","text":"Public Facing Workload Configuration Sample \u00b6 This page describes the steps needed to configure a public facing web application that is deployed within a workload AWS Account in the Secure Environment Accelerator (SEA). The high-level steps are the following: 1) Create a SSL public certificate in AWS Certificate Manager. 2) Create a DNS entry for the web application. 3) Create Application Load Balancer Target Groups for the web application 4) Create an Application Load Balancer Rule to forward traffic to the Firewalls. 5) Configure the Firewalls. The screenshots and steps in this page are specific to the Fortigate Firewalls. Perimeter SEA AWS Account \u00b6 SSL Certificate Configuration \u00b6 1) Within the Perimeter SEA AWS Account, navigate to the Certificate Manager service. 2) Follow the steps to request a new public certificate. This will be used to support https for the web application. Note that the SEA deploys 'example' certificates, but these should not be used at the perimeter. Here's an example showing a wildcard cert. 3) Navigate to the ALBs and select the Load Balancer that will support the incoming requests for the web application. In this example, it will be the 'Public-DevTest-perimeter-alb'. 4) Select the 'Public-DevTest-perimeter-alb' ALB and click the View/edit certificates link button. 5) Click the + button and select the new SSL Certificate. Click Add . 6) Return back to the ALBs and select the 'Public-DevTest-perimeter-alb' ALB. Select the default HTTPS listener and click Edit 7) Change the Default SSL certificate to the newly created public cert and update the settings. ALB Target Group Configuration \u00b6 1) Navigate to the EC2 Load Balancers and view the default Application Load Balancers (ALB). 2) List the ALB Target Groups These are pairs of targets (one for each firewall) that direct traffic from the perimeter ALB to the firewall. The two pairs were created as part of the default configuration and provide health checks to the shared VPCs. For support a new web application, a new pair will be created. One for each firewall (i.e. one per AZ). 1) Click the Create target group button. (Note: This will be repeated for each Firewall). 2) Enter the following parameter values: - Target group name: Public-DevTest-SampleApp-azA - Protocol: HTTPS - Port: (pick an unused port on the Firewall). Example 7006 - VPC: Perimeter_VPC 5) When Registering a target, pick the instance that aligns with the Availability Zone (AZ) that is being configured. Example: Firewall_az[A|B]. If creating 'Public-DevTest-SampleApp- azA ', then choose Firewall instance 'Firewall_ azA '. 6) Ensure that the port value is using the previous entered port value. Click the Include as pending below . 7) Click the Create target group button when ready. 8) Repeat for the additional firewalls. ALB Listener Rule Configuration \u00b6 1) Create a DNS entry for the web application that resolves to the perimeter ALB being configured. For example: webapplication.mydomain.ca resolves to 'Public-DevTest-perimeter-alb-1616856287.ca-central-1.elb.amazonaws.com' 2) Navigate to the ALBs and select the 'Public-DevTest-perimeter-alb' ALB. Click the View/edit rules link button. 3) Click the + button to create a new rule. Then click the + Insert Rule button. 4) Configure a match condition on Host header... . enter the value of the DNS entry for the web application. 5) Click the checkmark to update it. 6) Click the + Add action and select Forward to... 7) Configure both Targets using the ones previously created (one per firewall). Adjust for 50% load balanced traffic. 8) Click the checkmark to update and then click the Save button. Fortigate Firewall Configuration \u00b6 The following configuration will be executed per Firewall instance (twice with the default SEA configuration). 1) Log in to the firewall instance. 2) Switch the Virtual Domain (vdom) to FG-traffic . Navigate to Policy & Objects and select Addresses Create a new entry using the following parameter values: Name: Dev1-SampleWebApplication-ALB-FQDN Type: FQDN FQDN: (use the DNS value of the internal load balancer in front of the web application) Interface: tgw-vpn1 After saving the entry, refresh the Address grid and verify that the row colour is white. Navigate to Policy & Objects and select Virtual IPs Make note of the used ip address in the Details column. In the example above \u201c100.96.250.22\u201d. Click the CLI command icon in the top right corner. Note that the following must be done using the CLI. 9) Update the following script template replacing values for the following: name, extip, mapped-addr, extport config firewall vip edit \"Dev1-SampleWebApplication-ALB\" set type fqdn set extip 100.96.250.22 set extintf \"port1\" set portforward enable set mapped-addr \"Dev1-SampleWebApplication-ALB-FQDN\" set extport 7006 set mappedport 443 next end 10) Returning back to the UI interface shows the new entry. 11) Navigate to Policy & Objects and select IPv4 Policy and expand public (port1) 12) Locate the desired policy (ex: Dev-Test #8 in the example below). Right-click and click Edit . 13) Locate the Destination field entry and click the + button. 14) Locate the newly created VirtualIP entry (ex: Dev1-SampleWbApplication-ALB) and save the changes. NOTE: The entry is NOT the Address/FQDN entry. 15) After refreshing the page, the row background should be white, and the new destination is visible.","title":"Expose Public Facing Workload"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#public-facing-workload-configuration-sample","text":"This page describes the steps needed to configure a public facing web application that is deployed within a workload AWS Account in the Secure Environment Accelerator (SEA). The high-level steps are the following: 1) Create a SSL public certificate in AWS Certificate Manager. 2) Create a DNS entry for the web application. 3) Create Application Load Balancer Target Groups for the web application 4) Create an Application Load Balancer Rule to forward traffic to the Firewalls. 5) Configure the Firewalls. The screenshots and steps in this page are specific to the Fortigate Firewalls.","title":"Public Facing Workload Configuration Sample"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#perimeter-sea-aws-account","text":"","title":"Perimeter SEA AWS Account"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#ssl-certificate-configuration","text":"1) Within the Perimeter SEA AWS Account, navigate to the Certificate Manager service. 2) Follow the steps to request a new public certificate. This will be used to support https for the web application. Note that the SEA deploys 'example' certificates, but these should not be used at the perimeter. Here's an example showing a wildcard cert. 3) Navigate to the ALBs and select the Load Balancer that will support the incoming requests for the web application. In this example, it will be the 'Public-DevTest-perimeter-alb'. 4) Select the 'Public-DevTest-perimeter-alb' ALB and click the View/edit certificates link button. 5) Click the + button and select the new SSL Certificate. Click Add . 6) Return back to the ALBs and select the 'Public-DevTest-perimeter-alb' ALB. Select the default HTTPS listener and click Edit 7) Change the Default SSL certificate to the newly created public cert and update the settings.","title":"SSL Certificate Configuration"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#alb-target-group-configuration","text":"1) Navigate to the EC2 Load Balancers and view the default Application Load Balancers (ALB). 2) List the ALB Target Groups These are pairs of targets (one for each firewall) that direct traffic from the perimeter ALB to the firewall. The two pairs were created as part of the default configuration and provide health checks to the shared VPCs. For support a new web application, a new pair will be created. One for each firewall (i.e. one per AZ). 1) Click the Create target group button. (Note: This will be repeated for each Firewall). 2) Enter the following parameter values: - Target group name: Public-DevTest-SampleApp-azA - Protocol: HTTPS - Port: (pick an unused port on the Firewall). Example 7006 - VPC: Perimeter_VPC 5) When Registering a target, pick the instance that aligns with the Availability Zone (AZ) that is being configured. Example: Firewall_az[A|B]. If creating 'Public-DevTest-SampleApp- azA ', then choose Firewall instance 'Firewall_ azA '. 6) Ensure that the port value is using the previous entered port value. Click the Include as pending below . 7) Click the Create target group button when ready. 8) Repeat for the additional firewalls.","title":"ALB Target Group Configuration"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#alb-listener-rule-configuration","text":"1) Create a DNS entry for the web application that resolves to the perimeter ALB being configured. For example: webapplication.mydomain.ca resolves to 'Public-DevTest-perimeter-alb-1616856287.ca-central-1.elb.amazonaws.com' 2) Navigate to the ALBs and select the 'Public-DevTest-perimeter-alb' ALB. Click the View/edit rules link button. 3) Click the + button to create a new rule. Then click the + Insert Rule button. 4) Configure a match condition on Host header... . enter the value of the DNS entry for the web application. 5) Click the checkmark to update it. 6) Click the + Add action and select Forward to... 7) Configure both Targets using the ones previously created (one per firewall). Adjust for 50% load balanced traffic. 8) Click the checkmark to update and then click the Save button.","title":"ALB Listener Rule Configuration"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#fortigate-firewall-configuration","text":"The following configuration will be executed per Firewall instance (twice with the default SEA configuration). 1) Log in to the firewall instance. 2) Switch the Virtual Domain (vdom) to FG-traffic . Navigate to Policy & Objects and select Addresses Create a new entry using the following parameter values: Name: Dev1-SampleWebApplication-ALB-FQDN Type: FQDN FQDN: (use the DNS value of the internal load balancer in front of the web application) Interface: tgw-vpn1 After saving the entry, refresh the Address grid and verify that the row colour is white. Navigate to Policy & Objects and select Virtual IPs Make note of the used ip address in the Details column. In the example above \u201c100.96.250.22\u201d. Click the CLI command icon in the top right corner. Note that the following must be done using the CLI. 9) Update the following script template replacing values for the following: name, extip, mapped-addr, extport config firewall vip edit \"Dev1-SampleWebApplication-ALB\" set type fqdn set extip 100.96.250.22 set extintf \"port1\" set portforward enable set mapped-addr \"Dev1-SampleWebApplication-ALB-FQDN\" set extport 7006 set mappedport 443 next end 10) Returning back to the UI interface shows the new entry. 11) Navigate to Policy & Objects and select IPv4 Policy and expand public (port1) 12) Locate the desired policy (ex: Dev-Test #8 in the example below). Right-click and click Edit . 13) Locate the Destination field entry and click the + button. 14) Locate the newly created VirtualIP entry (ex: Dev1-SampleWbApplication-ALB) and save the changes. NOTE: The entry is NOT the Address/FQDN entry. 15) After refreshing the page, the row background should be white, and the new destination is visible.","title":"Fortigate Firewall Configuration"},{"location":"installation/","text":"Installation and Upgrades \u00b6 This section contains information on the installation and upgrade procedures for ASEA. Installation Upgrades Existing Orgs & Accounts Design Decisions Multi-file Config Capabilities Accelerator Object Naming State Machine Behavior and Inputs ASEA 1.5.0 Specific Upgrade Instructions Key Account and Capability Overview","title":"Installation and Upgrades"},{"location":"installation/#installation-and-upgrades","text":"This section contains information on the installation and upgrade procedures for ASEA. Installation Upgrades Existing Orgs & Accounts Design Decisions Multi-file Config Capabilities Accelerator Object Naming State Machine Behavior and Inputs ASEA 1.5.0 Specific Upgrade Instructions Key Account and Capability Overview","title":"Installation and Upgrades"},{"location":"installation/customization-index/","text":"Accelerator Configuration File Customization and Sample Configs \u00b6 Sample Accelerator Configuration Files \u00b6 Summary Sample config files can be found in this folder Most of the examples reflect a Medium Security profile (NIST, ITSG, FEDRAMP) Unsure where to start, use the config.lite-CTNFW-example.json file (the CT w/NFW variant of option 2) Frugal and want something comprehensive to experiment with, use the config.test-example.json file (option 5) Config file schema ) documentation (Draft) Estimated monthly pricing of example configurations Samples with Descriptions Full configuration file ( config.example.json ) The full configuration file was based on feedback from customers moving into AWS at scale and at a rapid pace. Customers of this nature have indicated that they do not want to have to upsize their perimeter firewalls or add Interface endpoints as their developers start to use new AWS services. These are the two most expensive components of the deployed architecture solution. Default settings: AWS Control Tower: No Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP) Lite weight configuration files Four variants with differing central ingress/egress firewalls Variant 1: Recommended starting point ( config.lite-CTNFW-example.json ) Default Settings: AWS Control Tower: Yes Firewall: AWS Network Firewall Variant 2: Recommended for new GC PBMM customers ( config.lite-VPN-example.json ) requires 3rd party licensing (BYOL or PAYGO) Default Settings: AWS Control Tower: No Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP) Variant 3: ( config.lite-NFW-example.json ) Same as Variant 1 config without AWS Control Tower Default Settings: AWS Control Tower: No Firewall: AWS Network Firewall Variant 4: ( config.lite-GWLB-example.json ) requires 3rd party licensing (BYOL or PAYGO) Default Settings: AWS Control Tower: No Firewall: Gateway Load Balancer with Checkpoint firewalls in an autoscaling group To reduce solution costs and allow customers to grow into more advanced AWS capabilities, we created these lite weight configurations that does not sacrifice functionality, but could limit performance. These config files: only deploys the 9 required centralized Interface Endpoints (removes 50 from full config). All services remain accessible using the AWS public endpoints, but require traversing the perimeter firewalls removes the perimeter VPC Interface Endpoints reduces the Fortigate instance sizes from c5n.2xl to c5n.xl (VM08 to VM04) in Variant 2: IPSec VPN with Active/Active Fortinet cluster option removes the Unclass ou and VPC AWS Control Tower can be implemented in all sample configs using Variant 1: AWS Control Tower with AWS Network Firewall as an example (new installs only). The Accelerator allows customers to easily add or change this functionality in future, as and when required without any impact Ultra-Lite sample configuration Variant 1: ( config.ultralite-CT-example.json ) AWS Control Tower: Yes Firewall: None Networking: None Variant 2 : ( config.ultralite-example.json ) AWS Control Tower: No Firewall: None Networking: None This configuration file was created to represent an extremely minimalistic Accelerator deployment, simply to demonstrate the art of the possible for an extremely simple config. This example is NOT recommended as it violates many AWS best practices. This This config has: no shared-network or perimeter accounts no networking (VPC, TGW, ELB, SG, NACL, endpoints) or route53 (zones, resolvers) objects no Managed AD, AD Connector, rsyslog cluster, RDGW host, or 3rd party firewalls only enables/deploys AWS security services in 2 regions (ca-central-1, us-east-1) (Not recommended) only deploys 2 AWS config rules w/SSM remediation renamed log-archive (Logs), security (Audit) and operations (Ops) account names Multi-Region sample configuration file ( config.multi-region-example.json ) This configuration file was created to represent a more advanced multi-region version of the Full configuration file from bullet 1 above. This config: adds a TGW in us-east-1, peered to the TGW in ca-central-1 adds TGW static routes, including several dummy sample static routes adds a central Endpoint VPC in us-east-1 with us-east-1 endpoints configured adds a shared VPC for all UnClass OU accounts in us-east-1, connected to the us-east-1 TGW (accessible through ca-central-1) creates additional zones and resolver rules Sends us-east-1 CloudWatch Logs to the central S3 log-archive bucket in ca-central-1 Deploys SSM documents to us-east-1 and remediates configured rules in UnClass OU adds a local account specific VPC, in us-east-1, in the account MyUnClass and connects it to the us-east-1 TGW (i.e. shares TGW) local account VPC set to use central endpoints, associates appropriate centralized hosted zones to VPC (also creates 5 local endpoints) adds a VGW for DirectConnect to the perimeter VPC adds the 3rd AZ in ca-central-1 (MAD & ADC in AZ a & b) Default Settings: AWS Control Tower: No Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP) Test configuration file ( config.test-example.json ) (Use for testing Full/Lite configurations or Low Security Profiles) Further reduces solution costs, while demonstrating full solution functionality (NOT recommendend for production). This config file: uses the Lite weight configuration as the starting point (NFW variant) consolidates Dev/Test/Prod OU to a single Workloads OU/VPC only enables Security Hub, Config and Macie in ca-central-1 and us-east-1 removes the Fortigate firewall cluster (per NFW variant) removes the rsyslog cluster reduces the RDGW instance sizes from t2.large to t2.medium reduces the size of the MAD from Enterprise to Standard edition removes the on-premise R53 resolvers (hybrid dns) reduced various log retention periods and the VPCFlow log interval removes the two example workload accounts adds AWS Network Firewall (NFW) and AWS NATGW for centralized ingress/egress (per NFW variant) Default Settings: AWS Control Tower: No Firewall: AWS Network Firewall Deployment Customizations \u00b6 Multi-file config file and YAML formatting option The sample configuration files are provided as single, all encompassing, json files. The Accelerator also supports both splitting the config file into multiple component files and configuration files built using YAML instead of json. Details can be found in the linked document. Sample Snippets The sample configuration files do not include the full range of supported configuration file parameters and values, additional configuration file parameters and values can be found in the sample snippets document. Third Party Firewall example configs: The Accelerator is provided with a sample 3rd party configuration file to demonstrate automated deployment of 3rd party firewall technologies. Given the code is vendor agnostic, this process should be able to be leveraged to deploy other vendors firewall appliances. When and if other options become available, we will add them here as well. Automated firewall configuration customization possibilities Sample Fortinet Fortigate firewall config file Other Configuration File Hints and Tips \u00b6 It is critical that all accounts that are leveraged by other accounts (i.e. accounts that any workload accounts are dependant on), are included in the mandatory-accounts section of the config file (i.e. shared-network, log-archive, operations) Account pointers within the config file point to the account key (i.e. ( mandatory-account-configs\\account-key ) and NOT the account name field ( mandatory-account-configs\\account-key\\account-name: \"account name\" ). This allows for easy account names, duplicate account names, and no requirement to update account pointers during account renames. If any of the account pointers within global-options does not point to a valid mandatory account key, the State Machine will fail with the error EnvironmentVariable value cannot be null before starting CodeBuild Phase -1 You cannot supply (or change) configuration file values to something not supported by the AWS platform For example, CWL retention only supports specific retention values (not any number) Shard count - can only increase/reduce by half the current limit. i.e. you can change from 1 - 2 , 2 - 3 , 4 - 6 Always add any new items to the END of all lists or sections in the config file, otherwise Update validation checks will fail (VPC's, subnets, share-to, etc.) To skip, remove or uninstall a component, you can often simply change the section header, instead of removing the section change \"deployments\"/\"firewalls\" to \"deployments\"/\"xxfirewalls\" and it will uninstall the firewalls and maintain the old config file settings for future use Objects with the parameter deploy: true, support setting the value to false to remove the deployment As you grow and add AWS accounts, the Kinesis Data stream in the log-archive account will need to be monitored and have its capacity (shard count) increased by setting \"kinesis-stream-shard-count\" variable under \"central-log-services\" in the config file Updates to NACL's requires changing the rule number ( 100 to 101 ) or they will fail to update When adding a new subnet or subnets to a VPC (including enabling an additional AZ), you need to: increment any impacted NACL id's in the config file ( 100 to 101 , 32000 to 32001 ) (CFN does not allow nacl updates) make a minor change to any impacted route table names ( MyRouteTable to MyRouteTable1 ) (CFN does not allow updates to route table associated ids) The sample VPN firewall configuration uses an instance with 4 NIC's, make sure you use an instance size that supports 4 ENI's Firewall names, CGW names, TGW names, MAD Directory ID, account keys, and OU's must all be unique throughout the entire configuration file (also true for VPC names given NACL and security group referencing design) The configuration file does have validation checks in place that prevent users from making certain major unsupported configuration changes The configuration file does NOT have extensive error checking. It is expected you know what you are doing. We eventually hope to offer a config file, wizard based GUI editor and add the validation logic in this separate tool. In most cases the State Machine will fail with an error, and you will simply need to troubleshoot, rectify and rerun the state machine. You cannot move an account between top-level OU's. This would be a security violation and cause other issues. You can move accounts between sub-ou. Note: The Control Tower version of the Accelerator does NOT support sub-ou's. When using YAML configuration files, we only support the subset of yaml that converts to JSON (we do not support anchors) Security Group names were designed to be identical between environments, if you want the VPC name in the SG name, you need to do it manually in the config file Adding more than approximately 50 new VPC Interface Endpoints across all regions in any one account in any single state machine execution will cause the state machine to fail due to Route 53 throttling errors. If adding endpoints at scale, only deploy 1 region at a time. In this scenario, the stack(s) will fail to properly delete, also based on the throttling, and will require manual removal. We do not support Directory unsharing or ADC deletion, delete methods were not implemented. We only support ADC creation in mandatory accounts. If use-central-endpoints is changed from true to false, you cannot add a local vpc endpoint on the same state machine execution (add the endpoint on a prior or subsequent execution) If you update the 3rd party firewall names, be sure to update the routes and alb's which point to them. Firewall licensing occurs through the management port, which requires a VPC route back to the firewall to get internet access and validate the firewall license. Removing the AWS NFW requires 2 state machine executions, in the first you must remove all routes that reference the NFW, and in the second you can remove or xx out the NFW (also true for the GWLB implementation ). Config file and Deployment Protections \u00b6 The config file is moved to AWS CodeCommit after the first execution of the state machine to provide strong configuration history, versioning and change control After each successful state machine execution, we record the commit id of the config file used for that execution in secrets manager On every state machine execution, before making any changes, the Accelerator compares the latest version of the config file stored in CodeCommit with the version of the config file from the last successful state machine execution (after replacing all variables) If the config file includes any changes we consider to be significant or breaking, we immediately fail the state machine if a customer somehow accidentally uploads a different customers config file into their Accelerator CodeCommit repository, the state machine will fail if a customer makes what we consider to be a major change to the config file, the state machine will fail if a customer makes a change that we believe has a high likelihood to cause a deployment failure, the state machine will fail If a customer believes they understand the full implications of the changes they are making (and has made any required manual changes to allow successful execution), we have provided protection override flags. These overrides should be used with extremely caution: To provide maximum protection we have provided scoped override flags. Customers can provide a flag or flags to only bypass specific type(s) of config file validations or blocks. If using an override flag, we recommend customers use these scoped flags in most situations. If a customer is purposefully making extensive changes across the config file and wants to simply override all checks with a single override flag, we also have this option, but discourage it use. The various override flags and their format can be found in here . Summary of Example Config File Minimum Changes for New Installs \u00b6 At a minimum you should consider reviewing the following config file sections and make the required changes. Global Options \u00b6 S3 Central Bucket global-options/central-bucket : \"AWSDOC-EXAMPLE-BUCKET\" replace with your-bucket-name as referenced in the Installation Guide S3 Creation - Step #5 Central Log Services SNS Emails global-options/central-log-services/sns-subscription-emails : \"myemail+notifyT-xxx@example.com\" update the 3 email addresses (high, medium and low) as required. Each address will receives alerts or alarms of the specified level. The same email address can be used for all three. The default dynamic CIDR pools ( global-options/cidr-pools ) listed below are used to assign ranges based on the subnet mask set in each VPC and subnet throughout the configuration file. global-options/cidr-pools/0/cidr : \"10.0.0.0/13\" The main address pool used to dynamically assign CIDR ranges for most VPCs global-options/cidr-pools/1/cidr : \"100.96.252.0/23\" Address pool used to dynamically assign CIDR ranges for the Managed Active Directory subnets in the Ops account global-options/cidr-pools/2/cidr : \"100.96.250.0/23\" Address pool used to dynamically assign CIDR ranges for the Perimeter VPC global-options/cidr-pools/3/cidr : \"10.249.1.0/24\" A non-routable pool of addresses used to dynamically assign CIDR ranges for the Active Directory Connector subnets in the Organization Management/root account Mandatory Account Configs \u00b6 All mandatory accounts specific to your config file, that are present under the mandatory-account-config section require you to assign a unique email address for each account listed below. Replace the email values in the JSON config file for these accounts with unique email addresses. mandatory-account-configs/shared-network/email : \"myemail+aseaT-network@example.com---------------------REPLACE------------\" mandatory-account-configs/operations/email : \"myemail+aseaT-operations@example.com---------------------REPLACE------------\" mandatory-account-configs/perimeter/email : \"myemail+aseaT-perimeter@example.com---------------------REPLACE------------\" mandatory-account-configs/management/email : \"myemail+aseaT-management@example.com---------------------REPLACE------------\" (Note: This is the email of your root account) mandatory-account-configs/log-archive/email : \"myemail+aseaT-log@example.com---------------------REPLACE------------\" mandatory-account-configs/security/email : \"myemail+aseaT-sec@example.com---------------------REPLACE------------\" Budget Alerts email addresses need to be replaced with an email address in your organization. It can be the same email address for all budget alerts. Config located at the following path (Multiple exist for different thresholds, update all under each account): mandatory-account-configs/shared-network/budget/alerts/emails : \"myemail+aseaT-budg@example.com\" mandatory-account-configs/perimeter/budget/alerts/emails : \"myemail+aseaT-budg@example.com\" mandatory-account-configs/management/budget/alerts/emails : \"myemail+aseaT-budg@example.com\" For the shared-network account, review and update the following (or delete the sections): mandatory-account-configs/shared-network/vpc/on-premise-rules/zone : \"on-premise-privatedomain1.example.ca\" (qty 2) mandatory-account-configs/shared-network/vpc/zones/private : \"cloud-hosted-privatedomain.example.ca\" mandatory-account-configs/shared-network/vpc/zones/public : \"cloud-hosted-publicdomain.example.ca\" For the operations account, review and update the following: mandatory-account-configs/operations/deployments/mad/dns-domain : \"example.local\" mandatory-account-configs/operations/deployments/mad/netbios-domain : \"example\" mandatory-account-configs/operations/deployments/mad/log-group-name : \"/${ACCELERATOR_PREFIX_ND}/MAD/example.local\" (replace example.local) mandatory-account-configs/operations/deployments/mad/ad-users (update user, email and group of each user as required) do not remove or change permissions on the adconnector-usr For perimeter account, review and update the following: mandatory-account-configs/perimeter/certificates/priv-key : \"certs/example1-cert.key\" mandatory-account-configs/perimeter/certificates/cert : \"certs/example1-cert.crt\" If you are using VPN config: mandatory-account-configs/perimeter/deployments/firewalls/image-id : \"ami-0d8e2e78e928def11\" Update AMI with the AMI collected from the Marketplace for Fortinet FortiGate (BYOL) Next-Generation Firewall mandatory-account-configs/perimeter/deployments/xxfirewall-manager/image-id : \"ami-0e9f45c3ec34c3a9a\" Update AMI with the AMI collected from the Marketplace for Fortinet FortiManager (BYOL) Centralized Security Management NOTE: Default config of \"xxfirewall-manager\" will prevent the firewall manager from being deployed. To deploy the firewall manager remove the \"xx\" to set the parameter to \"firewall-manager\" mandatory-account-configs/perimeter/deployments/firewalls/license : [\"firewall/license1.lic\", \"firewall/license2.lic\"] Two Fortinet FortiGate firewall licenses, if you don't have any license files, update the config file with an empty array (\"license\": []). Do NOT use the following: [\"\"] Place files in a folder (eg. firewall) in the same S3 bucket in your Organization Management account as the deployment configuration file. mandatory-account-configs/perimeter/deployments/firewalls/config : \"firewall/firewall-example.txt\" The Fortinet configuration file to initially configure the firewalls. Sample configuration files can be found in the reference-artifacts/Third-Party folder Place file in a folder (eg. firewall) in the same S3 bucket in your Organization Management account as the deployment configuration file. If you are using GWLB config: mandatory-account-configs/perimeter/deployments/firewalls/Checkpoint-Firewall - image-id : \"ami-0217611bf09d5b4c1\" Update AMI with the AMI collected from the Marketplace for CloudGuard Network Security for GWLB - BYOL mandatory-account-configs/perimeter/deployments/firewall-manager/image-id : \"ami-0071a3b4ef9ac766a\" Update AMI with the AMI collected from the Marketplace for Checkpoint Security Management mandatory-account-configs/perimeter/deployments/firewall-manager/version : \"R8110BYOLMGMT\" Update version based on the selected ami-id version from the Private Marketplace For management , review and update the following: mandatory-account-configs/management/account-name : \"ASEA-Main\" Update this field with your Organization Management (root) account name, if it is not set to ASEA-Main. mandatory-account-configs/management/iam/users the names of your break-glass and ASEA operation users Workload Account Configs \u00b6 As mentioned in the Installation Guide, we recommend not adding more than 1 or 2 workload accounts to the config file during the initial deployment as it will increase risks of hitting a limit. Once the Accelerator is successfully deployed, add the additional accounts back into the config file and rerun the state machine. Review the workload accounts in the config that you selected and change the name and email as desired Modify mydevacct1 with the account name of your choosing Modify mydevacct1/account-name : \"MyDev1\" with the account name Modify mydevacct1/email : \"myemail+aseaT-dev1@example.com---------------------REPLACE------------\" with a unique email address for the account Modify mydevacct1/description : \"This is an OPTIONAL SAMPLE workload account...\" with a description relevant to your account Modify mydevacct1/ou : \"Dev\" with the OU that you would like the account to be attached to Organization Units \u00b6 For all organization units, update the budget alerts email addresses: organizational-units/core/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Central/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Dev/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Test/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Prod/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Sandbox/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" For organization units with certificates , review the certificates and update as you see fit. These certificates are used in the alb section under alb/cert-name of each OU","title":"Accelerator Configuration File Customization and Sample Configs"},{"location":"installation/customization-index/#accelerator-configuration-file-customization-and-sample-configs","text":"","title":"Accelerator Configuration File Customization and Sample Configs"},{"location":"installation/customization-index/#sample-accelerator-configuration-files","text":"Summary Sample config files can be found in this folder Most of the examples reflect a Medium Security profile (NIST, ITSG, FEDRAMP) Unsure where to start, use the config.lite-CTNFW-example.json file (the CT w/NFW variant of option 2) Frugal and want something comprehensive to experiment with, use the config.test-example.json file (option 5) Config file schema ) documentation (Draft) Estimated monthly pricing of example configurations Samples with Descriptions Full configuration file ( config.example.json ) The full configuration file was based on feedback from customers moving into AWS at scale and at a rapid pace. Customers of this nature have indicated that they do not want to have to upsize their perimeter firewalls or add Interface endpoints as their developers start to use new AWS services. These are the two most expensive components of the deployed architecture solution. Default settings: AWS Control Tower: No Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP) Lite weight configuration files Four variants with differing central ingress/egress firewalls Variant 1: Recommended starting point ( config.lite-CTNFW-example.json ) Default Settings: AWS Control Tower: Yes Firewall: AWS Network Firewall Variant 2: Recommended for new GC PBMM customers ( config.lite-VPN-example.json ) requires 3rd party licensing (BYOL or PAYGO) Default Settings: AWS Control Tower: No Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP) Variant 3: ( config.lite-NFW-example.json ) Same as Variant 1 config without AWS Control Tower Default Settings: AWS Control Tower: No Firewall: AWS Network Firewall Variant 4: ( config.lite-GWLB-example.json ) requires 3rd party licensing (BYOL or PAYGO) Default Settings: AWS Control Tower: No Firewall: Gateway Load Balancer with Checkpoint firewalls in an autoscaling group To reduce solution costs and allow customers to grow into more advanced AWS capabilities, we created these lite weight configurations that does not sacrifice functionality, but could limit performance. These config files: only deploys the 9 required centralized Interface Endpoints (removes 50 from full config). All services remain accessible using the AWS public endpoints, but require traversing the perimeter firewalls removes the perimeter VPC Interface Endpoints reduces the Fortigate instance sizes from c5n.2xl to c5n.xl (VM08 to VM04) in Variant 2: IPSec VPN with Active/Active Fortinet cluster option removes the Unclass ou and VPC AWS Control Tower can be implemented in all sample configs using Variant 1: AWS Control Tower with AWS Network Firewall as an example (new installs only). The Accelerator allows customers to easily add or change this functionality in future, as and when required without any impact Ultra-Lite sample configuration Variant 1: ( config.ultralite-CT-example.json ) AWS Control Tower: Yes Firewall: None Networking: None Variant 2 : ( config.ultralite-example.json ) AWS Control Tower: No Firewall: None Networking: None This configuration file was created to represent an extremely minimalistic Accelerator deployment, simply to demonstrate the art of the possible for an extremely simple config. This example is NOT recommended as it violates many AWS best practices. This This config has: no shared-network or perimeter accounts no networking (VPC, TGW, ELB, SG, NACL, endpoints) or route53 (zones, resolvers) objects no Managed AD, AD Connector, rsyslog cluster, RDGW host, or 3rd party firewalls only enables/deploys AWS security services in 2 regions (ca-central-1, us-east-1) (Not recommended) only deploys 2 AWS config rules w/SSM remediation renamed log-archive (Logs), security (Audit) and operations (Ops) account names Multi-Region sample configuration file ( config.multi-region-example.json ) This configuration file was created to represent a more advanced multi-region version of the Full configuration file from bullet 1 above. This config: adds a TGW in us-east-1, peered to the TGW in ca-central-1 adds TGW static routes, including several dummy sample static routes adds a central Endpoint VPC in us-east-1 with us-east-1 endpoints configured adds a shared VPC for all UnClass OU accounts in us-east-1, connected to the us-east-1 TGW (accessible through ca-central-1) creates additional zones and resolver rules Sends us-east-1 CloudWatch Logs to the central S3 log-archive bucket in ca-central-1 Deploys SSM documents to us-east-1 and remediates configured rules in UnClass OU adds a local account specific VPC, in us-east-1, in the account MyUnClass and connects it to the us-east-1 TGW (i.e. shares TGW) local account VPC set to use central endpoints, associates appropriate centralized hosted zones to VPC (also creates 5 local endpoints) adds a VGW for DirectConnect to the perimeter VPC adds the 3rd AZ in ca-central-1 (MAD & ADC in AZ a & b) Default Settings: AWS Control Tower: No Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP) Test configuration file ( config.test-example.json ) (Use for testing Full/Lite configurations or Low Security Profiles) Further reduces solution costs, while demonstrating full solution functionality (NOT recommendend for production). This config file: uses the Lite weight configuration as the starting point (NFW variant) consolidates Dev/Test/Prod OU to a single Workloads OU/VPC only enables Security Hub, Config and Macie in ca-central-1 and us-east-1 removes the Fortigate firewall cluster (per NFW variant) removes the rsyslog cluster reduces the RDGW instance sizes from t2.large to t2.medium reduces the size of the MAD from Enterprise to Standard edition removes the on-premise R53 resolvers (hybrid dns) reduced various log retention periods and the VPCFlow log interval removes the two example workload accounts adds AWS Network Firewall (NFW) and AWS NATGW for centralized ingress/egress (per NFW variant) Default Settings: AWS Control Tower: No Firewall: AWS Network Firewall","title":"Sample Accelerator Configuration Files"},{"location":"installation/customization-index/#deployment-customizations","text":"Multi-file config file and YAML formatting option The sample configuration files are provided as single, all encompassing, json files. The Accelerator also supports both splitting the config file into multiple component files and configuration files built using YAML instead of json. Details can be found in the linked document. Sample Snippets The sample configuration files do not include the full range of supported configuration file parameters and values, additional configuration file parameters and values can be found in the sample snippets document. Third Party Firewall example configs: The Accelerator is provided with a sample 3rd party configuration file to demonstrate automated deployment of 3rd party firewall technologies. Given the code is vendor agnostic, this process should be able to be leveraged to deploy other vendors firewall appliances. When and if other options become available, we will add them here as well. Automated firewall configuration customization possibilities Sample Fortinet Fortigate firewall config file","title":"Deployment Customizations"},{"location":"installation/customization-index/#other-configuration-file-hints-and-tips","text":"It is critical that all accounts that are leveraged by other accounts (i.e. accounts that any workload accounts are dependant on), are included in the mandatory-accounts section of the config file (i.e. shared-network, log-archive, operations) Account pointers within the config file point to the account key (i.e. ( mandatory-account-configs\\account-key ) and NOT the account name field ( mandatory-account-configs\\account-key\\account-name: \"account name\" ). This allows for easy account names, duplicate account names, and no requirement to update account pointers during account renames. If any of the account pointers within global-options does not point to a valid mandatory account key, the State Machine will fail with the error EnvironmentVariable value cannot be null before starting CodeBuild Phase -1 You cannot supply (or change) configuration file values to something not supported by the AWS platform For example, CWL retention only supports specific retention values (not any number) Shard count - can only increase/reduce by half the current limit. i.e. you can change from 1 - 2 , 2 - 3 , 4 - 6 Always add any new items to the END of all lists or sections in the config file, otherwise Update validation checks will fail (VPC's, subnets, share-to, etc.) To skip, remove or uninstall a component, you can often simply change the section header, instead of removing the section change \"deployments\"/\"firewalls\" to \"deployments\"/\"xxfirewalls\" and it will uninstall the firewalls and maintain the old config file settings for future use Objects with the parameter deploy: true, support setting the value to false to remove the deployment As you grow and add AWS accounts, the Kinesis Data stream in the log-archive account will need to be monitored and have its capacity (shard count) increased by setting \"kinesis-stream-shard-count\" variable under \"central-log-services\" in the config file Updates to NACL's requires changing the rule number ( 100 to 101 ) or they will fail to update When adding a new subnet or subnets to a VPC (including enabling an additional AZ), you need to: increment any impacted NACL id's in the config file ( 100 to 101 , 32000 to 32001 ) (CFN does not allow nacl updates) make a minor change to any impacted route table names ( MyRouteTable to MyRouteTable1 ) (CFN does not allow updates to route table associated ids) The sample VPN firewall configuration uses an instance with 4 NIC's, make sure you use an instance size that supports 4 ENI's Firewall names, CGW names, TGW names, MAD Directory ID, account keys, and OU's must all be unique throughout the entire configuration file (also true for VPC names given NACL and security group referencing design) The configuration file does have validation checks in place that prevent users from making certain major unsupported configuration changes The configuration file does NOT have extensive error checking. It is expected you know what you are doing. We eventually hope to offer a config file, wizard based GUI editor and add the validation logic in this separate tool. In most cases the State Machine will fail with an error, and you will simply need to troubleshoot, rectify and rerun the state machine. You cannot move an account between top-level OU's. This would be a security violation and cause other issues. You can move accounts between sub-ou. Note: The Control Tower version of the Accelerator does NOT support sub-ou's. When using YAML configuration files, we only support the subset of yaml that converts to JSON (we do not support anchors) Security Group names were designed to be identical between environments, if you want the VPC name in the SG name, you need to do it manually in the config file Adding more than approximately 50 new VPC Interface Endpoints across all regions in any one account in any single state machine execution will cause the state machine to fail due to Route 53 throttling errors. If adding endpoints at scale, only deploy 1 region at a time. In this scenario, the stack(s) will fail to properly delete, also based on the throttling, and will require manual removal. We do not support Directory unsharing or ADC deletion, delete methods were not implemented. We only support ADC creation in mandatory accounts. If use-central-endpoints is changed from true to false, you cannot add a local vpc endpoint on the same state machine execution (add the endpoint on a prior or subsequent execution) If you update the 3rd party firewall names, be sure to update the routes and alb's which point to them. Firewall licensing occurs through the management port, which requires a VPC route back to the firewall to get internet access and validate the firewall license. Removing the AWS NFW requires 2 state machine executions, in the first you must remove all routes that reference the NFW, and in the second you can remove or xx out the NFW (also true for the GWLB implementation ).","title":"Other Configuration File Hints and Tips"},{"location":"installation/customization-index/#config-file-and-deployment-protections","text":"The config file is moved to AWS CodeCommit after the first execution of the state machine to provide strong configuration history, versioning and change control After each successful state machine execution, we record the commit id of the config file used for that execution in secrets manager On every state machine execution, before making any changes, the Accelerator compares the latest version of the config file stored in CodeCommit with the version of the config file from the last successful state machine execution (after replacing all variables) If the config file includes any changes we consider to be significant or breaking, we immediately fail the state machine if a customer somehow accidentally uploads a different customers config file into their Accelerator CodeCommit repository, the state machine will fail if a customer makes what we consider to be a major change to the config file, the state machine will fail if a customer makes a change that we believe has a high likelihood to cause a deployment failure, the state machine will fail If a customer believes they understand the full implications of the changes they are making (and has made any required manual changes to allow successful execution), we have provided protection override flags. These overrides should be used with extremely caution: To provide maximum protection we have provided scoped override flags. Customers can provide a flag or flags to only bypass specific type(s) of config file validations or blocks. If using an override flag, we recommend customers use these scoped flags in most situations. If a customer is purposefully making extensive changes across the config file and wants to simply override all checks with a single override flag, we also have this option, but discourage it use. The various override flags and their format can be found in here .","title":"Config file and Deployment Protections"},{"location":"installation/customization-index/#summary-of-example-config-file-minimum-changes-for-new-installs","text":"At a minimum you should consider reviewing the following config file sections and make the required changes.","title":"Summary of Example Config File Minimum Changes for New Installs"},{"location":"installation/customization-index/#global-options","text":"S3 Central Bucket global-options/central-bucket : \"AWSDOC-EXAMPLE-BUCKET\" replace with your-bucket-name as referenced in the Installation Guide S3 Creation - Step #5 Central Log Services SNS Emails global-options/central-log-services/sns-subscription-emails : \"myemail+notifyT-xxx@example.com\" update the 3 email addresses (high, medium and low) as required. Each address will receives alerts or alarms of the specified level. The same email address can be used for all three. The default dynamic CIDR pools ( global-options/cidr-pools ) listed below are used to assign ranges based on the subnet mask set in each VPC and subnet throughout the configuration file. global-options/cidr-pools/0/cidr : \"10.0.0.0/13\" The main address pool used to dynamically assign CIDR ranges for most VPCs global-options/cidr-pools/1/cidr : \"100.96.252.0/23\" Address pool used to dynamically assign CIDR ranges for the Managed Active Directory subnets in the Ops account global-options/cidr-pools/2/cidr : \"100.96.250.0/23\" Address pool used to dynamically assign CIDR ranges for the Perimeter VPC global-options/cidr-pools/3/cidr : \"10.249.1.0/24\" A non-routable pool of addresses used to dynamically assign CIDR ranges for the Active Directory Connector subnets in the Organization Management/root account","title":"Global Options"},{"location":"installation/customization-index/#mandatory-account-configs","text":"All mandatory accounts specific to your config file, that are present under the mandatory-account-config section require you to assign a unique email address for each account listed below. Replace the email values in the JSON config file for these accounts with unique email addresses. mandatory-account-configs/shared-network/email : \"myemail+aseaT-network@example.com---------------------REPLACE------------\" mandatory-account-configs/operations/email : \"myemail+aseaT-operations@example.com---------------------REPLACE------------\" mandatory-account-configs/perimeter/email : \"myemail+aseaT-perimeter@example.com---------------------REPLACE------------\" mandatory-account-configs/management/email : \"myemail+aseaT-management@example.com---------------------REPLACE------------\" (Note: This is the email of your root account) mandatory-account-configs/log-archive/email : \"myemail+aseaT-log@example.com---------------------REPLACE------------\" mandatory-account-configs/security/email : \"myemail+aseaT-sec@example.com---------------------REPLACE------------\" Budget Alerts email addresses need to be replaced with an email address in your organization. It can be the same email address for all budget alerts. Config located at the following path (Multiple exist for different thresholds, update all under each account): mandatory-account-configs/shared-network/budget/alerts/emails : \"myemail+aseaT-budg@example.com\" mandatory-account-configs/perimeter/budget/alerts/emails : \"myemail+aseaT-budg@example.com\" mandatory-account-configs/management/budget/alerts/emails : \"myemail+aseaT-budg@example.com\" For the shared-network account, review and update the following (or delete the sections): mandatory-account-configs/shared-network/vpc/on-premise-rules/zone : \"on-premise-privatedomain1.example.ca\" (qty 2) mandatory-account-configs/shared-network/vpc/zones/private : \"cloud-hosted-privatedomain.example.ca\" mandatory-account-configs/shared-network/vpc/zones/public : \"cloud-hosted-publicdomain.example.ca\" For the operations account, review and update the following: mandatory-account-configs/operations/deployments/mad/dns-domain : \"example.local\" mandatory-account-configs/operations/deployments/mad/netbios-domain : \"example\" mandatory-account-configs/operations/deployments/mad/log-group-name : \"/${ACCELERATOR_PREFIX_ND}/MAD/example.local\" (replace example.local) mandatory-account-configs/operations/deployments/mad/ad-users (update user, email and group of each user as required) do not remove or change permissions on the adconnector-usr For perimeter account, review and update the following: mandatory-account-configs/perimeter/certificates/priv-key : \"certs/example1-cert.key\" mandatory-account-configs/perimeter/certificates/cert : \"certs/example1-cert.crt\" If you are using VPN config: mandatory-account-configs/perimeter/deployments/firewalls/image-id : \"ami-0d8e2e78e928def11\" Update AMI with the AMI collected from the Marketplace for Fortinet FortiGate (BYOL) Next-Generation Firewall mandatory-account-configs/perimeter/deployments/xxfirewall-manager/image-id : \"ami-0e9f45c3ec34c3a9a\" Update AMI with the AMI collected from the Marketplace for Fortinet FortiManager (BYOL) Centralized Security Management NOTE: Default config of \"xxfirewall-manager\" will prevent the firewall manager from being deployed. To deploy the firewall manager remove the \"xx\" to set the parameter to \"firewall-manager\" mandatory-account-configs/perimeter/deployments/firewalls/license : [\"firewall/license1.lic\", \"firewall/license2.lic\"] Two Fortinet FortiGate firewall licenses, if you don't have any license files, update the config file with an empty array (\"license\": []). Do NOT use the following: [\"\"] Place files in a folder (eg. firewall) in the same S3 bucket in your Organization Management account as the deployment configuration file. mandatory-account-configs/perimeter/deployments/firewalls/config : \"firewall/firewall-example.txt\" The Fortinet configuration file to initially configure the firewalls. Sample configuration files can be found in the reference-artifacts/Third-Party folder Place file in a folder (eg. firewall) in the same S3 bucket in your Organization Management account as the deployment configuration file. If you are using GWLB config: mandatory-account-configs/perimeter/deployments/firewalls/Checkpoint-Firewall - image-id : \"ami-0217611bf09d5b4c1\" Update AMI with the AMI collected from the Marketplace for CloudGuard Network Security for GWLB - BYOL mandatory-account-configs/perimeter/deployments/firewall-manager/image-id : \"ami-0071a3b4ef9ac766a\" Update AMI with the AMI collected from the Marketplace for Checkpoint Security Management mandatory-account-configs/perimeter/deployments/firewall-manager/version : \"R8110BYOLMGMT\" Update version based on the selected ami-id version from the Private Marketplace For management , review and update the following: mandatory-account-configs/management/account-name : \"ASEA-Main\" Update this field with your Organization Management (root) account name, if it is not set to ASEA-Main. mandatory-account-configs/management/iam/users the names of your break-glass and ASEA operation users","title":"Mandatory Account Configs"},{"location":"installation/customization-index/#workload-account-configs","text":"As mentioned in the Installation Guide, we recommend not adding more than 1 or 2 workload accounts to the config file during the initial deployment as it will increase risks of hitting a limit. Once the Accelerator is successfully deployed, add the additional accounts back into the config file and rerun the state machine. Review the workload accounts in the config that you selected and change the name and email as desired Modify mydevacct1 with the account name of your choosing Modify mydevacct1/account-name : \"MyDev1\" with the account name Modify mydevacct1/email : \"myemail+aseaT-dev1@example.com---------------------REPLACE------------\" with a unique email address for the account Modify mydevacct1/description : \"This is an OPTIONAL SAMPLE workload account...\" with a description relevant to your account Modify mydevacct1/ou : \"Dev\" with the OU that you would like the account to be attached to","title":"Workload Account Configs"},{"location":"installation/customization-index/#organization-units","text":"For all organization units, update the budget alerts email addresses: organizational-units/core/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Central/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Dev/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Test/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Prod/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" organizational-units/Sandbox/default-budgets/alerts/emails : \"myemail+aseaT-budg@example.com\" For organization units with certificates , review the certificates and update as you see fit. These certificates are used in the alb section under alb/cert-name of each OU","title":"Organization Units"},{"location":"installation/design/","text":"Design Notes \u00b6 Accelerator Design Constraints / Decisions \u00b6 The Organization Management (root) account does NOT have any preventative controls to protect the integrity of the Accelerator codebase, deployed objects or guardrails. Do not delete, modify, or change anything in the Organization Management (root) account unless you are certain as to what you are doing. More specifically, do NOT delete, or change any buckets in the Organization Management (root) account. While generally protected, do not delete/update/change s3 buckets with cdk-asea-, or asea- in any sub-accounts. ALB automated deployments only supports Forward and not redirect rules. The Accelerator deploys SNS topics to send email alerts and notifications. Given email is not a secure transport mechanism, we have chosen not to enable SNS encryption on these topics at this time. AWS generally discourages cross-account KMS key usage. As the Accelerator centralizes logs across an entire organization as a security best practice, this is an exception/example of a unique situation where cross-account KMS key access is required. The Accelerator aggregates all logs in the log-archive account using Kinesis Data and Kinesis Firehose as aggregation tools where the logs could persist for up to 24 hours. These logs are encrypted with Customer Managed KMS keys once stored in S3 (ELB logs only support AES256). These logs are also encrypted in transit using TLS encryption. At this time, we have not enabled Kinesis at-rest encryption, we will reconsider this decision based on customer feedback. AWS Config Aggregator is deployed in the Organization Management (root) account as enablement through Organizations is simpler to implement. AWS Organizations only supported deploying the Aggregator in the Organization Management (root) account and not in a designated administrative account when we implemented this feature. We have a backlog item to update the code to move the Aggregator to the security account. An Organization CloudTrail is deployed, which is created in the primary region in the Organization Management (root) AWS account. All AWS account CloudTrails are centralized into this single CloudWatch Log Group. Starting in v1.1.9 this is where we deploy the CloudWatch Alarms which trigger for ALL accounts in the organization. Security Hub will erroneously report that the only account and/or region that is compliant with certain rules is the primary region of the Organization Management (root) account. We are working with the Security Hub team to rectify this situation in future Security Hub/Accelerator releases (resolved in Accelerator v1.5.0). Only 1 auto-deployed MAD in any mandatory-account is supported today. VPC Endpoints have no Name tags applied as CloudFormation does not currently support tagging VPC Endpoints. If the Organization Management (root) account coincidentally already has an ADC with the same domain name, we do not create/deploy a new ADC. You must manually create a new ADC (it won't cause issues). 3rd party firewall updates are to be performed using the firewall OS based update capabilities. To update the AMI using the Accelerator, you must first remove the firewalls and then redeploy them (as the EIP's will block a parallel deployment), or deploy a second parallel FW cluster and de-provision the first cluster when ready. When adding more than 100 accounts to an OU which uses shared VPC's, you must first increase the Quota Participant accounts per VPC in the shared VPC owner account (i.e. shared-network). Trapping this quota before the SM fails has been added to the backlog. The default limit for Directory Sharing is 125 accounts for an Enterprise Managed Active Directory (MAD), a quota increase needs to be manually requested through support from the account containing the MAD before this limit is reached. Standard MAD has a sharing limit of 5 accounts (and only supports a small quota increase). The MAD sharing limit is not available in the Service Quota's tools.","title":"Design Decisions"},{"location":"installation/design/#design-notes","text":"","title":"Design Notes"},{"location":"installation/design/#accelerator-design-constraints-decisions","text":"The Organization Management (root) account does NOT have any preventative controls to protect the integrity of the Accelerator codebase, deployed objects or guardrails. Do not delete, modify, or change anything in the Organization Management (root) account unless you are certain as to what you are doing. More specifically, do NOT delete, or change any buckets in the Organization Management (root) account. While generally protected, do not delete/update/change s3 buckets with cdk-asea-, or asea- in any sub-accounts. ALB automated deployments only supports Forward and not redirect rules. The Accelerator deploys SNS topics to send email alerts and notifications. Given email is not a secure transport mechanism, we have chosen not to enable SNS encryption on these topics at this time. AWS generally discourages cross-account KMS key usage. As the Accelerator centralizes logs across an entire organization as a security best practice, this is an exception/example of a unique situation where cross-account KMS key access is required. The Accelerator aggregates all logs in the log-archive account using Kinesis Data and Kinesis Firehose as aggregation tools where the logs could persist for up to 24 hours. These logs are encrypted with Customer Managed KMS keys once stored in S3 (ELB logs only support AES256). These logs are also encrypted in transit using TLS encryption. At this time, we have not enabled Kinesis at-rest encryption, we will reconsider this decision based on customer feedback. AWS Config Aggregator is deployed in the Organization Management (root) account as enablement through Organizations is simpler to implement. AWS Organizations only supported deploying the Aggregator in the Organization Management (root) account and not in a designated administrative account when we implemented this feature. We have a backlog item to update the code to move the Aggregator to the security account. An Organization CloudTrail is deployed, which is created in the primary region in the Organization Management (root) AWS account. All AWS account CloudTrails are centralized into this single CloudWatch Log Group. Starting in v1.1.9 this is where we deploy the CloudWatch Alarms which trigger for ALL accounts in the organization. Security Hub will erroneously report that the only account and/or region that is compliant with certain rules is the primary region of the Organization Management (root) account. We are working with the Security Hub team to rectify this situation in future Security Hub/Accelerator releases (resolved in Accelerator v1.5.0). Only 1 auto-deployed MAD in any mandatory-account is supported today. VPC Endpoints have no Name tags applied as CloudFormation does not currently support tagging VPC Endpoints. If the Organization Management (root) account coincidentally already has an ADC with the same domain name, we do not create/deploy a new ADC. You must manually create a new ADC (it won't cause issues). 3rd party firewall updates are to be performed using the firewall OS based update capabilities. To update the AMI using the Accelerator, you must first remove the firewalls and then redeploy them (as the EIP's will block a parallel deployment), or deploy a second parallel FW cluster and de-provision the first cluster when ready. When adding more than 100 accounts to an OU which uses shared VPC's, you must first increase the Quota Participant accounts per VPC in the shared VPC owner account (i.e. shared-network). Trapping this quota before the SM fails has been added to the backlog. The default limit for Directory Sharing is 125 accounts for an Enterprise Managed Active Directory (MAD), a quota increase needs to be manually requested through support from the account containing the MAD before this limit is reached. Standard MAD has a sharing limit of 5 accounts (and only supports a small quota increase). The MAD sharing limit is not available in the Service Quota's tools.","title":"Accelerator Design Constraints / Decisions"},{"location":"installation/existing-orgs/","text":"Existing Organizations / Accounts \u00b6 Considerations: Importing existing AWS Accounts / Deploying Into Existing AWS Organizations \u00b6 The Accelerator can be installed into existing AWS Organizations our early adopters have all successfully deployed into existing organizations Existing AWS accounts can also be imported into an Accelerator managed Organization Caveats: Per AWS Best Practices, the Accelerator deletes the default VPC's in all AWS accounts, worldwide. The inability to delete default VPC's in pre-existing accounts will fail the installation/account import process. Ensure default VPC's can or are deleted before importing existing accounts. On failure, either rectify the situation, or remove the account from Accelerator management and rerun the state machine The Accelerator will NOT alter existing (legacy) constructs (e.g. VPC's, EBS volumes, etc.). For imported and pre-existing accounts, objects the Accelerator prevents from being created using preventative guardrails will continue to exist and not conform to the prescriptive security guidance Existing workloads should be migrated to Accelerator managed VPC's and legacy VPC's deleted to gain the full governance benefits of the Accelerator (centralized flow logging, centralized ingress/egress, no IGW's, Session Manager access, existing non-encrypted EBS volumes, etc.) Existing AWS services will be reconfigured as defined in the Accelerator configuration file (overwriting existing settings) We do NOT support any workloads running or users operating in the Organization Management (root) AWS account. The Organization Management (root) AWS account MUST be tightly controlled Importing existing workload accounts is fully supported, we do NOT support, recommend and strongly discourage importing mandatory accounts, unless they were clean/empty accounts. Mandatory accounts are critical to ensuring governance across the entire solution We've tried to ensure all customer deployments are smooth. Given the breadth and depth of the AWS service offerings and the flexibility in the available deployment options, there may be scenarios that cause deployments into existing Organizations to initially fail. In these situations, simply rectify the conflict and re-run the state machine. If the Firewall Manager administrative account is already set for your organization, it needs to be unset before starting a deployment. Process to import existing AWS accounts into an Accelerator managed Organization \u00b6 Newly invited AWS accounts in an Organization will land in the root ou Unlike newly created AWS accounts which immediately have a Deny-All SCP applied, imported accounts are not locked down as we do not want to break existing workloads (these account are already running without Accelerator guardrails) In AWS Organizations, select ALL the newly invited AWS accounts and move them all (preferably at once) to the correct destination OU (assuming the same OU for all accounts) In case you need to move accounts to multiple OU's we have added a 2 minute delay before triggering the State Machine Any accounts moved after the 2 minute window will NOT be properly ingested, and will need to be ingested on a subsequent State Machine Execution This will first trigger an automated update to the config file and then trigger the state machine after a 2 minute delay, automatically importing the moved accounts into the Accelerator per the destination OU configuration As previously documented, accounts CANNOT be moved between OU's to maintain compliance, so select the proper top-level OU with care If you need to customize each of the accounts configurations, you can manually update the configuration file either before or after you move the account to the correct ou if before, you also need to include the standard 4 account config file parameters, if after, you can simply add your new custom parameters to the account entry the Accelerator creates if you add your imported accounts to the config file, moving the first account to the correct ou will trigger the state machine after a 2 minutes delay. If you don't move all accounts to their correct ou's within 2 minutes, your state machine will fail. Simply finish moving all accounts to their correct OU's and then rerun the state machine. If additional accounts are moved into OUs while the state machine is executing, they will not trigger another state machine execution, those accounts will only be ingested on the next execution of the state machine customers can either manually initiate the state machine once the current execution completes, or, the currently running state machine can be stopped and restarted to capture all changes at once Are you unsure if an account had its guardrails applied? The message sent to the state machine Status SNS topic (and corresponding email address) on a successful state machine execution provides a list of all successfully processed accounts. The state machine is both highly parallel and highly resilient, stopping the state machine should not have any negative impact. Importing 1 or 10 accounts generally takes about the same amount of time for the Accelerator to process, so it may be worth stopping the current execution and rerunning to capture all changes in a single execution. We have added a 2 min delay before triggering the state machine, allowing customers to make multiple changes within a short timeframe and have them all captured automatically in the same state machine execution. Deploying the Accelerator into an existing Organization \u00b6 As stated above, if the ALZ was previously deployed into the Organization, please work with your AWS account team to find the best mechanism to uninstall the ALZ solution Ensure all existing sub-accounts have the role name defined in organization-admin-role installed and set to trust the Organization Management (root) AWS Organization account prior to v1.2.5, this role must be named: AWSCloudFormationStackSetExecutionRole if using the default role ( AWSCloudFormationStackSetExecutionRole ) we have provided a CloudFormation stack which can be executed in each sub-account to simplify this process As stated above, we recommend starting with new AWS accounts for the mandatory functions (shared-network, perimeter, security, log-archive accounts). To better ensure a clean initial deployment, we also recommend the installation be completed while ignoring most of your existing AWS sub-accounts, importing them post installation: create a new OU (i.e. Imported-Accounts ), placing most of the existing accounts into this OU temporarily, and adding this OU name to the global-options\\ignored-ous config parameter; any remaining accounts must be in the correct ou, per the Accelerator config file; install the Accelerator; import the skipped accounts into the Accelerator using the above import process, paying attention to the below notes NOTES: Do NOT move any accounts from any ignored-ous to the root ou, they will immediately be quarantined with a Deny-All SCP, they need to be moved directly to their destination ou As stated above, when importing accounts, there may be situations we are not able to fully handle If doing a mass import, we suggest you take a quick look and if the solution is not immediately obvious, move the account which caused the failure back to ignored-ous and continue importing the remainder of your accounts. Once you have the majority imported, you can circle back and import outstanding problem accounts with the ability to focus on each individual issue The challenge could be as simple as someone has instances running in a default VPC, which may require some cleanup effort before we can import (coming soon, you will be able to exclude single account/region combinations from default VPC deletion to gain the benefits of the rest of the guardrails while you migrate workloads out of the default VPC)","title":"Existing Orgs & Accounts"},{"location":"installation/existing-orgs/#existing-organizations-accounts","text":"","title":"Existing Organizations / Accounts"},{"location":"installation/existing-orgs/#considerations-importing-existing-aws-accounts-deploying-into-existing-aws-organizations","text":"The Accelerator can be installed into existing AWS Organizations our early adopters have all successfully deployed into existing organizations Existing AWS accounts can also be imported into an Accelerator managed Organization Caveats: Per AWS Best Practices, the Accelerator deletes the default VPC's in all AWS accounts, worldwide. The inability to delete default VPC's in pre-existing accounts will fail the installation/account import process. Ensure default VPC's can or are deleted before importing existing accounts. On failure, either rectify the situation, or remove the account from Accelerator management and rerun the state machine The Accelerator will NOT alter existing (legacy) constructs (e.g. VPC's, EBS volumes, etc.). For imported and pre-existing accounts, objects the Accelerator prevents from being created using preventative guardrails will continue to exist and not conform to the prescriptive security guidance Existing workloads should be migrated to Accelerator managed VPC's and legacy VPC's deleted to gain the full governance benefits of the Accelerator (centralized flow logging, centralized ingress/egress, no IGW's, Session Manager access, existing non-encrypted EBS volumes, etc.) Existing AWS services will be reconfigured as defined in the Accelerator configuration file (overwriting existing settings) We do NOT support any workloads running or users operating in the Organization Management (root) AWS account. The Organization Management (root) AWS account MUST be tightly controlled Importing existing workload accounts is fully supported, we do NOT support, recommend and strongly discourage importing mandatory accounts, unless they were clean/empty accounts. Mandatory accounts are critical to ensuring governance across the entire solution We've tried to ensure all customer deployments are smooth. Given the breadth and depth of the AWS service offerings and the flexibility in the available deployment options, there may be scenarios that cause deployments into existing Organizations to initially fail. In these situations, simply rectify the conflict and re-run the state machine. If the Firewall Manager administrative account is already set for your organization, it needs to be unset before starting a deployment.","title":"Considerations: Importing existing AWS Accounts / Deploying Into Existing AWS Organizations"},{"location":"installation/existing-orgs/#process-to-import-existing-aws-accounts-into-an-accelerator-managed-organization","text":"Newly invited AWS accounts in an Organization will land in the root ou Unlike newly created AWS accounts which immediately have a Deny-All SCP applied, imported accounts are not locked down as we do not want to break existing workloads (these account are already running without Accelerator guardrails) In AWS Organizations, select ALL the newly invited AWS accounts and move them all (preferably at once) to the correct destination OU (assuming the same OU for all accounts) In case you need to move accounts to multiple OU's we have added a 2 minute delay before triggering the State Machine Any accounts moved after the 2 minute window will NOT be properly ingested, and will need to be ingested on a subsequent State Machine Execution This will first trigger an automated update to the config file and then trigger the state machine after a 2 minute delay, automatically importing the moved accounts into the Accelerator per the destination OU configuration As previously documented, accounts CANNOT be moved between OU's to maintain compliance, so select the proper top-level OU with care If you need to customize each of the accounts configurations, you can manually update the configuration file either before or after you move the account to the correct ou if before, you also need to include the standard 4 account config file parameters, if after, you can simply add your new custom parameters to the account entry the Accelerator creates if you add your imported accounts to the config file, moving the first account to the correct ou will trigger the state machine after a 2 minutes delay. If you don't move all accounts to their correct ou's within 2 minutes, your state machine will fail. Simply finish moving all accounts to their correct OU's and then rerun the state machine. If additional accounts are moved into OUs while the state machine is executing, they will not trigger another state machine execution, those accounts will only be ingested on the next execution of the state machine customers can either manually initiate the state machine once the current execution completes, or, the currently running state machine can be stopped and restarted to capture all changes at once Are you unsure if an account had its guardrails applied? The message sent to the state machine Status SNS topic (and corresponding email address) on a successful state machine execution provides a list of all successfully processed accounts. The state machine is both highly parallel and highly resilient, stopping the state machine should not have any negative impact. Importing 1 or 10 accounts generally takes about the same amount of time for the Accelerator to process, so it may be worth stopping the current execution and rerunning to capture all changes in a single execution. We have added a 2 min delay before triggering the state machine, allowing customers to make multiple changes within a short timeframe and have them all captured automatically in the same state machine execution.","title":"Process to import existing AWS accounts into an Accelerator managed Organization"},{"location":"installation/existing-orgs/#deploying-the-accelerator-into-an-existing-organization","text":"As stated above, if the ALZ was previously deployed into the Organization, please work with your AWS account team to find the best mechanism to uninstall the ALZ solution Ensure all existing sub-accounts have the role name defined in organization-admin-role installed and set to trust the Organization Management (root) AWS Organization account prior to v1.2.5, this role must be named: AWSCloudFormationStackSetExecutionRole if using the default role ( AWSCloudFormationStackSetExecutionRole ) we have provided a CloudFormation stack which can be executed in each sub-account to simplify this process As stated above, we recommend starting with new AWS accounts for the mandatory functions (shared-network, perimeter, security, log-archive accounts). To better ensure a clean initial deployment, we also recommend the installation be completed while ignoring most of your existing AWS sub-accounts, importing them post installation: create a new OU (i.e. Imported-Accounts ), placing most of the existing accounts into this OU temporarily, and adding this OU name to the global-options\\ignored-ous config parameter; any remaining accounts must be in the correct ou, per the Accelerator config file; install the Accelerator; import the skipped accounts into the Accelerator using the above import process, paying attention to the below notes NOTES: Do NOT move any accounts from any ignored-ous to the root ou, they will immediately be quarantined with a Deny-All SCP, they need to be moved directly to their destination ou As stated above, when importing accounts, there may be situations we are not able to fully handle If doing a mass import, we suggest you take a quick look and if the solution is not immediately obvious, move the account which caused the failure back to ignored-ous and continue importing the remainder of your accounts. Once you have the majority imported, you can circle back and import outstanding problem accounts with the ability to focus on each individual issue The challenge could be as simple as someone has instances running in a default VPC, which may require some cleanup effort before we can import (coming soon, you will be able to exclude single account/region combinations from default VPC deletion to gain the benefits of the rest of the guardrails while you migrate workloads out of the default VPC)","title":"Deploying the Accelerator into an existing Organization"},{"location":"installation/install/","text":"Accelerator Installation and Upgrade Guide \u00b6 We encourage customers installing the Accelerator to get the support of their local AWS account team (SA, TAM, CSM, Proserve) to assist with the installation of the Accelerator, as the Accelerator leverages, deploys, or orchestrates over 30 different AWS services. Users are strongly encouraged to also read the Accelerator Operations/Troubleshooting Guide before installation. The Operations/Troubleshooting Guide provides details as to what is being performed at each stage of the installation process, including detailed troubleshooting guidance. These installation instructions assume one of the prescribed architectures is being deployed. Installation \u00b6 Prerequisites \u00b6 General \u00b6 Management or root AWS Organization account (the AWS Accelerator cannot be deployed in an AWS sub-account) No additional AWS accounts need to be pre-created before Accelerator installation If required, a limit increase to support your desired number of new AWS sub-accounts (default limit is 10 sub-accounts) Valid Accelerator configuration file, updated to reflect your requirements (see below) Determine your primary or Accelerator control or home region, this is the AWS region in which you will most often operate Government of Canada customers are still required to do a standalone installation at this time, please request standalone installation instructions from your Account SA or TAM The Accelerator can be installed into existing AWS Organizations - see caveats and notes in section 4 below Existing AWS Landing Zone Solution (ALZ) customers are required to remove their ALZ deployment before deploying the Accelerator. Scripts are available to assist with this process. Changes to the Accelerator codebase are strongly discouraged unless they are contributed and accepted back to the solution. Code customization will block the ability to upgrade to the latest release and upgrades are encouraged to be done between quarterly to semi-annually. The solution was designed to be extremely customizable without changing code, existing customers following these guidelines have been able to upgrade across more than 50 Accelerator releases, while maintaining their customizations and gaining the latest bug fixes, features and enhancements without any developer or professional services based support. Please see this FAQ for more details. Production Deployment Planning \u00b6 General \u00b6 For any deployment of the Accelerator which is intended to be used for production workloads, you must evaluate all these decisions carefully. Failure to understand these choices could cause challenges down the road. If this is a \"test\" or \"internal\" deployment of the Accelerator which will not be used for production workloads, you can leave the default config values. Config file schema ) documentation (Draft) OU Structure Planning \u00b6 Plan your OU and core account structure carefully. By default, we suggest: Security, Infrastructure, Central, Sandbox, Dev, Test, Prod . The Security OU will contain the Security account, the Log Archive account, and the Organization Management account. The Infrastructure OU will hold the remainder of the accounts shared or utilized by the rest of the organization ( Shared Network , Perimeter , and Operations ). The remainder of the OUs correspond with major permission shifts in the SDLC cycle and NOT every stage an organization has in their SDLC cycle (i.e. QA or pre-prod would be included in one of the other OUs). The Central OU is used to hold accounts with workloads shared across Dev, Test, and Prod environments like centralized CI/CD tooling. The v1.5.0 release aligns the Accelerator OU and account structure with AWS multi-account guidance, splitting the core OU into the Security and Infrastructure OUs. Note: While OUs can be renamed or additional OUs added at a later point in time, deployed AWS accounts CANNOT be moved between top-level OUs (guardrail violation), nor can top-level OUs easily be deleted (requires deleting all AWS accounts from within the OU first). Network Configuration Planning \u00b6 If deploying the prescriptive architecture using the Full or Lite sample config files, you will need the following network constructs: Six (6) RFC1918 Class B address blocks (CIDR's) which do not conflict with your on-premise networks (a single /13 block works well) VPC CIDR blocks cannot be changed after installation, this is simply the way the AWS platform works, given everything is built on top of them. Carefully consider your address block selection. one block for each OU, except Sandbox which is not routable (Sandbox OU will use a 7th non-routed address block) the \"core\" Class B range will be split to support the Endpoint VPC and Perimeter VPC (with extra addresses remaining for future use) Given a shared VPC architecture is leveraged (prevents stranded islands of CIDR blocks and reduces networking costs), we have assigned a class B address block to each VPC to future proof the deployment. Smaller customers can successfully deploy with a half class B CIDR block per shared VPC. Two (2) RFC6598 /23 address blocks (Government of Canada (GC) requirement only) Used for AWS Managed Active Directory (MAD) deployment and perimeter underlay network non-GC customers can replace the RFC6598 address space with the extra unused addresses from the above RFC1918 CIDR range above (the App2 subnets in the Central VPC and the Perimeter VPC address space) BGP ASN's for network routing, one for each of: Transit Gateway (one unique ASN per TGW, multi-region example requires a second ASN) IPSec VPN Firewall Cluster (if deployed) VGW for DirectConnect connectivity (only shown in the config.multi-region-example.json) For example: the Control Tower with Network Firewall example config requires a single BGP ASN for the TGW, the IPSec VPN example requires two BGP ASN's, and the multi-region example requires five unique BGP ASN's. NOTE: Prior to v1.5.0 CIDR ranges were assigned to each VPC and subnet throughout the config file. This required customers to perform extensive updates across the config file when needing to move to specific IP ranges compatible with a customer's existing on-premise networks. While this is still supported for those wanting to control exactly what address is used on every subnet, the solution has added support for dynamic CIDR assignments and the sample config files have been updated to reflect. New installs will have CIDR's pulled from CIDR pools, defined in the global-options section of the config file with state maintained in DynamoDB. The v1.5.0 custom upgrade guide will provides details on the upgrade process and requirements to migrate to the new CIDR assignment system, if desired. A script was created to assist with this migration. DNS, Domain Name, TLS Certificate Planning \u00b6 If deploying the prescriptive architecture, you must decide on: A unique Windows domain name ( organizationaws / organization.aws , organizationcloud / organization.cloud , etc.). Given this is designed as the primary identity store and used to domain join all cloud hosted workloads, changing this in future is difficult. Pick a Windows domain name that does NOT conflict with your on-premise AD domains, ensuring the naming convention conforms to your organizations domain naming standards to ensure you can eventually create a domain trust between the MAD and on-premise domains/forests DNS Domain names and DNS server IP's for on-premise private DNS zones requiring cloud resolution (can be added in future) DNS Domain for a cloud hosted public zone \"public\": [\"organization.cloud-nuage.canada.ca\"] (can be added in future) DNS Domain for a cloud hosted private zone \"private\": [\"organization.cloud-nuage.gc.ca\"] (can be added in future) Wildcard TLS certificate for each of the 2 previous zones (can be added/changed in future) Email Address Planning \u00b6 While you require a minimum of 6 unique email addresses (1 per sub-account being created), we recommend at least 20 unique email ALIASES associated with a single mailbox, never used before to open AWS accounts, such that you do not need to request new email aliases every time you need to create a new AWS account and they can all be monitored via a single mailbox. These email addresses can never have been used to previously open an AWS account. You additionally require email addresses for the following additional purposes (these can be existing monitored mailboxes and do not need to be unique): Accelerator execution (state machine) notification events (1 address) High, Medium and Low security alerts (3 addresses if you wish to segregate alerts) Budget notifications Centralized Ingress/Egress Firewalls \u00b6 As of v1.5.0 the Accelerator offers multiple automated firewall deployment options: a) AWS Network Firewall (native AWS Cloud service) Defined in the config file as part of a VPC b) 3rd party firewalls interconnected to the cloud tenancy via IPSec VPN (Active/Active using BGP + ECMP) Defined in the config file under deployments w/TGW VPN attachments this was the only automated option prior to v1.5.0 a sample Fortinet Fortigate configuration is provided (both PAYGO and BYOL supported) For Fortinet BYOL, requires minimum 2 valid license files (evaluation licenses adequate) (can be added in future) c) 3rd party firewalls interconnected to the cloud tenancy via Gateway Load Balancer (GWLB) in an auto-scaling group Defined in the config file under both deployments and load balancers a sample Checkpoint CloudGuard configuration is provided (both PAYGO and BYOL supported) d) Customer gateway (CGW) creation, to enable connectivity to on-premises firewalls or manually deployed cloud firewalls Defined in the config file under deployments w/TGW VPN attachments (but without an AMI or VPC association) Examples of each of the firewall options have been included as variants of the Lite config file example . Note: While we only provide a single example for each 3rd party implementation today, the implementations are generic and should be usable by any 3rd party firewall vendor, assuming they support the required features and protocols. The two examples were driven by customer demand and heavy lifting by the 3rd party vendor. We look forward to additional vendors developing and contributing additional sample configurations. For new 3rd party integrations, we encourage the use of the GWLB approach. Other \u00b6 We recommend installing with the default Accelerator Name ( ASEA ) and Accelerator Prefix ( ASEA- ), but allow customization. Prior to v1.5.0 the defaults were ( PBMM ) and ( PBMMAccel- ) respectively. the Accelerator name and prefix CANNOT be changed after the initial installation; the Accelerator prefix including the mandatory dash cannot be longer than 10 characters. New installations, which now leverage Control Tower, require the organization-admin-role be set to AWSControlTowerExecution . Existing standalone installations will continue to utilize their existing role name for the organization-admin-role , typically OrganizationAccountAccessRole , as this role is used by AWS Organizations by default when no role name is specified while creating AWS accounts through the AWS console. the Accelerator leverages this role name to create all new accounts in the organization; this role name, as defined in the config file, MUST be utilized when manually creating all new sub-accounts in the Organization; existing installs wishing to change the role name are required to first deploy a new role with a trust to the root account, in all accounts in the organization. Accelerator Pre-Install Steps \u00b6 General \u00b6 Before installing, you must first: Login to the Organization Management (root) AWS account with AdministratorAccess . Set the region to your desired home region (i.e. ca-central-1 ) Install AWS Control Tower: Government of Canada customers are required to skip this step OU and account names can ONLY be customized during initial installation. These values MUST match with the values supplied in the Accelerator config file. Go to the AWS Control Tower console and click Set up landing zone Select your home region (i.e. ca-central-1 ) the Accelerator home region must match the Control Tower home region Select all regions for Additional AWS Regions for governance , click Next The Control Tower and Accelerator regions MUST be properly aligned If a region is not governed by Control Tower, it must NOT be listed in control-tower-supported-regions To manage a region requires the region: be enabled in Control Tower (if supported) added to the config file control-tower-supported-regions list (if supported) added to the config file supported-regions list (even if not supported by Control Tower, as the Accelerator can manage regions not yet supported by Control Tower, but only when NOT listed in control-tower-supported-regions ) While we highly recommend guardrail deployment for all AWS enabled by default regions, at minimum the home region MUST be enabled in Control Tower and must be listed in control-tower-supported-regions both the home-region and ${GBL_REGION} must be listed in supported-regions For the Foundational OU , leave the default value Security For the Additional OU provide the value Infrastructure , click Next Enter the email addresses for your Log Archive and Audit accounts, change the Audit account name to Security , click Next OU and account names can ONLY be customized during initial installation. OU names, account names and email addresses must match identically with the values supplied in the Accelerator config file. Click setup and wait ~60 minutes for the Control Tower installation to complete Select Add or register organizational units , Click Add an OU Type Dev , click Add , wait until the OU is finished provisioning (or it will error) Repeat step 9 for each OU (i.e. Test , Prod , Central , Sandbox ) Select Account factory , Edit, Subnets: 0, Deselect all regions, click Save In AWS Organizations, move the Management account from the root OU into the Security OU Verify: AWS Organizations is enabled in All features mode if required, navigate to AWS Organizations, click Create Organization , Create Organization Service Control Policies are enabled if required, in Organizations, select Policies , Service control policies , Enable service control policies Verify the Organization Management (root) account email address In AWS Organizations, Settings, \"Send Verification Request\" Once it arrives, complete the validation by clicking the validation link in the email Create a new KMS key to encrypt your source configuration bucket (you can use an existing key) AWS Key Management Service, Customer Managed Keys, Create Key, Symmetric, and then provide a key name ( ASEA-Source-Bucket-Key ), Next Select a key administrator (Admin Role or Group for the Organization Management account), Next Select key users (Admin Role or Group for the Organization Management account), Next Validate an entry exists to \"Enable IAM User Permissions\" (critical step if using an existing key) \"arn:aws:iam::123456789012:root\" , where 123456789012 is your Organization Management account id. Click Finish Select the new key, Select Key Rotation , Automatically rotate this CMK every year , click Save. Enable \"Cost Explorer\" (My Account, Cost Explorer, Enable Cost Explorer) With recent platform changes, Cost Explorer may now be auto-enabled (unable to confirm) Enable \"Receive Billing Alerts\" (My Account, Billing Preferences, Receive Billing Alerts) It is extremely important that all the account contact details be validated in the Organization Management (root) account before deploying any new sub-accounts. This information is copied to every new sub-account on creation. Subsequent changes to this information require manually updating it in each sub-account. Go to My Account and verify/update the information lists under both the Contact Information section and the Alternate Contacts section. Please ESPECIALLY make sure the email addresses and Phone numbers are valid and regularly monitored. If we need to reach you due to suspicious account activity, billing issues, or other urgent problems with your account - this is the information that is used. It is CRITICAL it is kept accurate and up to date at all times. Create GitHub Personal Access Token and Store in Secrets Manager \u00b6 As of v1.5.0, the Accelerator offers deployment from either GitHub or CodeCommit: GitHub (recommended) You require a GitHub access token to access the code repository Instructions on how to create a personal access token are located here . Select the scope public_repo underneath the section repo: Full control over private repositories . Store the personal access token in Secrets Manager as plain text. Name the secret accelerator/github-token (case sensitive). Via AWS console Store a new secret, and select Other type of secrets , Plaintext Paste your secret with no formatting no leading or trailing spaces (i.e. completely remove the example text) Select the key you created above ( ASEA-Source-Bucket-Key ), Set the secret name to accelerator/github-token (case sensitive) Select Disable rotation CodeCommit (alternative option) Multiple options exist for downloading the GitHub Accelerator codebase and pushing it into CodeCommit. As this option is only for advanced users, detailed instructions are not provided. In your AWS Organization Management account, open CodeCommit and create a new repository named aws-secure-environment-accelerator Go to GitHub and download the repository Source code zip or tarball for the release you wish to deploy Do NOT download the code off the main GitHub branch, this will leave you in a completely unsupported state (and with beta code) Push the extracted codebase into the newly created CodeCommit repository, maintaining the file/folder hierarchy Set the default CodeCommit branch for the new repository to main Create a branch following the Accelerator naming format for your release (i.e. release/v1.5.0 ) AWS Internal (Employee) Accounts Only \u00b6 If deploying to an internal AWS employee account and installing the solution with a 3rd party firewall, you need to enable Private Marketplace (PMP) before starting: In the Organization Management account go here: https://aws.amazon.com/marketplace/privatemarketplace/create Click Create a Private Marketplace , and wait for activation to complete Go to the \"Account Groups\" sub-menu, click Create account group Enter an Account Group Title (i.e. Default ) and Add the Management (root) account number in Associate AWS account Associate the default experience New Private Marketplace , then click Create account group and wait for it to create Go to \"Experiences\" sub-menu, select New Private Marketplace Select the \"Settings\" sub-tab, and click the Not Live slider to make it Live and wait for it to complete Ensure the \"Software requests\" slider is set to Requests off and wait for it to complete Change the name field (i.e. append -PMP ) and change the color, so it is clear PMP is enabled for users, click Update Go to the \"Products\" sub-tab, then select the All AWS Marketplace products nested sub-tab Search Private Marketplace for the Fortinet or Checkpoint products and select Fortinet FortiGate (BYOL) Next-Generation Firewall and Fortinet FortiManager (BYOL) Centralized Security Management or CloudGuard Network Security for Gateway Load Balancer - BYOL and Check Point Security Management (BYOL) Select \"Add\" in the top right Due to PMP provisioning delays, this sometimes fails when attempted immediately following enablement of PMP or if adding each product individually - retry after 20 minutes. While not used in this account, you must now subscribe to the two subscriptions and accept the EULA for each product (you will need to do the same in the perimeter account, once provisioned below) To subscribe, select the \"Approved products\" tab Click on the product you want to subscribe, in this case Fortinet FortiGate (BYOL) Next-Generation Firewall and Fortinet FortiManager (BYOL Centralized Security Management or CloudGuard Network Security for Gateway Load Balancer - BYOL and Check Point Security Management (BYOL) Click on \"Continue to Subscribe\" Click on \"Accept Terms\" and wait for subscription to be completed If you are deploying in any region except ca-central-1 or wish to switch to a different license type, you need the new AMI id's. After successfully subscribing, continue one more step and click the \u201cContinue to Configuration\u201d. When you get the below screen, select your region and version ( Fortinet v6.4.7 , Checkpoint Mgmt R81.10-335.883 and CloudGuard R80.40-294.374 recommended at this time). Marketplace will provide the required AMI id. Document the two AMI id's, as you will need to update them in your config.json file below. Basic Accelerator Configuration \u00b6 Select a sample config file as a baseline starting point IMPORTANT: Use a config file from the Github code branch you are deploying from, as valid parameters change over time. The main branch is NOT the current release and often will not work with the GA releases. sample config files can be found in this folder; descriptions of the sample config files and customization guidance can be found here ; unsure where to start, use the config.lite-CTNFW-example.json , where CTNFW is for Control Tower w/NFW; These configuration files can be used, as-is, with only minor modification to successfully deploy the sample architectures; On upgrades, compare your deployed configuration file with the latest branch configuration file for any new or changed parameters; At minimum, you MUST update the AWS account names and email addresses in the sample file: For existing accounts, they must match identically to both the account names and email addresses defined in AWS Organizations; For new accounts, they must reflect the new account name/email you want created; All new AWS accounts require a unique email address which has never before been used to create an AWS account; When updating the budget or SNS notification email addresses within the sample config, a single email address for all is sufficient; Update the IP address in the \"alarm-not-ip\" variable with your on-premise IP ranges (used for the AWS-SSO-Authentication-From-Unapproved-IP alarm); If deploying the Managed AD, update the dns-domain, netbios-domain, log-group-name, as well as the AD users and groups that will be created; For a test deployment, the remainder of the values can be used as-is; While it is generally supported, we recommend not adding more than 1 or 2 workload accounts to the config file during the initial deployment as it will increase risks of hitting a limit. Once the Accelerator is successfully deployed, add the additional accounts to the config file and rerun the state machine. More information here on the fields in the config file that need to be updated. A successful deployment of the prescriptive architecture requires VPC access to 9 AWS endpoints, you cannot remove both the perimeter firewalls (all public endpoints) and the 9 required central VPC endpoints from the config file (ec2, ec2messages, ssm, ssmmessages, cloudformation, secretsmanager, kms, logs, monitoring). When deploying to regions other than ca-central-1 , you need to modify your config file as follows (for Canada Central 1, the AMI ids are pre-populated for you): Update the firewall and firewall manager AMI id's to reflect your home regions regional AMI id's (see 2.3.3, item 13), making sure you select the right version and region per the recommendations. Validate all the Interface Endpoints defined in your config file are supported in your home region (i.e. Endpoint VPC). Remove unsupported endpoints from the config file, add additional endpoints as available. If you are installing into a home region which is explicitly named in any of the replacements\\addl_regions_x, remove it from the list. If deploying in us-east-1, remove ${GBL_REGION}. Create an S3 bucket in your Organization Management account your-bucket-name you must supply this bucket name in the CFN parameters and in the config file ( global-options\\central-bucket ) the bucket name must be the same in both spots the bucket must have versioning enabled the bucket must be S3-KMS encrypted using the ASEA-Source-Bucket-Key created above Place your customized config file(s), named config.json (or config.yaml ), in your new bucket If required, place the firewall configuration and license files in the folder and path defined in the config file For AWS Network Firewall: nfw/nfw-example-policy.json For Fortinet: firewall/firewall-example.txt , firewall/license1.lic and firewall/license2.lic We have made a sample available here : ./reference-artifacts/Third-Party/ the samples configures an active / active firewall pair with two tunnels per firewall If you updated your perimeter VPC subnet names, you must also make these changes in your firewall-example.txt file If you don't have any license files, update the config file with an empty array ( \"license\": [] ). Do NOT use the following: [\"\"] . The basic Checkpoint configuration is stored directly in config.json Place any defined certificate files in the folder and path defined in the config file i.e. certs/example1-cert.key , certs/example1-cert.crt Sample available here : ./reference-artifacts/Certs-Sample/* Ideally you would generate real certificates using your existing certificate authority Should you wish, instructions are provided to aid in generating your own self-signed certificates (Self signed certificates are NOT secure and simply for demo purposes) Use the examples to demonstrate Accelerator TLS functionality only Detach ALL SCPs (except FullAWSAccess which remains in place) from all OU's and accounts before proceeding For Control Tower based installs do NOT detach Control Tower SCPs (i.e. aws-guardrails-xxxxxx) Installation will fail if this step is skipped Installation \u00b6 You can find the latest release in the repository here . We only support new installations of v1.3.9 or above (older releases continue to function) Download the CloudFormation (CFN) template for the release you plan to install (either AcceleratorInstallerXXX.template.json for GitHub or AcceleratorInstallerXXX-CodeCommit.template.json for CodeCommit) Use the provided CloudFormation template to deploy a new stack in your Management (root) AWS account As previously stated we do not support installation in sub-accounts Login to your Organization Management account and make sure you are in your desired home region (i.e. ca-central-1 ) (your desired primary or control region) Navigate to CloudFormation in the AWS Console and click Create stack with new resources (standard) , then Select \"Template is ready\" For the \"Specify template\" select \"Upload a template file\" Select the *.template.json file you downloaded in step 2 above Click Next Fill out the required parameters - LEAVE THE DEFAULTS UNLESS SPECIFIED BELOW Specify Stack Name STARTING with ASEA- (case sensitive) suggest a suffix of orgname or username Change ConfigS3Bucket to the name of the bucket you created above your-bucket-name Add an Email address to be used for State Machine Status notification The GithubBranch should point to the release you selected if upgrading, change it to point to the desired release the latest stable branch is currently release/v1.5.0 , case sensitive click Next Finish deploying the stack Apply a tag on the stack, Key= Accelerator , Value= ASEA (case sensitive). ENABLE STACK TERMINATION PROTECTION under Stack creation options Click Next , Acknowledge resource creation, and click Create stack The stack typically takes under 5 minutes to deploy. Once deployed, you should see a CodePipeline project named ASEA-InstallerPipeline in your account. This pipeline connects to Github, pulls the code from the prescribed branch and deploys the Accelerator state machine. if the CloudFormation fails to deploy with an Internal Failure , or, if the pipeline fails connecting to GitHub, then: fix the issue with your GitHub secret created in section 2.3.2, then delete the Installer CloudFormation stack you just deployed, and restart at step 3 of this section. For new stack deployments, when the stack deployment completes, the Accelerator state machine will automatically execute (in Code Pipeline). When upgrading you must manually Release Change to start the pipeline. While the pipeline is running: review the list of Known Installation Issues in section 2.5.1 below review the Accelerator Basic Operation and Frequently Asked Questions (FAQ) Document Once the pipeline completes (~10 mins), the main state machine, named ASEA-MainStateMachine_sm , will start in Step Functions The state machine time is dependent on the quantity of resources being deployed. On an initial installation of a more complex sample configuration files, it takes approximately 2 hours to execute (depending on the configuration file). Timing for subsequent executions depends entirely on what resources are changed in the configuration file, but often takes as little as 20 minutes. While you can watch the state machine in Step Functions, you will also be notified via email when the State Machine completes (or fails). Successful state machine executions include a list of all accounts which were successfully processed by the Accelerator. The configuration file will be automatically moved into Code Commit (and deleted from S3). From this point forward, you must update your configuration file in CodeCommit. You will receive an email from the State Machine SNS topic and the 3 SNS alerting topics. Please confirm all four (4) email subscriptions to enable receipt of state machine status and security alert messages. Until completed, you will not receive any email messages (must be completed within 7-days). If the state machine fails : Refer to the Troubleshooting Guide for instructions on how to inspect and retrieve the error You can also refer to the FAQ and Known Installation Issues Once the error is resolved, re-run the step function ASEA-MainStateMachine_sm using {\"scope\": \"FULL\",\"mode\": \"APPLY\"} as input If deploying a prescriptive architecture with 3rd party firewalls, after the perimeter account is created in AWS Organizations, but before the Accelerator reaches Stage 2: NOTE: If you miss the step, or fail to execute it in time, no need to be concerned, you will simply need to re-run the main state machine ( ASEA-MainStateMachine_sm ) to deploy the firewall (no SM input parameters required) Login to the perimeter sub-account (Assume your organization-admin-role ) Activate the 3rd party vendor firewall and firewall manager AMI's in the AWS Marketplace Navigate back to your private marketplace Note: Employees should see the private marketplace, including the custom color specified in prerequisite step 4 above. Select \"Discover products\" from the side bar, then in the \"Refine Results\" select \"Private Marketplace => Approved Products\" Subscribe and Accept the Terms for each product (firewall and firewall manager) When complete, you should see the marketplace products as subscriptions in the Perimeter account : If deploying the prescriptive architecture, once the main state machine ( ASEA-MainStateMachine_sm ) completes successfully, confirm the status of your perimeter firewall deployment If you have t2.micro ec2 instances running in any account which had the account-warming flag set to true, they will be removed on the next state machine execution; If your perimeter firewalls were defined but not deployed on first run, you will need to rerun the state machine. This happens when: you were unable to activate the firewall AMI's before stage 2 (step 16) we were not able to fully activate your account before we were ready to deploy your firewalls. This case can be identified by a running EC2 micro instance in the account, or by looking for the following log entry 'Minimum 15 minutes of account warming required for account'. In these cases, simply select the ASEA-MainStateMachine_sm in Step Functions and select Start Execution (no SM input parameters required) Known Installation Issues \u00b6 Current Issues: When a new installation includes AWS Network Firewall (NFW), we are seeing State Machine failures in Phase 1 in the Perimeter account. Timing issues are causing the first deployment of the underlying CloudFormation stack to fail and rollback, when we automatically retry the stacks deployment, we attempt to recreate the NFW CloudWatch Log groups which were retained, causing a failure. A fix is in the works. Manually delete the two NFW log groups from the perimeter account ( /ASEA/Nfw/Central-Firewall/Alert and /ASEA/Nfw/Central-Firewall/Flow ) using either the Accelerator Pipeline Role or the Org Admin Role and rerun the state machine with the input of {\"scope\": \"FULL\", \"mode\": \"APPLY\"} . If dns-resolver-logging is enabled, VPC names containing spaces are not supported at this time as the VPC name is used as part of the log group name and spaces are not supported in log group names. By default in many of the sample config files, the VPC name is auto-generated from the OU name using a variable. In this situation, spaces are also not permitted in OU names (i.e. if any account in the OU has a VPC with resolver logging enabled and the VPC is using the OU as part of its name). On larger deployments we are occassionally seeing state machine failures when Creating Config Recorders . Simply rerun the state machine with the input of {\"scope\": \"FULL\", \"mode\": \"APPLY\"} . Occasionally CloudFormation fails to return a completion signal. After the credentials eventually fail (1 hr), the state machine fails. Simply rerun the state machine with the input of {\"scope\": \"FULL\", \"mode\": \"APPLY\"} . Applying new Control Tower Detective guardrails fails in v1.5.0. This is fixed in the next release. Issues in Older Releases: New installs to releases prior to v1.3.9 are no longer supported. Upgrades to releases prior to v1.3.8 are no longer supported. Post-Installation \u00b6 The Accelerator installation is complete, but several manual steps remain: Enable and configure AWS SSO in your home region (i.e. ca-central-1) Login to the AWS Console using your Organization Management account Navigate to AWS Single Sign-On, click Enable SSO Set the SSO directory to AD (\"Settings\" => \"Identity Source\" => \"Identity Source\" => click Change , Select Active Directory, and select your domain from the list) Under \"Identity Source\" section, Click Edit beside \"Attribute mappings\", then set the email attribute to: ${dir:email} and click Save Changes Configure Multi-factor authentication, we recommend the following minimum settings: Every time they sign in (always-on) Security key and built-in authenticators Authenticator apps Require them to provide a one-time password sent by email to sign in Users can add and manage their own MFA devices Create all the default permission sets and any desired custom permission sets e.g. Select AWS accounts from the side bar, select \"Permission sets\" tab then Create permission set Use an existing job function policy => Next Select job function policy AdministratorAccess Add Tags, Review and Create repeat for each default permission set and any required custom permission sets For Control Tower based installations, remove the orphaned Permission Sets from each AWS accounts (select the account, expand Permission Sets, click Remove for each) Map MAD groups to permission sets and accounts Select AWS accounts from the side bar and select AWS organization tab Select the accounts you want to map to each MAD group and click Assign users Select your DNS domain e.g. example.local , and search for the group you would like to assign (e.g. aws- for the pre-created groups) and click Search connected directory Select the desired group aws-log-archive-View Select the permission set you would like to assign to the MAD group to (e.g. ViewOnlyAccess ) Click Finish (Note: if it fails during provisioning, simply select the failed accounts and click on \"Retry changes\") AWS SSO should be the primary mechanism for all access to all AWS accounts in your Organization, to determine or update the login page for your organization: Click on Dashboard within the AWS SSO console and note the User portal URL Share this url with all your users NOTE: the url prefix can only be changed ONCE (ever) using the settings tab, so update with caution Any pre-created AWS MAD users passwords are available in secrets manager in the AWS management account. To reset these passwords login to the Operations account through AWS SSO, then: Navigate to \"Directory Service\" , select the directory => Actions => Reset user password Users can change their passwords from any MAD domain connected instance Configure the new alb-forwarding feature (added in v1.5.0) AWS ALB's are published using DNS names which resolve to backing IP's which could silently change at any time due to a scaling event, maintenance, or a hardware failure. While published as a DNS name, ALB's can only target IP addresses. This presents a challenge as we need the ALB's in the perimeter account to target ALB's in the various back-end workload accounts; Alb-forwarding solves this problem by executing a small snippet of code every 60 seconds which updates managed alb listeners with any IP changes, ensuring any managed flows do not go offline. This removes the requirement to leverage a 3rd party appliance to perform NAT to a DNS name; Alb-forwarding is enabled on the perimeter vpc in the perimeter account in many of the sample configs and can be used with Accelerator created or customer created ALB's. This setting causes a new DynamoDB table named ASEA-Alb-Ip-Forwarding-vpc-* to be created in the perimeter account. Additionally the parameter store in the perimeter account contains the details of all Accelerator deployed alb's across the environment for easy reference; Stepts to configure: First you need to manually create a listener on the front-end alb (without a target group), multiple listeners are supported; Next, for each application that needs to be published, a record needs to be added to the DynamoDB table, see sample below; Records can be added to the table for any alb in the account running the alb-forwarding tool. Records can be added at any time. DDB change logs will trigger the initial creation of the appropriate target group(s) and IP addresses will be verified and updated every 60 seconds therafter. Sample DynamoDB JSON to add an entry to the table: { \"id\": \"App1\", \"targetAlbDnsName\": \"internal-Core-mydevacct1-alb-123456789.ca-central-1.elb.amazonaws.com\", \"targetGroupDestinationPort\": 443, \"targetGroupProtocol\": \"HTTPS\", \"vpcId\": \"vpc-0a6f44a80514daaaf\", \"rule\": { \"sourceListenerArn\": \"arn:aws:elasticloadbalancing:ca-central-1:123456789012:listener/app/Public-DevTest-perimeter-alb/b1b12e7a0c412bf3/ef9b022a4fdd8bdf\", \"condition\": { \"paths\": [\"/img/*\", \"/myApp2\"], \"hosts\": [\"aws.amazon.com\"], \"priority\": 30 } } } - where `id` is any unique text, `targetAlbDnsName` is the DNS address for the backend alb for this application (found in parameter store), `vpcId` is the vpc id containing the front-end alb (in this account), `sourceListenerArn` is the arn of the listener of the front-end alb, `paths` and `hosts` are both optional, but one of the two must be supplied. Finally, `priority` must be unique and is used to order the listener rules. Priorities should be spaced at least 40 apart to allow for easy insertion of new applications and forwarder rules. - the provided `targetAlbDnsName` must resolve to addresses within a [supported](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html) IP address space. On a per role basis, you need to enable the CWL Account Selector in the Security and the Operations accounts, in each account: Go to CloudWatch, Settings, Under Cross-account cross-region select Configure , Under View cross-account cross-region select Edit , choose AWS Organization account selector , click Save changes Configure central Ingress/Egress firewalls, if deployed Layer 3/4 appliance based inspection is an optional feature General If deployed, login to any 3rd party firewalls and firewall manager appliances and update any default passwords; Tighten security groups on the 3rd party firewall instances (using the Accelerator configuration file), further limiting access to firewall management interfaces to a set of designated and controlled CIDR ranges; Update the firewall configuration per your organizations security requirements and best practices; Diagrams reflecting perimeter traffic flows when NFW and/or GWLB are used can be found here on slides 6 through 9. AWS Network Firewall The AWS Network Firewall policies and rules deployed by the Accelerator, can only be updated using the Accelerator. Customers wishing to manage the AWS Network Firewall from the console GUI, must create a new policy with new rules created through the console and then manually associate this new policy to the Accelerator deployed Network Firewall. Customers can choose either option, but they cannot be mixed to ensures that Accelerator updates do not overwrite console based updates. Fortinet Manually update firewall configuration to forward all logs to the Accelerator deployed NLB addresses fronting the rsyslog cluster login to each firewall, select Log Settings , check Send logs to syslog , put the NLB FQDN in the IP Address/FQDN field (stored in parameter store of perimeter account) Manually update the firewall configuration to connect perimeter ALB high port flows through to internal account ALB's Note: while this option is still available, a new alb-forwarding mechanism is available in v1.5.0 (see section 2 above) which simplifies and eliminates this more complicated \"NAT to DNS name\" option; login to each firewall, switch to FG-traffic vdom, select Policies & Objects , select Addresses , Expand Addresses Set Prod1-ALB-FQDN to point to a reliable sub-account ALB FQDN, this is used for full-path health checks on all ALB's Set additional DevX-ALB-FQDN , TestX-ALB-FQDN and ProdX-ALB-FQDN to point to workload account ALB FQDNs Two of each type of ALB FQDN records have been created, when you need more, you need to create BOTH an additional FQDN and a new VIP, per ALB Each new VIP will use a new high port (i.e. 7007, 7008, etc.), all of which map back to port 443 Detailed steps can be read here . Checkpoint Download and install Checkpoint SmartConsole client on your PC (only available for Windows, if you have a Mac, you can use Workspaces to install the client) Retrieve the firewall and firewall manager SSL certificate from secrets manager in the perimeter account and save to a pem file (convert to ppk on Windows) Login to the AWS Console for the Perimeter account Navigate to Secrets Manager and select Secrets Look for Secret name ASEA/keypair/ASEA-Appliances and click on it Go to the Secret value section and click on Retrieve secret value Make sure you have the Plaintext tab selected and copy the content to a .pem file using your preferred text editor. (the file will need to be converted to ppk format on Windows) Wait approx. 25 min. after the managers \"Launch time\" and then SSH into the Firewall Manager using the SSL certificate (.pem file retrieved above) and login user admin The firewall manager EC2 instance name is ASEA-Checkpoint-FirewallMgr Once you SSH successfully, execute the following commands: set user admin password set expert-password set user admin shell /bin/bash save config The following commands are useful for troubleshooting (in expert mode): autoprov_cfg -v (check cme at Take 155 or greater) autoprov_cfg show all (check cme configuration) cat /var/log/aws-user-data.log (validate bootstrap, file should end with \"Publish operation\" succeeded (100%) ) tail -f /var/log/CPcme/cme.log (watch to ensure it finds the instances, establishes SIC and adds the nodes) Login to SmartConsole, and update the firewall policy per your organizations security requirements An outbound rule allowing http and https should exist From the RDGW host in Operations, test to see if outbound web browsing is enabled NOTES: No best practice or security configuration has been configured on the Checkpoint firewalls. These firewalls have been configured to work with GWLB, but otherwise have the default/basic Checkpoint out-of-box configuration installed Do NOT reboot the Checkpoint appliances until bootstrap is complete (~25 minutes for the manager), or you will be required to redeploy the instance Recover root passwords for all sub-accounts and apply strong passwords Process documented here Enable MFA for all IAM users and all root account users, recommendations: Yubikeys provide the strongest form of MFA protection and are strongly encouraged for all account root users and all IAM users in the Organization Management (root) account the Organization Management (root) account requires a dedicated Yubikey (if access is required to a sub-account root user, we do not want to expose the Organization Management accounts Yubikey) every ~50 sub-accounts requires a dedicated Yubikey (minimize the required number of Yubikeys and the scope of impact should a Yubikey be lost or compromised) each IAM breakglass user requires a dedicated Yubikey, as do any additional IAM users in the Organization Management (root) account. While some CSPs do not recommend MFA on the breakglass users, it is strongly encouraged in AWS all other AWS users (AWS SSO, IAM in sub-accounts, etc.) can leverage virtual MFA devices (like Google Authenticator on a mobile device) Customers are responsible for the ongoing management and rotation of all passwords on a regular basis per their organizational password policy. This includes the passwords of all IAM users, MAD users, firewall users, or other users, whether deployed by the Accelerator or not. We do NOT automatically rotate any passwords, but strongly encourage customers do so, on a regular basis. During the installation we request required limit increases, resources dependent on these limits will not be deployed Limit increase requests are controlled through the Accelerator configuration file \"limits\":{} setting The sample configuration file requests increases to your EIP count in the perimeter account and to the VPC count and Interface Endpoint count in the shared-network account You should receive emails from support confirming the limit increases On the next state machine execution, resources blocked by limits should be deployed (i.e. additional VPC's and Endpoints) If more than 2 days elapses without the limits being increased, on the next state machine execution, they will be re-requested Note: After a successful install the Control Tower Organizational units dashboard will indicate 2 of 3 in the Accounts enrolled column for the Security OU, as it does not enable enrollment of the management account in guardrails. The Accelerator compliments Control Tower and enables guardrails in the management account which is important to high compliance customers.","title":"Installation"},{"location":"installation/install/#accelerator-installation-and-upgrade-guide","text":"We encourage customers installing the Accelerator to get the support of their local AWS account team (SA, TAM, CSM, Proserve) to assist with the installation of the Accelerator, as the Accelerator leverages, deploys, or orchestrates over 30 different AWS services. Users are strongly encouraged to also read the Accelerator Operations/Troubleshooting Guide before installation. The Operations/Troubleshooting Guide provides details as to what is being performed at each stage of the installation process, including detailed troubleshooting guidance. These installation instructions assume one of the prescribed architectures is being deployed.","title":"Accelerator Installation and Upgrade Guide"},{"location":"installation/install/#installation","text":"","title":"Installation"},{"location":"installation/install/#prerequisites","text":"","title":"Prerequisites"},{"location":"installation/install/#general","text":"Management or root AWS Organization account (the AWS Accelerator cannot be deployed in an AWS sub-account) No additional AWS accounts need to be pre-created before Accelerator installation If required, a limit increase to support your desired number of new AWS sub-accounts (default limit is 10 sub-accounts) Valid Accelerator configuration file, updated to reflect your requirements (see below) Determine your primary or Accelerator control or home region, this is the AWS region in which you will most often operate Government of Canada customers are still required to do a standalone installation at this time, please request standalone installation instructions from your Account SA or TAM The Accelerator can be installed into existing AWS Organizations - see caveats and notes in section 4 below Existing AWS Landing Zone Solution (ALZ) customers are required to remove their ALZ deployment before deploying the Accelerator. Scripts are available to assist with this process. Changes to the Accelerator codebase are strongly discouraged unless they are contributed and accepted back to the solution. Code customization will block the ability to upgrade to the latest release and upgrades are encouraged to be done between quarterly to semi-annually. The solution was designed to be extremely customizable without changing code, existing customers following these guidelines have been able to upgrade across more than 50 Accelerator releases, while maintaining their customizations and gaining the latest bug fixes, features and enhancements without any developer or professional services based support. Please see this FAQ for more details.","title":"General"},{"location":"installation/install/#production-deployment-planning","text":"","title":"Production Deployment Planning"},{"location":"installation/install/#general_1","text":"For any deployment of the Accelerator which is intended to be used for production workloads, you must evaluate all these decisions carefully. Failure to understand these choices could cause challenges down the road. If this is a \"test\" or \"internal\" deployment of the Accelerator which will not be used for production workloads, you can leave the default config values. Config file schema ) documentation (Draft)","title":"General"},{"location":"installation/install/#ou-structure-planning","text":"Plan your OU and core account structure carefully. By default, we suggest: Security, Infrastructure, Central, Sandbox, Dev, Test, Prod . The Security OU will contain the Security account, the Log Archive account, and the Organization Management account. The Infrastructure OU will hold the remainder of the accounts shared or utilized by the rest of the organization ( Shared Network , Perimeter , and Operations ). The remainder of the OUs correspond with major permission shifts in the SDLC cycle and NOT every stage an organization has in their SDLC cycle (i.e. QA or pre-prod would be included in one of the other OUs). The Central OU is used to hold accounts with workloads shared across Dev, Test, and Prod environments like centralized CI/CD tooling. The v1.5.0 release aligns the Accelerator OU and account structure with AWS multi-account guidance, splitting the core OU into the Security and Infrastructure OUs. Note: While OUs can be renamed or additional OUs added at a later point in time, deployed AWS accounts CANNOT be moved between top-level OUs (guardrail violation), nor can top-level OUs easily be deleted (requires deleting all AWS accounts from within the OU first).","title":"OU Structure Planning"},{"location":"installation/install/#network-configuration-planning","text":"If deploying the prescriptive architecture using the Full or Lite sample config files, you will need the following network constructs: Six (6) RFC1918 Class B address blocks (CIDR's) which do not conflict with your on-premise networks (a single /13 block works well) VPC CIDR blocks cannot be changed after installation, this is simply the way the AWS platform works, given everything is built on top of them. Carefully consider your address block selection. one block for each OU, except Sandbox which is not routable (Sandbox OU will use a 7th non-routed address block) the \"core\" Class B range will be split to support the Endpoint VPC and Perimeter VPC (with extra addresses remaining for future use) Given a shared VPC architecture is leveraged (prevents stranded islands of CIDR blocks and reduces networking costs), we have assigned a class B address block to each VPC to future proof the deployment. Smaller customers can successfully deploy with a half class B CIDR block per shared VPC. Two (2) RFC6598 /23 address blocks (Government of Canada (GC) requirement only) Used for AWS Managed Active Directory (MAD) deployment and perimeter underlay network non-GC customers can replace the RFC6598 address space with the extra unused addresses from the above RFC1918 CIDR range above (the App2 subnets in the Central VPC and the Perimeter VPC address space) BGP ASN's for network routing, one for each of: Transit Gateway (one unique ASN per TGW, multi-region example requires a second ASN) IPSec VPN Firewall Cluster (if deployed) VGW for DirectConnect connectivity (only shown in the config.multi-region-example.json) For example: the Control Tower with Network Firewall example config requires a single BGP ASN for the TGW, the IPSec VPN example requires two BGP ASN's, and the multi-region example requires five unique BGP ASN's. NOTE: Prior to v1.5.0 CIDR ranges were assigned to each VPC and subnet throughout the config file. This required customers to perform extensive updates across the config file when needing to move to specific IP ranges compatible with a customer's existing on-premise networks. While this is still supported for those wanting to control exactly what address is used on every subnet, the solution has added support for dynamic CIDR assignments and the sample config files have been updated to reflect. New installs will have CIDR's pulled from CIDR pools, defined in the global-options section of the config file with state maintained in DynamoDB. The v1.5.0 custom upgrade guide will provides details on the upgrade process and requirements to migrate to the new CIDR assignment system, if desired. A script was created to assist with this migration.","title":"Network Configuration Planning"},{"location":"installation/install/#dns-domain-name-tls-certificate-planning","text":"If deploying the prescriptive architecture, you must decide on: A unique Windows domain name ( organizationaws / organization.aws , organizationcloud / organization.cloud , etc.). Given this is designed as the primary identity store and used to domain join all cloud hosted workloads, changing this in future is difficult. Pick a Windows domain name that does NOT conflict with your on-premise AD domains, ensuring the naming convention conforms to your organizations domain naming standards to ensure you can eventually create a domain trust between the MAD and on-premise domains/forests DNS Domain names and DNS server IP's for on-premise private DNS zones requiring cloud resolution (can be added in future) DNS Domain for a cloud hosted public zone \"public\": [\"organization.cloud-nuage.canada.ca\"] (can be added in future) DNS Domain for a cloud hosted private zone \"private\": [\"organization.cloud-nuage.gc.ca\"] (can be added in future) Wildcard TLS certificate for each of the 2 previous zones (can be added/changed in future)","title":"DNS, Domain Name, TLS Certificate Planning"},{"location":"installation/install/#email-address-planning","text":"While you require a minimum of 6 unique email addresses (1 per sub-account being created), we recommend at least 20 unique email ALIASES associated with a single mailbox, never used before to open AWS accounts, such that you do not need to request new email aliases every time you need to create a new AWS account and they can all be monitored via a single mailbox. These email addresses can never have been used to previously open an AWS account. You additionally require email addresses for the following additional purposes (these can be existing monitored mailboxes and do not need to be unique): Accelerator execution (state machine) notification events (1 address) High, Medium and Low security alerts (3 addresses if you wish to segregate alerts) Budget notifications","title":"Email Address Planning"},{"location":"installation/install/#centralized-ingressegress-firewalls","text":"As of v1.5.0 the Accelerator offers multiple automated firewall deployment options: a) AWS Network Firewall (native AWS Cloud service) Defined in the config file as part of a VPC b) 3rd party firewalls interconnected to the cloud tenancy via IPSec VPN (Active/Active using BGP + ECMP) Defined in the config file under deployments w/TGW VPN attachments this was the only automated option prior to v1.5.0 a sample Fortinet Fortigate configuration is provided (both PAYGO and BYOL supported) For Fortinet BYOL, requires minimum 2 valid license files (evaluation licenses adequate) (can be added in future) c) 3rd party firewalls interconnected to the cloud tenancy via Gateway Load Balancer (GWLB) in an auto-scaling group Defined in the config file under both deployments and load balancers a sample Checkpoint CloudGuard configuration is provided (both PAYGO and BYOL supported) d) Customer gateway (CGW) creation, to enable connectivity to on-premises firewalls or manually deployed cloud firewalls Defined in the config file under deployments w/TGW VPN attachments (but without an AMI or VPC association) Examples of each of the firewall options have been included as variants of the Lite config file example . Note: While we only provide a single example for each 3rd party implementation today, the implementations are generic and should be usable by any 3rd party firewall vendor, assuming they support the required features and protocols. The two examples were driven by customer demand and heavy lifting by the 3rd party vendor. We look forward to additional vendors developing and contributing additional sample configurations. For new 3rd party integrations, we encourage the use of the GWLB approach.","title":"Centralized Ingress/Egress Firewalls"},{"location":"installation/install/#other","text":"We recommend installing with the default Accelerator Name ( ASEA ) and Accelerator Prefix ( ASEA- ), but allow customization. Prior to v1.5.0 the defaults were ( PBMM ) and ( PBMMAccel- ) respectively. the Accelerator name and prefix CANNOT be changed after the initial installation; the Accelerator prefix including the mandatory dash cannot be longer than 10 characters. New installations, which now leverage Control Tower, require the organization-admin-role be set to AWSControlTowerExecution . Existing standalone installations will continue to utilize their existing role name for the organization-admin-role , typically OrganizationAccountAccessRole , as this role is used by AWS Organizations by default when no role name is specified while creating AWS accounts through the AWS console. the Accelerator leverages this role name to create all new accounts in the organization; this role name, as defined in the config file, MUST be utilized when manually creating all new sub-accounts in the Organization; existing installs wishing to change the role name are required to first deploy a new role with a trust to the root account, in all accounts in the organization.","title":"Other"},{"location":"installation/install/#accelerator-pre-install-steps","text":"","title":"Accelerator Pre-Install Steps"},{"location":"installation/install/#general_2","text":"Before installing, you must first: Login to the Organization Management (root) AWS account with AdministratorAccess . Set the region to your desired home region (i.e. ca-central-1 ) Install AWS Control Tower: Government of Canada customers are required to skip this step OU and account names can ONLY be customized during initial installation. These values MUST match with the values supplied in the Accelerator config file. Go to the AWS Control Tower console and click Set up landing zone Select your home region (i.e. ca-central-1 ) the Accelerator home region must match the Control Tower home region Select all regions for Additional AWS Regions for governance , click Next The Control Tower and Accelerator regions MUST be properly aligned If a region is not governed by Control Tower, it must NOT be listed in control-tower-supported-regions To manage a region requires the region: be enabled in Control Tower (if supported) added to the config file control-tower-supported-regions list (if supported) added to the config file supported-regions list (even if not supported by Control Tower, as the Accelerator can manage regions not yet supported by Control Tower, but only when NOT listed in control-tower-supported-regions ) While we highly recommend guardrail deployment for all AWS enabled by default regions, at minimum the home region MUST be enabled in Control Tower and must be listed in control-tower-supported-regions both the home-region and ${GBL_REGION} must be listed in supported-regions For the Foundational OU , leave the default value Security For the Additional OU provide the value Infrastructure , click Next Enter the email addresses for your Log Archive and Audit accounts, change the Audit account name to Security , click Next OU and account names can ONLY be customized during initial installation. OU names, account names and email addresses must match identically with the values supplied in the Accelerator config file. Click setup and wait ~60 minutes for the Control Tower installation to complete Select Add or register organizational units , Click Add an OU Type Dev , click Add , wait until the OU is finished provisioning (or it will error) Repeat step 9 for each OU (i.e. Test , Prod , Central , Sandbox ) Select Account factory , Edit, Subnets: 0, Deselect all regions, click Save In AWS Organizations, move the Management account from the root OU into the Security OU Verify: AWS Organizations is enabled in All features mode if required, navigate to AWS Organizations, click Create Organization , Create Organization Service Control Policies are enabled if required, in Organizations, select Policies , Service control policies , Enable service control policies Verify the Organization Management (root) account email address In AWS Organizations, Settings, \"Send Verification Request\" Once it arrives, complete the validation by clicking the validation link in the email Create a new KMS key to encrypt your source configuration bucket (you can use an existing key) AWS Key Management Service, Customer Managed Keys, Create Key, Symmetric, and then provide a key name ( ASEA-Source-Bucket-Key ), Next Select a key administrator (Admin Role or Group for the Organization Management account), Next Select key users (Admin Role or Group for the Organization Management account), Next Validate an entry exists to \"Enable IAM User Permissions\" (critical step if using an existing key) \"arn:aws:iam::123456789012:root\" , where 123456789012 is your Organization Management account id. Click Finish Select the new key, Select Key Rotation , Automatically rotate this CMK every year , click Save. Enable \"Cost Explorer\" (My Account, Cost Explorer, Enable Cost Explorer) With recent platform changes, Cost Explorer may now be auto-enabled (unable to confirm) Enable \"Receive Billing Alerts\" (My Account, Billing Preferences, Receive Billing Alerts) It is extremely important that all the account contact details be validated in the Organization Management (root) account before deploying any new sub-accounts. This information is copied to every new sub-account on creation. Subsequent changes to this information require manually updating it in each sub-account. Go to My Account and verify/update the information lists under both the Contact Information section and the Alternate Contacts section. Please ESPECIALLY make sure the email addresses and Phone numbers are valid and regularly monitored. If we need to reach you due to suspicious account activity, billing issues, or other urgent problems with your account - this is the information that is used. It is CRITICAL it is kept accurate and up to date at all times.","title":"General"},{"location":"installation/install/#create-github-personal-access-token-and-store-in-secrets-manager","text":"As of v1.5.0, the Accelerator offers deployment from either GitHub or CodeCommit: GitHub (recommended) You require a GitHub access token to access the code repository Instructions on how to create a personal access token are located here . Select the scope public_repo underneath the section repo: Full control over private repositories . Store the personal access token in Secrets Manager as plain text. Name the secret accelerator/github-token (case sensitive). Via AWS console Store a new secret, and select Other type of secrets , Plaintext Paste your secret with no formatting no leading or trailing spaces (i.e. completely remove the example text) Select the key you created above ( ASEA-Source-Bucket-Key ), Set the secret name to accelerator/github-token (case sensitive) Select Disable rotation CodeCommit (alternative option) Multiple options exist for downloading the GitHub Accelerator codebase and pushing it into CodeCommit. As this option is only for advanced users, detailed instructions are not provided. In your AWS Organization Management account, open CodeCommit and create a new repository named aws-secure-environment-accelerator Go to GitHub and download the repository Source code zip or tarball for the release you wish to deploy Do NOT download the code off the main GitHub branch, this will leave you in a completely unsupported state (and with beta code) Push the extracted codebase into the newly created CodeCommit repository, maintaining the file/folder hierarchy Set the default CodeCommit branch for the new repository to main Create a branch following the Accelerator naming format for your release (i.e. release/v1.5.0 )","title":"Create GitHub Personal Access Token and Store in Secrets Manager"},{"location":"installation/install/#aws-internal-employee-accounts-only","text":"If deploying to an internal AWS employee account and installing the solution with a 3rd party firewall, you need to enable Private Marketplace (PMP) before starting: In the Organization Management account go here: https://aws.amazon.com/marketplace/privatemarketplace/create Click Create a Private Marketplace , and wait for activation to complete Go to the \"Account Groups\" sub-menu, click Create account group Enter an Account Group Title (i.e. Default ) and Add the Management (root) account number in Associate AWS account Associate the default experience New Private Marketplace , then click Create account group and wait for it to create Go to \"Experiences\" sub-menu, select New Private Marketplace Select the \"Settings\" sub-tab, and click the Not Live slider to make it Live and wait for it to complete Ensure the \"Software requests\" slider is set to Requests off and wait for it to complete Change the name field (i.e. append -PMP ) and change the color, so it is clear PMP is enabled for users, click Update Go to the \"Products\" sub-tab, then select the All AWS Marketplace products nested sub-tab Search Private Marketplace for the Fortinet or Checkpoint products and select Fortinet FortiGate (BYOL) Next-Generation Firewall and Fortinet FortiManager (BYOL) Centralized Security Management or CloudGuard Network Security for Gateway Load Balancer - BYOL and Check Point Security Management (BYOL) Select \"Add\" in the top right Due to PMP provisioning delays, this sometimes fails when attempted immediately following enablement of PMP or if adding each product individually - retry after 20 minutes. While not used in this account, you must now subscribe to the two subscriptions and accept the EULA for each product (you will need to do the same in the perimeter account, once provisioned below) To subscribe, select the \"Approved products\" tab Click on the product you want to subscribe, in this case Fortinet FortiGate (BYOL) Next-Generation Firewall and Fortinet FortiManager (BYOL Centralized Security Management or CloudGuard Network Security for Gateway Load Balancer - BYOL and Check Point Security Management (BYOL) Click on \"Continue to Subscribe\" Click on \"Accept Terms\" and wait for subscription to be completed If you are deploying in any region except ca-central-1 or wish to switch to a different license type, you need the new AMI id's. After successfully subscribing, continue one more step and click the \u201cContinue to Configuration\u201d. When you get the below screen, select your region and version ( Fortinet v6.4.7 , Checkpoint Mgmt R81.10-335.883 and CloudGuard R80.40-294.374 recommended at this time). Marketplace will provide the required AMI id. Document the two AMI id's, as you will need to update them in your config.json file below.","title":"AWS Internal (Employee) Accounts Only"},{"location":"installation/install/#basic-accelerator-configuration","text":"Select a sample config file as a baseline starting point IMPORTANT: Use a config file from the Github code branch you are deploying from, as valid parameters change over time. The main branch is NOT the current release and often will not work with the GA releases. sample config files can be found in this folder; descriptions of the sample config files and customization guidance can be found here ; unsure where to start, use the config.lite-CTNFW-example.json , where CTNFW is for Control Tower w/NFW; These configuration files can be used, as-is, with only minor modification to successfully deploy the sample architectures; On upgrades, compare your deployed configuration file with the latest branch configuration file for any new or changed parameters; At minimum, you MUST update the AWS account names and email addresses in the sample file: For existing accounts, they must match identically to both the account names and email addresses defined in AWS Organizations; For new accounts, they must reflect the new account name/email you want created; All new AWS accounts require a unique email address which has never before been used to create an AWS account; When updating the budget or SNS notification email addresses within the sample config, a single email address for all is sufficient; Update the IP address in the \"alarm-not-ip\" variable with your on-premise IP ranges (used for the AWS-SSO-Authentication-From-Unapproved-IP alarm); If deploying the Managed AD, update the dns-domain, netbios-domain, log-group-name, as well as the AD users and groups that will be created; For a test deployment, the remainder of the values can be used as-is; While it is generally supported, we recommend not adding more than 1 or 2 workload accounts to the config file during the initial deployment as it will increase risks of hitting a limit. Once the Accelerator is successfully deployed, add the additional accounts to the config file and rerun the state machine. More information here on the fields in the config file that need to be updated. A successful deployment of the prescriptive architecture requires VPC access to 9 AWS endpoints, you cannot remove both the perimeter firewalls (all public endpoints) and the 9 required central VPC endpoints from the config file (ec2, ec2messages, ssm, ssmmessages, cloudformation, secretsmanager, kms, logs, monitoring). When deploying to regions other than ca-central-1 , you need to modify your config file as follows (for Canada Central 1, the AMI ids are pre-populated for you): Update the firewall and firewall manager AMI id's to reflect your home regions regional AMI id's (see 2.3.3, item 13), making sure you select the right version and region per the recommendations. Validate all the Interface Endpoints defined in your config file are supported in your home region (i.e. Endpoint VPC). Remove unsupported endpoints from the config file, add additional endpoints as available. If you are installing into a home region which is explicitly named in any of the replacements\\addl_regions_x, remove it from the list. If deploying in us-east-1, remove ${GBL_REGION}. Create an S3 bucket in your Organization Management account your-bucket-name you must supply this bucket name in the CFN parameters and in the config file ( global-options\\central-bucket ) the bucket name must be the same in both spots the bucket must have versioning enabled the bucket must be S3-KMS encrypted using the ASEA-Source-Bucket-Key created above Place your customized config file(s), named config.json (or config.yaml ), in your new bucket If required, place the firewall configuration and license files in the folder and path defined in the config file For AWS Network Firewall: nfw/nfw-example-policy.json For Fortinet: firewall/firewall-example.txt , firewall/license1.lic and firewall/license2.lic We have made a sample available here : ./reference-artifacts/Third-Party/ the samples configures an active / active firewall pair with two tunnels per firewall If you updated your perimeter VPC subnet names, you must also make these changes in your firewall-example.txt file If you don't have any license files, update the config file with an empty array ( \"license\": [] ). Do NOT use the following: [\"\"] . The basic Checkpoint configuration is stored directly in config.json Place any defined certificate files in the folder and path defined in the config file i.e. certs/example1-cert.key , certs/example1-cert.crt Sample available here : ./reference-artifacts/Certs-Sample/* Ideally you would generate real certificates using your existing certificate authority Should you wish, instructions are provided to aid in generating your own self-signed certificates (Self signed certificates are NOT secure and simply for demo purposes) Use the examples to demonstrate Accelerator TLS functionality only Detach ALL SCPs (except FullAWSAccess which remains in place) from all OU's and accounts before proceeding For Control Tower based installs do NOT detach Control Tower SCPs (i.e. aws-guardrails-xxxxxx) Installation will fail if this step is skipped","title":"Basic Accelerator Configuration"},{"location":"installation/install/#installation_1","text":"You can find the latest release in the repository here . We only support new installations of v1.3.9 or above (older releases continue to function) Download the CloudFormation (CFN) template for the release you plan to install (either AcceleratorInstallerXXX.template.json for GitHub or AcceleratorInstallerXXX-CodeCommit.template.json for CodeCommit) Use the provided CloudFormation template to deploy a new stack in your Management (root) AWS account As previously stated we do not support installation in sub-accounts Login to your Organization Management account and make sure you are in your desired home region (i.e. ca-central-1 ) (your desired primary or control region) Navigate to CloudFormation in the AWS Console and click Create stack with new resources (standard) , then Select \"Template is ready\" For the \"Specify template\" select \"Upload a template file\" Select the *.template.json file you downloaded in step 2 above Click Next Fill out the required parameters - LEAVE THE DEFAULTS UNLESS SPECIFIED BELOW Specify Stack Name STARTING with ASEA- (case sensitive) suggest a suffix of orgname or username Change ConfigS3Bucket to the name of the bucket you created above your-bucket-name Add an Email address to be used for State Machine Status notification The GithubBranch should point to the release you selected if upgrading, change it to point to the desired release the latest stable branch is currently release/v1.5.0 , case sensitive click Next Finish deploying the stack Apply a tag on the stack, Key= Accelerator , Value= ASEA (case sensitive). ENABLE STACK TERMINATION PROTECTION under Stack creation options Click Next , Acknowledge resource creation, and click Create stack The stack typically takes under 5 minutes to deploy. Once deployed, you should see a CodePipeline project named ASEA-InstallerPipeline in your account. This pipeline connects to Github, pulls the code from the prescribed branch and deploys the Accelerator state machine. if the CloudFormation fails to deploy with an Internal Failure , or, if the pipeline fails connecting to GitHub, then: fix the issue with your GitHub secret created in section 2.3.2, then delete the Installer CloudFormation stack you just deployed, and restart at step 3 of this section. For new stack deployments, when the stack deployment completes, the Accelerator state machine will automatically execute (in Code Pipeline). When upgrading you must manually Release Change to start the pipeline. While the pipeline is running: review the list of Known Installation Issues in section 2.5.1 below review the Accelerator Basic Operation and Frequently Asked Questions (FAQ) Document Once the pipeline completes (~10 mins), the main state machine, named ASEA-MainStateMachine_sm , will start in Step Functions The state machine time is dependent on the quantity of resources being deployed. On an initial installation of a more complex sample configuration files, it takes approximately 2 hours to execute (depending on the configuration file). Timing for subsequent executions depends entirely on what resources are changed in the configuration file, but often takes as little as 20 minutes. While you can watch the state machine in Step Functions, you will also be notified via email when the State Machine completes (or fails). Successful state machine executions include a list of all accounts which were successfully processed by the Accelerator. The configuration file will be automatically moved into Code Commit (and deleted from S3). From this point forward, you must update your configuration file in CodeCommit. You will receive an email from the State Machine SNS topic and the 3 SNS alerting topics. Please confirm all four (4) email subscriptions to enable receipt of state machine status and security alert messages. Until completed, you will not receive any email messages (must be completed within 7-days). If the state machine fails : Refer to the Troubleshooting Guide for instructions on how to inspect and retrieve the error You can also refer to the FAQ and Known Installation Issues Once the error is resolved, re-run the step function ASEA-MainStateMachine_sm using {\"scope\": \"FULL\",\"mode\": \"APPLY\"} as input If deploying a prescriptive architecture with 3rd party firewalls, after the perimeter account is created in AWS Organizations, but before the Accelerator reaches Stage 2: NOTE: If you miss the step, or fail to execute it in time, no need to be concerned, you will simply need to re-run the main state machine ( ASEA-MainStateMachine_sm ) to deploy the firewall (no SM input parameters required) Login to the perimeter sub-account (Assume your organization-admin-role ) Activate the 3rd party vendor firewall and firewall manager AMI's in the AWS Marketplace Navigate back to your private marketplace Note: Employees should see the private marketplace, including the custom color specified in prerequisite step 4 above. Select \"Discover products\" from the side bar, then in the \"Refine Results\" select \"Private Marketplace => Approved Products\" Subscribe and Accept the Terms for each product (firewall and firewall manager) When complete, you should see the marketplace products as subscriptions in the Perimeter account : If deploying the prescriptive architecture, once the main state machine ( ASEA-MainStateMachine_sm ) completes successfully, confirm the status of your perimeter firewall deployment If you have t2.micro ec2 instances running in any account which had the account-warming flag set to true, they will be removed on the next state machine execution; If your perimeter firewalls were defined but not deployed on first run, you will need to rerun the state machine. This happens when: you were unable to activate the firewall AMI's before stage 2 (step 16) we were not able to fully activate your account before we were ready to deploy your firewalls. This case can be identified by a running EC2 micro instance in the account, or by looking for the following log entry 'Minimum 15 minutes of account warming required for account'. In these cases, simply select the ASEA-MainStateMachine_sm in Step Functions and select Start Execution (no SM input parameters required)","title":"Installation"},{"location":"installation/install/#known-installation-issues","text":"Current Issues: When a new installation includes AWS Network Firewall (NFW), we are seeing State Machine failures in Phase 1 in the Perimeter account. Timing issues are causing the first deployment of the underlying CloudFormation stack to fail and rollback, when we automatically retry the stacks deployment, we attempt to recreate the NFW CloudWatch Log groups which were retained, causing a failure. A fix is in the works. Manually delete the two NFW log groups from the perimeter account ( /ASEA/Nfw/Central-Firewall/Alert and /ASEA/Nfw/Central-Firewall/Flow ) using either the Accelerator Pipeline Role or the Org Admin Role and rerun the state machine with the input of {\"scope\": \"FULL\", \"mode\": \"APPLY\"} . If dns-resolver-logging is enabled, VPC names containing spaces are not supported at this time as the VPC name is used as part of the log group name and spaces are not supported in log group names. By default in many of the sample config files, the VPC name is auto-generated from the OU name using a variable. In this situation, spaces are also not permitted in OU names (i.e. if any account in the OU has a VPC with resolver logging enabled and the VPC is using the OU as part of its name). On larger deployments we are occassionally seeing state machine failures when Creating Config Recorders . Simply rerun the state machine with the input of {\"scope\": \"FULL\", \"mode\": \"APPLY\"} . Occasionally CloudFormation fails to return a completion signal. After the credentials eventually fail (1 hr), the state machine fails. Simply rerun the state machine with the input of {\"scope\": \"FULL\", \"mode\": \"APPLY\"} . Applying new Control Tower Detective guardrails fails in v1.5.0. This is fixed in the next release. Issues in Older Releases: New installs to releases prior to v1.3.9 are no longer supported. Upgrades to releases prior to v1.3.8 are no longer supported.","title":"Known Installation Issues"},{"location":"installation/install/#post-installation","text":"The Accelerator installation is complete, but several manual steps remain: Enable and configure AWS SSO in your home region (i.e. ca-central-1) Login to the AWS Console using your Organization Management account Navigate to AWS Single Sign-On, click Enable SSO Set the SSO directory to AD (\"Settings\" => \"Identity Source\" => \"Identity Source\" => click Change , Select Active Directory, and select your domain from the list) Under \"Identity Source\" section, Click Edit beside \"Attribute mappings\", then set the email attribute to: ${dir:email} and click Save Changes Configure Multi-factor authentication, we recommend the following minimum settings: Every time they sign in (always-on) Security key and built-in authenticators Authenticator apps Require them to provide a one-time password sent by email to sign in Users can add and manage their own MFA devices Create all the default permission sets and any desired custom permission sets e.g. Select AWS accounts from the side bar, select \"Permission sets\" tab then Create permission set Use an existing job function policy => Next Select job function policy AdministratorAccess Add Tags, Review and Create repeat for each default permission set and any required custom permission sets For Control Tower based installations, remove the orphaned Permission Sets from each AWS accounts (select the account, expand Permission Sets, click Remove for each) Map MAD groups to permission sets and accounts Select AWS accounts from the side bar and select AWS organization tab Select the accounts you want to map to each MAD group and click Assign users Select your DNS domain e.g. example.local , and search for the group you would like to assign (e.g. aws- for the pre-created groups) and click Search connected directory Select the desired group aws-log-archive-View Select the permission set you would like to assign to the MAD group to (e.g. ViewOnlyAccess ) Click Finish (Note: if it fails during provisioning, simply select the failed accounts and click on \"Retry changes\") AWS SSO should be the primary mechanism for all access to all AWS accounts in your Organization, to determine or update the login page for your organization: Click on Dashboard within the AWS SSO console and note the User portal URL Share this url with all your users NOTE: the url prefix can only be changed ONCE (ever) using the settings tab, so update with caution Any pre-created AWS MAD users passwords are available in secrets manager in the AWS management account. To reset these passwords login to the Operations account through AWS SSO, then: Navigate to \"Directory Service\" , select the directory => Actions => Reset user password Users can change their passwords from any MAD domain connected instance Configure the new alb-forwarding feature (added in v1.5.0) AWS ALB's are published using DNS names which resolve to backing IP's which could silently change at any time due to a scaling event, maintenance, or a hardware failure. While published as a DNS name, ALB's can only target IP addresses. This presents a challenge as we need the ALB's in the perimeter account to target ALB's in the various back-end workload accounts; Alb-forwarding solves this problem by executing a small snippet of code every 60 seconds which updates managed alb listeners with any IP changes, ensuring any managed flows do not go offline. This removes the requirement to leverage a 3rd party appliance to perform NAT to a DNS name; Alb-forwarding is enabled on the perimeter vpc in the perimeter account in many of the sample configs and can be used with Accelerator created or customer created ALB's. This setting causes a new DynamoDB table named ASEA-Alb-Ip-Forwarding-vpc-* to be created in the perimeter account. Additionally the parameter store in the perimeter account contains the details of all Accelerator deployed alb's across the environment for easy reference; Stepts to configure: First you need to manually create a listener on the front-end alb (without a target group), multiple listeners are supported; Next, for each application that needs to be published, a record needs to be added to the DynamoDB table, see sample below; Records can be added to the table for any alb in the account running the alb-forwarding tool. Records can be added at any time. DDB change logs will trigger the initial creation of the appropriate target group(s) and IP addresses will be verified and updated every 60 seconds therafter. Sample DynamoDB JSON to add an entry to the table: { \"id\": \"App1\", \"targetAlbDnsName\": \"internal-Core-mydevacct1-alb-123456789.ca-central-1.elb.amazonaws.com\", \"targetGroupDestinationPort\": 443, \"targetGroupProtocol\": \"HTTPS\", \"vpcId\": \"vpc-0a6f44a80514daaaf\", \"rule\": { \"sourceListenerArn\": \"arn:aws:elasticloadbalancing:ca-central-1:123456789012:listener/app/Public-DevTest-perimeter-alb/b1b12e7a0c412bf3/ef9b022a4fdd8bdf\", \"condition\": { \"paths\": [\"/img/*\", \"/myApp2\"], \"hosts\": [\"aws.amazon.com\"], \"priority\": 30 } } } - where `id` is any unique text, `targetAlbDnsName` is the DNS address for the backend alb for this application (found in parameter store), `vpcId` is the vpc id containing the front-end alb (in this account), `sourceListenerArn` is the arn of the listener of the front-end alb, `paths` and `hosts` are both optional, but one of the two must be supplied. Finally, `priority` must be unique and is used to order the listener rules. Priorities should be spaced at least 40 apart to allow for easy insertion of new applications and forwarder rules. - the provided `targetAlbDnsName` must resolve to addresses within a [supported](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html) IP address space. On a per role basis, you need to enable the CWL Account Selector in the Security and the Operations accounts, in each account: Go to CloudWatch, Settings, Under Cross-account cross-region select Configure , Under View cross-account cross-region select Edit , choose AWS Organization account selector , click Save changes Configure central Ingress/Egress firewalls, if deployed Layer 3/4 appliance based inspection is an optional feature General If deployed, login to any 3rd party firewalls and firewall manager appliances and update any default passwords; Tighten security groups on the 3rd party firewall instances (using the Accelerator configuration file), further limiting access to firewall management interfaces to a set of designated and controlled CIDR ranges; Update the firewall configuration per your organizations security requirements and best practices; Diagrams reflecting perimeter traffic flows when NFW and/or GWLB are used can be found here on slides 6 through 9. AWS Network Firewall The AWS Network Firewall policies and rules deployed by the Accelerator, can only be updated using the Accelerator. Customers wishing to manage the AWS Network Firewall from the console GUI, must create a new policy with new rules created through the console and then manually associate this new policy to the Accelerator deployed Network Firewall. Customers can choose either option, but they cannot be mixed to ensures that Accelerator updates do not overwrite console based updates. Fortinet Manually update firewall configuration to forward all logs to the Accelerator deployed NLB addresses fronting the rsyslog cluster login to each firewall, select Log Settings , check Send logs to syslog , put the NLB FQDN in the IP Address/FQDN field (stored in parameter store of perimeter account) Manually update the firewall configuration to connect perimeter ALB high port flows through to internal account ALB's Note: while this option is still available, a new alb-forwarding mechanism is available in v1.5.0 (see section 2 above) which simplifies and eliminates this more complicated \"NAT to DNS name\" option; login to each firewall, switch to FG-traffic vdom, select Policies & Objects , select Addresses , Expand Addresses Set Prod1-ALB-FQDN to point to a reliable sub-account ALB FQDN, this is used for full-path health checks on all ALB's Set additional DevX-ALB-FQDN , TestX-ALB-FQDN and ProdX-ALB-FQDN to point to workload account ALB FQDNs Two of each type of ALB FQDN records have been created, when you need more, you need to create BOTH an additional FQDN and a new VIP, per ALB Each new VIP will use a new high port (i.e. 7007, 7008, etc.), all of which map back to port 443 Detailed steps can be read here . Checkpoint Download and install Checkpoint SmartConsole client on your PC (only available for Windows, if you have a Mac, you can use Workspaces to install the client) Retrieve the firewall and firewall manager SSL certificate from secrets manager in the perimeter account and save to a pem file (convert to ppk on Windows) Login to the AWS Console for the Perimeter account Navigate to Secrets Manager and select Secrets Look for Secret name ASEA/keypair/ASEA-Appliances and click on it Go to the Secret value section and click on Retrieve secret value Make sure you have the Plaintext tab selected and copy the content to a .pem file using your preferred text editor. (the file will need to be converted to ppk format on Windows) Wait approx. 25 min. after the managers \"Launch time\" and then SSH into the Firewall Manager using the SSL certificate (.pem file retrieved above) and login user admin The firewall manager EC2 instance name is ASEA-Checkpoint-FirewallMgr Once you SSH successfully, execute the following commands: set user admin password set expert-password set user admin shell /bin/bash save config The following commands are useful for troubleshooting (in expert mode): autoprov_cfg -v (check cme at Take 155 or greater) autoprov_cfg show all (check cme configuration) cat /var/log/aws-user-data.log (validate bootstrap, file should end with \"Publish operation\" succeeded (100%) ) tail -f /var/log/CPcme/cme.log (watch to ensure it finds the instances, establishes SIC and adds the nodes) Login to SmartConsole, and update the firewall policy per your organizations security requirements An outbound rule allowing http and https should exist From the RDGW host in Operations, test to see if outbound web browsing is enabled NOTES: No best practice or security configuration has been configured on the Checkpoint firewalls. These firewalls have been configured to work with GWLB, but otherwise have the default/basic Checkpoint out-of-box configuration installed Do NOT reboot the Checkpoint appliances until bootstrap is complete (~25 minutes for the manager), or you will be required to redeploy the instance Recover root passwords for all sub-accounts and apply strong passwords Process documented here Enable MFA for all IAM users and all root account users, recommendations: Yubikeys provide the strongest form of MFA protection and are strongly encouraged for all account root users and all IAM users in the Organization Management (root) account the Organization Management (root) account requires a dedicated Yubikey (if access is required to a sub-account root user, we do not want to expose the Organization Management accounts Yubikey) every ~50 sub-accounts requires a dedicated Yubikey (minimize the required number of Yubikeys and the scope of impact should a Yubikey be lost or compromised) each IAM breakglass user requires a dedicated Yubikey, as do any additional IAM users in the Organization Management (root) account. While some CSPs do not recommend MFA on the breakglass users, it is strongly encouraged in AWS all other AWS users (AWS SSO, IAM in sub-accounts, etc.) can leverage virtual MFA devices (like Google Authenticator on a mobile device) Customers are responsible for the ongoing management and rotation of all passwords on a regular basis per their organizational password policy. This includes the passwords of all IAM users, MAD users, firewall users, or other users, whether deployed by the Accelerator or not. We do NOT automatically rotate any passwords, but strongly encourage customers do so, on a regular basis. During the installation we request required limit increases, resources dependent on these limits will not be deployed Limit increase requests are controlled through the Accelerator configuration file \"limits\":{} setting The sample configuration file requests increases to your EIP count in the perimeter account and to the VPC count and Interface Endpoint count in the shared-network account You should receive emails from support confirming the limit increases On the next state machine execution, resources blocked by limits should be deployed (i.e. additional VPC's and Endpoints) If more than 2 days elapses without the limits being increased, on the next state machine execution, they will be re-requested Note: After a successful install the Control Tower Organizational units dashboard will indicate 2 of 3 in the Accounts enrolled column for the Security OU, as it does not enable enrollment of the management account in guardrails. The Accelerator compliments Control Tower and enables guardrails in the management account which is important to high compliance customers.","title":"Post-Installation"},{"location":"installation/multi-file-config-capabilities/","text":"AWS Secure Environment Accelerator \u00b6 Multi-file Accelerator Config file and YAML Support Details \u00b6 Customers would like the ability to specify their configuration in YAML. This facilitates: \u00b6 commenting out entire sections, which is unavailable in standard JSON annotating aspects of configuration (e.g. cidr: \"10.100.0.0/16\" # We chose this for \\$reason.) aligning the Accelerator with CloudFormation, which supports JSON/YAML as input format Customers would like the configuration file split into multiple files: \u00b6 one file for Global options + Mandatory accounts one file per OU one file for every approx. 2000 lines of workload accounts (Code Commit diff stops working at 3000 lines, allow for adding to each file) Benefits: \u00b6 Easier cut/paste/comparison of OU configurations Allow CodeCommit diff functionality to function (File currently too large) Allow easier updates to workload accounts (simple append) Smaller scoped updates (de-risk accidentally changing the wrong section) Both a customer request and something the team thought was a good idea Steps FOR YAML: \u00b6 The loadAcceleratorConfig functionality should no longer assume config.json as the config filename in the config repo and/or S3, instead it should look for config.yaml and config.json Check for the existence of config.yaml and config.json (initially in S3, but also in CodeCommit on future executions) If both files exist, fail with an error message Infer the file type from the extension, and parse accordingly Any other failure should also be an error, fail with an error message The accelerator will continue to use JSON formatting internally, if a yaml file is supplied, we are simply converting it to JSON for use by the Accelerator All examples throughout this document use config.json as the example, but also apply to YAML Both JSON and YAML input files will be equally supported Only one file format is supported across all config files, either JSON or YAML, customers can NOT mix YAML and JSON file formats Steps For File Split: \u00b6 When the __LOAD keyword is encountered, search relatively (from the same location as root config file) for the file, and insert into the config tree, recursively following __LOAD if necessary (to max depth of 2). Any file referenced in __LOAD must parse successfully in one of the two formats, otherwise FAIL. { \"core\": { \"__LOAD\": \"ous/core.json\" } } Note that while we will provide sensible examples, there is no prescriptive requirement for file organization within a customer's configuration, customers can use the feature to break-out sections as is most effective for their deployment. Breaking out large repeatable sections like security groups is a good example and could be included off the main file, an account file, or off an ou file: \"security-groups\": [ \"__LOAD\": \"global/security-groups.json\" ] Examples: All in one (single file like today): . \u251c\u2500\u2500 config.json Split along major sections: . \u251c\u2500\u2500 config.json \u251c\u2500\u2500 ous \u2502 \u251c\u2500\u2500 core.json \u2502 \u251c\u2500\u2500 dev.json \u2502 \u2514\u2500\u2500 test.json ---> could be one per ou, could only be for some ou's as determined by customer \u251c\u2500\u2500 accounts \u2502 \u251c\u2500\u2500 workload-accounts-group1.json \u2502 \u251c\u2500\u2500 workload-accounts-group2.json \u2502 \u251c\u2500\u2500 my-workload-accounts.json \u2502 \u2514\u2500\u2500 more-accounts.json ---->we will encourage each file being as close to 2000 lines as possible (not one per account, not all in one file) \u2514\u2500\u2500 global \u251c\u2500\u2500 global-options.json \u251c\u2500\u2500 security-groups.json etc Max depth of 2 means config.json can load ou/dev.json, which can load global/security-groups.json. security-groups.json CANNOT load another sub-file (unless security-groups.json was only directly loaded from config.json). Dealing with Accelerator Automatic Config File Updates: \u00b6 When customers create AWS accounts directly through AWS Organizations, the Accelerator automatically updates the config file, adding these new accounts. If a customer renames an OU we automatically update the config file. With multi-part files, how do we know what source file to update? We require two mechanisms: Add the following new parameters to the global-options section of the config file \"workloadaccounts-param-filename\": \"accounts/more-accounts2.json\", \"workloadaccounts-prefix\" : \"accounts/more-accounts\", \"workloadaccounts-suffix\" : 3, filename is set to config.json , and prefix to config in a single file configuration scenario (suffix is not used) While OU contents can be moved into __LOADED sub-files, it was decided the OU object itself must remain in the main config file The above parameters: are required to be in the main config file and cannot be __LOAD 'ed Must be present or SM fails Are used to decide where to add new accounts to the config file Add the following new parameter to each mandatory and workload account config \"src-filename\": \"accounts/my-workload-accounts.json\", Accelerator Internal Operations: when updating an account in the config file, we use the \"src-filename\" parameters to find and update an accounts ou , ou-path , account-name , and email parameters When creating new accounts (inserting into config file): if the update is not going to make the file larger than 2000 lines, insert the new account into the config file \"workloadaccounts-param-filename\" if the insert will push the file over 2000 lines: create the next unused filename for the given prefix in Code Commit ( {\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format} ), i.e. \"accounts/more-accounts3.json\" insert the new account into the new file in it's entirety update \"workloadaccounts-param-filename\" to: {\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format} add a new load stmt to the workload-accounts section of the config file with the name {\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format} update \"workloadaccounts-suffix\" to: {\"workloadaccounts-suffix\"} + 1 be careful with comma's between files (JSON sections) when appending/connecting Example \u00b6 The entire main config file could be reduced to this: {\"global-options\": { \"workloadaccounts-param-filename\": \"accounts/more-accounts2.json\", \"workloadaccounts-prefix\" : \"accounts/more-accounts\", \"workloadaccounts-suffix\" : 3, \"__LOAD\": \"global/global-options.json\" }, \"mandatory-account-configs\": { \"__LOAD\": \"accounts/mandatory-accounts.json\" }, \"workload-account-configs\": { \"__LOAD\": [\"accounts/workload-accounts1.json\", \"accounts/my-other-accounts.json\", \"accounts/workload-accounts2.json\"] }, \"organizational-units\": { \"core\": { \"__LOAD\": \"ous/core.json\" }, \"Central\": { \"__LOAD\": \"ous/central.json\" }, \"Dev\": { \"__LOAD\": \"ous/dev.json\" }, \"Test\": { \"__LOAD\": \"ous/test.json\" }, \"Prod\": { \"__LOAD\": \"ous/prod.json\" }, \"UnClass\": { \"__LOAD\": \"ous/unclass.json\" }, \"Sandbox\": { \"__LOAD\": \"ous/sandbox.json\" } }} Acceptance Criteria: \u00b6 A new customer may start an Accelerator deployment with a config.json or config.yaml, and have it deploy as expected so long as the file is semantically correct according to structure and expected keys (and of course syntactically correct in either YAML or JSON) Accelerator should continue to function as it does today o i.e. on startup creates repo and copies all referenced config files, not just config.json to repo (json or YAML) o leverages config files in CodeCommit repo from this point forward (json or YAML as provided by customer) o SM runs against the commit id of each file at the start of the SM (i.e. don't allow changes to any file during execution) Accelerator leverages multiple config files to receive the same input parameters it previously did from one file All accelerator functionality both ALZ and Standalone versions continue to function as previously defined Customer can successfully provides multiple config files with the same result as the current one file ...Return to Customization Table of Contents","title":"AWS Secure Environment Accelerator"},{"location":"installation/multi-file-config-capabilities/#aws-secure-environment-accelerator","text":"","title":"AWS Secure Environment Accelerator"},{"location":"installation/multi-file-config-capabilities/#multi-file-accelerator-config-file-and-yaml-support-details","text":"","title":"Multi-file Accelerator Config file and YAML Support Details"},{"location":"installation/multi-file-config-capabilities/#customers-would-like-the-ability-to-specify-their-configuration-in-yaml-this-facilitates","text":"commenting out entire sections, which is unavailable in standard JSON annotating aspects of configuration (e.g. cidr: \"10.100.0.0/16\" # We chose this for \\$reason.) aligning the Accelerator with CloudFormation, which supports JSON/YAML as input format","title":"Customers would like the ability to specify their configuration in YAML. This facilitates:"},{"location":"installation/multi-file-config-capabilities/#customers-would-like-the-configuration-file-split-into-multiple-files","text":"one file for Global options + Mandatory accounts one file per OU one file for every approx. 2000 lines of workload accounts (Code Commit diff stops working at 3000 lines, allow for adding to each file)","title":"Customers would like the configuration file split into multiple files:"},{"location":"installation/multi-file-config-capabilities/#benefits","text":"Easier cut/paste/comparison of OU configurations Allow CodeCommit diff functionality to function (File currently too large) Allow easier updates to workload accounts (simple append) Smaller scoped updates (de-risk accidentally changing the wrong section) Both a customer request and something the team thought was a good idea","title":"Benefits:"},{"location":"installation/multi-file-config-capabilities/#steps-for-yaml","text":"The loadAcceleratorConfig functionality should no longer assume config.json as the config filename in the config repo and/or S3, instead it should look for config.yaml and config.json Check for the existence of config.yaml and config.json (initially in S3, but also in CodeCommit on future executions) If both files exist, fail with an error message Infer the file type from the extension, and parse accordingly Any other failure should also be an error, fail with an error message The accelerator will continue to use JSON formatting internally, if a yaml file is supplied, we are simply converting it to JSON for use by the Accelerator All examples throughout this document use config.json as the example, but also apply to YAML Both JSON and YAML input files will be equally supported Only one file format is supported across all config files, either JSON or YAML, customers can NOT mix YAML and JSON file formats","title":"Steps FOR YAML:"},{"location":"installation/multi-file-config-capabilities/#steps-for-file-split","text":"When the __LOAD keyword is encountered, search relatively (from the same location as root config file) for the file, and insert into the config tree, recursively following __LOAD if necessary (to max depth of 2). Any file referenced in __LOAD must parse successfully in one of the two formats, otherwise FAIL. { \"core\": { \"__LOAD\": \"ous/core.json\" } } Note that while we will provide sensible examples, there is no prescriptive requirement for file organization within a customer's configuration, customers can use the feature to break-out sections as is most effective for their deployment. Breaking out large repeatable sections like security groups is a good example and could be included off the main file, an account file, or off an ou file: \"security-groups\": [ \"__LOAD\": \"global/security-groups.json\" ] Examples: All in one (single file like today): . \u251c\u2500\u2500 config.json Split along major sections: . \u251c\u2500\u2500 config.json \u251c\u2500\u2500 ous \u2502 \u251c\u2500\u2500 core.json \u2502 \u251c\u2500\u2500 dev.json \u2502 \u2514\u2500\u2500 test.json ---> could be one per ou, could only be for some ou's as determined by customer \u251c\u2500\u2500 accounts \u2502 \u251c\u2500\u2500 workload-accounts-group1.json \u2502 \u251c\u2500\u2500 workload-accounts-group2.json \u2502 \u251c\u2500\u2500 my-workload-accounts.json \u2502 \u2514\u2500\u2500 more-accounts.json ---->we will encourage each file being as close to 2000 lines as possible (not one per account, not all in one file) \u2514\u2500\u2500 global \u251c\u2500\u2500 global-options.json \u251c\u2500\u2500 security-groups.json etc Max depth of 2 means config.json can load ou/dev.json, which can load global/security-groups.json. security-groups.json CANNOT load another sub-file (unless security-groups.json was only directly loaded from config.json).","title":"Steps For File Split:"},{"location":"installation/multi-file-config-capabilities/#dealing-with-accelerator-automatic-config-file-updates","text":"When customers create AWS accounts directly through AWS Organizations, the Accelerator automatically updates the config file, adding these new accounts. If a customer renames an OU we automatically update the config file. With multi-part files, how do we know what source file to update? We require two mechanisms: Add the following new parameters to the global-options section of the config file \"workloadaccounts-param-filename\": \"accounts/more-accounts2.json\", \"workloadaccounts-prefix\" : \"accounts/more-accounts\", \"workloadaccounts-suffix\" : 3, filename is set to config.json , and prefix to config in a single file configuration scenario (suffix is not used) While OU contents can be moved into __LOADED sub-files, it was decided the OU object itself must remain in the main config file The above parameters: are required to be in the main config file and cannot be __LOAD 'ed Must be present or SM fails Are used to decide where to add new accounts to the config file Add the following new parameter to each mandatory and workload account config \"src-filename\": \"accounts/my-workload-accounts.json\", Accelerator Internal Operations: when updating an account in the config file, we use the \"src-filename\" parameters to find and update an accounts ou , ou-path , account-name , and email parameters When creating new accounts (inserting into config file): if the update is not going to make the file larger than 2000 lines, insert the new account into the config file \"workloadaccounts-param-filename\" if the insert will push the file over 2000 lines: create the next unused filename for the given prefix in Code Commit ( {\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format} ), i.e. \"accounts/more-accounts3.json\" insert the new account into the new file in it's entirety update \"workloadaccounts-param-filename\" to: {\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format} add a new load stmt to the workload-accounts section of the config file with the name {\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format} update \"workloadaccounts-suffix\" to: {\"workloadaccounts-suffix\"} + 1 be careful with comma's between files (JSON sections) when appending/connecting","title":"Dealing with Accelerator Automatic Config File Updates:"},{"location":"installation/multi-file-config-capabilities/#example","text":"The entire main config file could be reduced to this: {\"global-options\": { \"workloadaccounts-param-filename\": \"accounts/more-accounts2.json\", \"workloadaccounts-prefix\" : \"accounts/more-accounts\", \"workloadaccounts-suffix\" : 3, \"__LOAD\": \"global/global-options.json\" }, \"mandatory-account-configs\": { \"__LOAD\": \"accounts/mandatory-accounts.json\" }, \"workload-account-configs\": { \"__LOAD\": [\"accounts/workload-accounts1.json\", \"accounts/my-other-accounts.json\", \"accounts/workload-accounts2.json\"] }, \"organizational-units\": { \"core\": { \"__LOAD\": \"ous/core.json\" }, \"Central\": { \"__LOAD\": \"ous/central.json\" }, \"Dev\": { \"__LOAD\": \"ous/dev.json\" }, \"Test\": { \"__LOAD\": \"ous/test.json\" }, \"Prod\": { \"__LOAD\": \"ous/prod.json\" }, \"UnClass\": { \"__LOAD\": \"ous/unclass.json\" }, \"Sandbox\": { \"__LOAD\": \"ous/sandbox.json\" } }}","title":"Example"},{"location":"installation/multi-file-config-capabilities/#acceptance-criteria","text":"A new customer may start an Accelerator deployment with a config.json or config.yaml, and have it deploy as expected so long as the file is semantically correct according to structure and expected keys (and of course syntactically correct in either YAML or JSON) Accelerator should continue to function as it does today o i.e. on startup creates repo and copies all referenced config files, not just config.json to repo (json or YAML) o leverages config files in CodeCommit repo from this point forward (json or YAML as provided by customer) o SM runs against the commit id of each file at the start of the SM (i.e. don't allow changes to any file during execution) Accelerator leverages multiple config files to receive the same input parameters it previously did from one file All accelerator functionality both ALZ and Standalone versions continue to function as previously defined Customer can successfully provides multiple config files with the same result as the current one file ...Return to Customization Table of Contents","title":"Acceptance Criteria:"},{"location":"installation/object-naming/","text":"Accelerator Object Naming \u00b6 Resources will have the 'Name' tag assigned, where Name={name}{suffix} No prefix or suffix will be applied to DNS records/zones (as that breaks them) When _ is not supported, a - will be used Stacks/stacksets/functions and non-end user accessed objects deployed in all accounts will also start with the {AcceleratorPrefix} prefix (i.e. \" PBMMAccel- \" or \" ASEA- \") The prefix does not apply to objects like VPC's, subnets, or TGW's which customers need to directly access. This is for objects deployed to build the customer accessible objects This prefix will be protected by SCP's so customers don't break 'managed' features Resources will have the tag 'Accelerator={AcceleratorName}' assigned when tags are supported Stacks will have the tag 'AcceleratorName={AcceleratorName}' assigned, which will often (but not always) be inherited by objects created by the stack (due to TGW duplicate tag issue) Defaults \u00b6 - the default {AcceleratorName} is 'PBMM' before v1.5.0 and 'ASEA' after v1.5.0 - the default {AcceleratorPrefix} is 'PBMMAccel-' before v1.5.0 and 'ASEA-' after v1.5.0 Suffix's \u00b6 suffix object type _vpc VPC _azN_net Subnet _azN_rt RouteTable _tgw Transit Gateway -key KMS key _pcx Peering Connection _sg Security Group _nacl NACL _alb Application Load Balancer _nlb Network Load Balancer _agw Appliance Gateway _vpce VPC Endpoint _AMI AMI _dhcp DHCP option set _snap snapshot _ebs Block storage _igw internet gateway _lgw Local gateway _nat NAT gateway _vpg Virtual private gateway _cgw Customer gateway _vpn VPN Connection _sm Step Functions _rule CW Event Rule _pl CodeBuild No Suffix \u00b6 suffix object type None Stacks None CFN_Stack_Sets None Lambda None Cloud Trails None CWL Groups None Config Rules None OU None Service Control Policy","title":"Object naming"},{"location":"installation/object-naming/#accelerator-object-naming","text":"Resources will have the 'Name' tag assigned, where Name={name}{suffix} No prefix or suffix will be applied to DNS records/zones (as that breaks them) When _ is not supported, a - will be used Stacks/stacksets/functions and non-end user accessed objects deployed in all accounts will also start with the {AcceleratorPrefix} prefix (i.e. \" PBMMAccel- \" or \" ASEA- \") The prefix does not apply to objects like VPC's, subnets, or TGW's which customers need to directly access. This is for objects deployed to build the customer accessible objects This prefix will be protected by SCP's so customers don't break 'managed' features Resources will have the tag 'Accelerator={AcceleratorName}' assigned when tags are supported Stacks will have the tag 'AcceleratorName={AcceleratorName}' assigned, which will often (but not always) be inherited by objects created by the stack (due to TGW duplicate tag issue)","title":"Accelerator Object Naming"},{"location":"installation/object-naming/#defaults","text":"- the default {AcceleratorName} is 'PBMM' before v1.5.0 and 'ASEA' after v1.5.0 - the default {AcceleratorPrefix} is 'PBMMAccel-' before v1.5.0 and 'ASEA-' after v1.5.0","title":"Defaults"},{"location":"installation/object-naming/#suffixs","text":"suffix object type _vpc VPC _azN_net Subnet _azN_rt RouteTable _tgw Transit Gateway -key KMS key _pcx Peering Connection _sg Security Group _nacl NACL _alb Application Load Balancer _nlb Network Load Balancer _agw Appliance Gateway _vpce VPC Endpoint _AMI AMI _dhcp DHCP option set _snap snapshot _ebs Block storage _igw internet gateway _lgw Local gateway _nat NAT gateway _vpg Virtual private gateway _cgw Customer gateway _vpn VPN Connection _sm Step Functions _rule CW Event Rule _pl CodeBuild","title":"Suffix's"},{"location":"installation/object-naming/#no-suffix","text":"suffix object type None Stacks None CFN_Stack_Sets None Lambda None Cloud Trails None CWL Groups None Config Rules None OU None Service Control Policy","title":"No Suffix"},{"location":"installation/services-list/","text":"AWS Secure Environment Accelerator Service List \u00b6 Services are listed based as being (L)everaged by the Accelerator or (O)rchestrated by the Accelerator SERVICES Compute - AWS Lambda (L) - Amazon EC2 (O) Monitoring & Alerts - Amazon CloudTrail (O) - AWS Config (O) - Amazon CloudWatch (L,O) - Amazon EventBridge (L,O) - Amazon SNS (L) - AWS Budgets (O) - Systems Manager Inventory (O) Infrastructure - AWS CodeCommit (L) - AWS CodeBuild (L) - AWS CodePipeline (L) - AWS CloudFormation (L) - AWS CDK, AWS SDK (L) - AWS Step Functions (L) - Kinesis Data Stream (L) - Kinesis Data Firehose (L) - Amazon SQS (L) Data - Amazon S3 (L,O) - Amazon DynamoDB (L) - Amazon ECR, ECR Public (L) - Systems Manager Parameter Store (L,O) - AWS Secrets Manager (L) Networking - Amazon VPC (O) - AWS Transit Gateway (O) - AWS PrivateLink (O) - Elastic Load Balancing (incl. ALB, NLB, GWLB) (O) - Route53 (O) - Route53 Resolver (O) Management - AWS Organizations (L,O) - AWS Resource Access Manager (RAM) (O) - AWS IAM (L,O) - AWS SSO (L) - AWS Directory Service (incl. AWS Managed AD and AD Connector) (O) - AWS Control Tower (L,O) - AWS IAM Access Analyzer (O) - AWS Cost and Usage Reports (O) - AWS Service Quotas (O) Security - AWS GuardDuty (O) - AWS Security Hub (O) - Amazon Macie (O) - Systems Manager Automation (O) - Systems Manager Session Manager (O) - AWS KMS (L,O) - AWS STS (L) - AWS Firewall Manager (O) - AWS Network Firewall (O) - AWS Certificate Manager (O) 3rd Party - Fortinet FortiGate and FortiManager (Firewall & Mgmt) (O) - Checkpoint CloudGuard and Manager (Firewall & Mgmt) (O) - rsyslog on Amazon Linux 2 (O) - RDGW Bastion on Windows (O) If we missed a service, let us know!","title":"Deployed Services List"},{"location":"installation/services-list/#aws-secure-environment-accelerator-service-list","text":"Services are listed based as being (L)everaged by the Accelerator or (O)rchestrated by the Accelerator SERVICES Compute - AWS Lambda (L) - Amazon EC2 (O) Monitoring & Alerts - Amazon CloudTrail (O) - AWS Config (O) - Amazon CloudWatch (L,O) - Amazon EventBridge (L,O) - Amazon SNS (L) - AWS Budgets (O) - Systems Manager Inventory (O) Infrastructure - AWS CodeCommit (L) - AWS CodeBuild (L) - AWS CodePipeline (L) - AWS CloudFormation (L) - AWS CDK, AWS SDK (L) - AWS Step Functions (L) - Kinesis Data Stream (L) - Kinesis Data Firehose (L) - Amazon SQS (L) Data - Amazon S3 (L,O) - Amazon DynamoDB (L) - Amazon ECR, ECR Public (L) - Systems Manager Parameter Store (L,O) - AWS Secrets Manager (L) Networking - Amazon VPC (O) - AWS Transit Gateway (O) - AWS PrivateLink (O) - Elastic Load Balancing (incl. ALB, NLB, GWLB) (O) - Route53 (O) - Route53 Resolver (O) Management - AWS Organizations (L,O) - AWS Resource Access Manager (RAM) (O) - AWS IAM (L,O) - AWS SSO (L) - AWS Directory Service (incl. AWS Managed AD and AD Connector) (O) - AWS Control Tower (L,O) - AWS IAM Access Analyzer (O) - AWS Cost and Usage Reports (O) - AWS Service Quotas (O) Security - AWS GuardDuty (O) - AWS Security Hub (O) - Amazon Macie (O) - Systems Manager Automation (O) - Systems Manager Session Manager (O) - AWS KMS (L,O) - AWS STS (L) - AWS Firewall Manager (O) - AWS Network Firewall (O) - AWS Certificate Manager (O) 3rd Party - Fortinet FortiGate and FortiManager (Firewall & Mgmt) (O) - Checkpoint CloudGuard and Manager (Firewall & Mgmt) (O) - rsyslog on Amazon Linux 2 (O) - RDGW Bastion on Windows (O) If we missed a service, let us know!","title":"AWS Secure Environment Accelerator Service List"},{"location":"installation/sm_inputs/","text":"1. State Machine Behavior and Inputs \u00b6 1. State Machine Behavior and Inputs 1.1. State Machine Behavior 2. Accelerator State Machine Inputs 2.1. Rebuild DynamoDB table contents 2.2. Bypass ALL config file validation checks 2.3. Bypassing SPECIFIC config file validation checks 2.4. Generate verbose logging within state machine 2.5. State Machine scoping inputs 2.6. Example of combined inputs 1.1. State Machine Behavior \u00b6 Accelerator v1.3.0 makes a significant change to the manner in which the state machine operates. These changes include: Reducing the default scope of execution of the state machine to only target newly created AWS accounts and AWS accounts listed in the mandatory accounts section of the config file. default scope refers to running the state machine without any input parameters; This new default scope disallows any changes to the config file outside new accounts; NOTE: it is critical that accounts for which others are dependant upon, MUST be located within the mandatory-account-configs section of the config file (i.e. management, log-archive, security, operations, shared-network, perimeter, etc.). The state machine now accepts a new input parameter, scope , which accepts the following values: FULL | NEW-ACCOUNTS | GLOBAL-OPTIONS | ACCOUNT | OU . when the scope parameter is supplied, you must also supply the mode parameter. At this time mode only accepts the value APPLY . To be specific \"mode\":\"APPLY\" is mandatory when running the state machine with the \"scope\": parameter. Starting the state machine with {\"scope\":\"FULL\",\"mode\":\"APPLY\"} makes the state machine execute as it did in v1.2.6 and below. The state machine targets all AWS accounts and allows changes across any section of the config file; The blocks and overrides described in section 1.4 above remain valid; FULL mode must be run at least once immediately after any Accelerator version upgrade. Code Pipeline automatically starts the state machine with {\"scope\":\"FULL\",\"mode\":\"APPLY\"} . If the state machine fails for any reason after upgrade, the state machine must be restarted with these parameters until a successful execution of the state machine has completed. Starting the state machine with {\"scope\":\"NEW-ACCOUNTS\",\"mode\":\"APPLY\"} is the same as operating the state machine with the default scope as described in the first bullet Starting the state machine with {\"scope\":\"GLOBAL-OPTIONS\",\"mode\":\"APPLY\"} restricts changes to the config file to the global-options section. If any other portion of the config file was updated or changed, the state machine will fail; The global options scope executes the state machine on the entire managed account footprint. Starting the state machine with {\"scope\":\"OU\",\"targetOus\":[X],\"mode\":\"APPLY\"} restricts changes to the config file to the specified organizational-units section(s) defined by targetOus . When scope=OU , targetOus becomes a mandatory parameter; X can be any one or more valid OU names, or the value \"ALL\" ; When [\"ALL\"] is specified, the state machine targets all AWS accounts, but only allows changes to the organizational-units section of the config file; When OUs are specified (i.e. [\"Dev\",\"Test\"] ), the state machine only targets mandatory accounts plus accounts in the specified OUs (Dev, Test), and only allows changes to the specified OUs sections (Dev, Test) of the config file; If any other portion of the config file was updated or changed, the state machine will fail. Starting the state machine with {\"scope\":\"ACCOUNT\",\"targetAccounts\":[X],\"mode\":\"APPLY\"} restricts changes to the config file to the specified xxx-account-configs section(s) defined by targetAccounts . When scope=ACCOUNT , targetAccounts becomes a mandatory parameter; X can be any one or more valid account numbers, the value \"NEW\" , or the value \"ALL\" ; When [\"ALL\"] is specified, the state machine targets all AWS accounts, but only allows changes to the xxx-account-configs sections of the config file; When specific accounts and/or NEW is specified (i.e. [\"NEW\", \"123456789012\", \"234567890123\"] ), the state machine only targets mandatory accounts plus the listed accounts and any newly created accounts. It also only allows changes to the specified accounts sections (New, 123456789012, 234567890123) of the config file; If any other portion of the config file was updated or changed, the state machine will fail. Starting in v1.3.0, we recommend running the state machine with the parameters that most tightly scope the state machines execution to your planned changes and minimizing the use of FULL scope execution. should you accidentally change the wrong section of the config file, you will be protected; as you grow and scale to hundreds or thousands of accounts, your state machine execution time will remain fast. NOTE 1: The scope setting has no impact on SCP application, limit requests, custom tagging, or directory sharing. NOTE 2: All comparisons for config file changes are assessed AFTER all replacements have been made. Changing variable names which result in the same end outcome do NOT appear as a change to the config file. 2. Accelerator State Machine Inputs \u00b6 2.1. Rebuild DynamoDB table contents \u00b6 With the exception of the Outputs table, the contents of the Accelerator DynamoDB tables are rebuilt on every state machine execution. We recently started depending on the Outputs DynamoDB tables to ensure the parameters in parameter store are consistently maintained in the same order as objects are created and deleted. Should the CONTENTS of the tables be destroyed or corrupted, customers can force a rebuild of the CloudFormation Outputs in DynamoDB by starting the state machine with the parameter: { \"storeAllOutputs\": true } This should be completed BEFORE running the state machine with a corrupt or empty DynamoDB table or the Accelerator is likely to reorder a customers parameters. If the DynamoDB tables were completely destroyed, they must be recreated before running the state machine with this parameter. 2.2. Bypass ALL config file validation checks \u00b6 This parameter should be specified with extreme caution, as it bypasses all config file validation. The state machine typically has protections enabled preventing customers from making breaking changes to the config file. Under certain conditions with the support of a trained expert, bypassing these checks is required. Start the state machine with the parameter: { \"overrideComparison\": true } Customers are encouraged to use the specific override variables below, rather than the all-inclusive override, to ensure they only bypasses intended config changes. 2.3. Bypassing SPECIFIC config file validation checks \u00b6 Providing any one or more of the following flags will only override the specified check(s): { \"configOverrides\": { 'ov-global-options': true, 'ov-del-accts': true, 'ov-ren-accts': true, 'ov-acct-email': true, 'ov-acct-ou': true, 'ov-acct-vpc': true, 'ov-acct-subnet': true, 'ov-acct-vpc-optin': true, 'ov-tgw': true, 'ov-mad': true, 'ov-ou-vpc': true, 'ov-ou-subnet': true, 'ov-share-to-ou': true, 'ov-share-to-accounts': true, 'ov-nacl': true, 'ov-nfw': true } } 2.4. Generate verbose logging within state machine \u00b6 Added \"verbose\": \"1\" state machine input options parameter is optional parameter defaults to 0 {\"scope\":\"FULL\", \"mode\":\"APPLY\", \"verbose\":\"1\"} 2.5. State Machine scoping inputs \u00b6 Summary of inputs, per section 1.1 above: {\"scope\":\"FULL\", \"mode\":\"APPLY\"} {\"scope\":\"NEW-ACCOUNTS\", \"mode\":\"APPLY\"} {\"scope\":\"GLOBAL-OPTIONS\", \"mode\":\"APPLY\"} {\"scope\":\"OU\", \"targetOus\":[\"ou-name\", \"ou-name\"], \"mode\":\"APPLY\"} {\"scope\":\"ACCOUNT\", \"targetAccounts\":[\"123456789012\",\"234567890123\"], \"mode\":\"APPLY\"} 2.6. Example of combined inputs \u00b6 {\"scope\": \"FULL\", \"mode\": \"APPLY\", \"configOverrides\": {\"ov-ou-vpc\": true, \"ov-ou-subnet\": true, \"ov-acct-vpc\": true }} ...Return to Accelerator Table of Contents","title":"1. **State Machine Behavior and Inputs**"},{"location":"installation/sm_inputs/#1-state-machine-behavior-and-inputs","text":"1. State Machine Behavior and Inputs 1.1. State Machine Behavior 2. Accelerator State Machine Inputs 2.1. Rebuild DynamoDB table contents 2.2. Bypass ALL config file validation checks 2.3. Bypassing SPECIFIC config file validation checks 2.4. Generate verbose logging within state machine 2.5. State Machine scoping inputs 2.6. Example of combined inputs","title":"1. State Machine Behavior and Inputs"},{"location":"installation/sm_inputs/#11-state-machine-behavior","text":"Accelerator v1.3.0 makes a significant change to the manner in which the state machine operates. These changes include: Reducing the default scope of execution of the state machine to only target newly created AWS accounts and AWS accounts listed in the mandatory accounts section of the config file. default scope refers to running the state machine without any input parameters; This new default scope disallows any changes to the config file outside new accounts; NOTE: it is critical that accounts for which others are dependant upon, MUST be located within the mandatory-account-configs section of the config file (i.e. management, log-archive, security, operations, shared-network, perimeter, etc.). The state machine now accepts a new input parameter, scope , which accepts the following values: FULL | NEW-ACCOUNTS | GLOBAL-OPTIONS | ACCOUNT | OU . when the scope parameter is supplied, you must also supply the mode parameter. At this time mode only accepts the value APPLY . To be specific \"mode\":\"APPLY\" is mandatory when running the state machine with the \"scope\": parameter. Starting the state machine with {\"scope\":\"FULL\",\"mode\":\"APPLY\"} makes the state machine execute as it did in v1.2.6 and below. The state machine targets all AWS accounts and allows changes across any section of the config file; The blocks and overrides described in section 1.4 above remain valid; FULL mode must be run at least once immediately after any Accelerator version upgrade. Code Pipeline automatically starts the state machine with {\"scope\":\"FULL\",\"mode\":\"APPLY\"} . If the state machine fails for any reason after upgrade, the state machine must be restarted with these parameters until a successful execution of the state machine has completed. Starting the state machine with {\"scope\":\"NEW-ACCOUNTS\",\"mode\":\"APPLY\"} is the same as operating the state machine with the default scope as described in the first bullet Starting the state machine with {\"scope\":\"GLOBAL-OPTIONS\",\"mode\":\"APPLY\"} restricts changes to the config file to the global-options section. If any other portion of the config file was updated or changed, the state machine will fail; The global options scope executes the state machine on the entire managed account footprint. Starting the state machine with {\"scope\":\"OU\",\"targetOus\":[X],\"mode\":\"APPLY\"} restricts changes to the config file to the specified organizational-units section(s) defined by targetOus . When scope=OU , targetOus becomes a mandatory parameter; X can be any one or more valid OU names, or the value \"ALL\" ; When [\"ALL\"] is specified, the state machine targets all AWS accounts, but only allows changes to the organizational-units section of the config file; When OUs are specified (i.e. [\"Dev\",\"Test\"] ), the state machine only targets mandatory accounts plus accounts in the specified OUs (Dev, Test), and only allows changes to the specified OUs sections (Dev, Test) of the config file; If any other portion of the config file was updated or changed, the state machine will fail. Starting the state machine with {\"scope\":\"ACCOUNT\",\"targetAccounts\":[X],\"mode\":\"APPLY\"} restricts changes to the config file to the specified xxx-account-configs section(s) defined by targetAccounts . When scope=ACCOUNT , targetAccounts becomes a mandatory parameter; X can be any one or more valid account numbers, the value \"NEW\" , or the value \"ALL\" ; When [\"ALL\"] is specified, the state machine targets all AWS accounts, but only allows changes to the xxx-account-configs sections of the config file; When specific accounts and/or NEW is specified (i.e. [\"NEW\", \"123456789012\", \"234567890123\"] ), the state machine only targets mandatory accounts plus the listed accounts and any newly created accounts. It also only allows changes to the specified accounts sections (New, 123456789012, 234567890123) of the config file; If any other portion of the config file was updated or changed, the state machine will fail. Starting in v1.3.0, we recommend running the state machine with the parameters that most tightly scope the state machines execution to your planned changes and minimizing the use of FULL scope execution. should you accidentally change the wrong section of the config file, you will be protected; as you grow and scale to hundreds or thousands of accounts, your state machine execution time will remain fast. NOTE 1: The scope setting has no impact on SCP application, limit requests, custom tagging, or directory sharing. NOTE 2: All comparisons for config file changes are assessed AFTER all replacements have been made. Changing variable names which result in the same end outcome do NOT appear as a change to the config file.","title":"1.1. State Machine Behavior"},{"location":"installation/sm_inputs/#2-accelerator-state-machine-inputs","text":"","title":"2. Accelerator State Machine Inputs"},{"location":"installation/sm_inputs/#21-rebuild-dynamodb-table-contents","text":"With the exception of the Outputs table, the contents of the Accelerator DynamoDB tables are rebuilt on every state machine execution. We recently started depending on the Outputs DynamoDB tables to ensure the parameters in parameter store are consistently maintained in the same order as objects are created and deleted. Should the CONTENTS of the tables be destroyed or corrupted, customers can force a rebuild of the CloudFormation Outputs in DynamoDB by starting the state machine with the parameter: { \"storeAllOutputs\": true } This should be completed BEFORE running the state machine with a corrupt or empty DynamoDB table or the Accelerator is likely to reorder a customers parameters. If the DynamoDB tables were completely destroyed, they must be recreated before running the state machine with this parameter.","title":"2.1. Rebuild DynamoDB table contents"},{"location":"installation/sm_inputs/#22-bypass-all-config-file-validation-checks","text":"This parameter should be specified with extreme caution, as it bypasses all config file validation. The state machine typically has protections enabled preventing customers from making breaking changes to the config file. Under certain conditions with the support of a trained expert, bypassing these checks is required. Start the state machine with the parameter: { \"overrideComparison\": true } Customers are encouraged to use the specific override variables below, rather than the all-inclusive override, to ensure they only bypasses intended config changes.","title":"2.2. Bypass ALL config file validation checks"},{"location":"installation/sm_inputs/#23-bypassing-specific-config-file-validation-checks","text":"Providing any one or more of the following flags will only override the specified check(s): { \"configOverrides\": { 'ov-global-options': true, 'ov-del-accts': true, 'ov-ren-accts': true, 'ov-acct-email': true, 'ov-acct-ou': true, 'ov-acct-vpc': true, 'ov-acct-subnet': true, 'ov-acct-vpc-optin': true, 'ov-tgw': true, 'ov-mad': true, 'ov-ou-vpc': true, 'ov-ou-subnet': true, 'ov-share-to-ou': true, 'ov-share-to-accounts': true, 'ov-nacl': true, 'ov-nfw': true } }","title":"2.3. Bypassing SPECIFIC config file validation checks"},{"location":"installation/sm_inputs/#24-generate-verbose-logging-within-state-machine","text":"Added \"verbose\": \"1\" state machine input options parameter is optional parameter defaults to 0 {\"scope\":\"FULL\", \"mode\":\"APPLY\", \"verbose\":\"1\"}","title":"2.4. Generate verbose logging within state machine"},{"location":"installation/sm_inputs/#25-state-machine-scoping-inputs","text":"Summary of inputs, per section 1.1 above: {\"scope\":\"FULL\", \"mode\":\"APPLY\"} {\"scope\":\"NEW-ACCOUNTS\", \"mode\":\"APPLY\"} {\"scope\":\"GLOBAL-OPTIONS\", \"mode\":\"APPLY\"} {\"scope\":\"OU\", \"targetOus\":[\"ou-name\", \"ou-name\"], \"mode\":\"APPLY\"} {\"scope\":\"ACCOUNT\", \"targetAccounts\":[\"123456789012\",\"234567890123\"], \"mode\":\"APPLY\"}","title":"2.5. State Machine scoping inputs"},{"location":"installation/sm_inputs/#26-example-of-combined-inputs","text":"{\"scope\": \"FULL\", \"mode\": \"APPLY\", \"configOverrides\": {\"ov-ou-vpc\": true, \"ov-ou-subnet\": true, \"ov-acct-vpc\": true }} ...Return to Accelerator Table of Contents","title":"2.6. Example of combined inputs"},{"location":"installation/upgrades/","text":"Upgrades \u00b6 Considerations \u00b6 Due to some breaking dependency issues, customers can only upgrade to v1.3.8 or above (older releases continue to function, but cannot be installed). While an upgrade path is planned, customers with a standalone Accelerator installation can upgrade to v1.5.0 but need to continue with a standalone installation until the Control Tower upgrade option becomes available. Always compare your configuration file with the config file from the release you are upgrading to in order to validate new or changed parameters or changes in parameter types / formats. do NOT update to the latest firewall AMI - see the the last bullet in section 5.1. Accelerator Design Constraints / Decisions do NOT update the organization-admin-role - see bullet 2 in section 2.2.7. Other do NOT update account-keys (i.e. existing installations cannot change the internal values to management from master ) do NOT make changes outside those required for the upgrade (those stated in the release notes or found through the comparison with the sample config file(s)). Customers wishing to change existing Accelerator configuration should either do so before their upgrade, ensuring a clean/successful state machine execution, or after a successful upgrade. The Accelerator name and prefix CANNOT be changed after the initial installation Customers which customized any of the Accelerator provided default configuration files (SCPs, rsyslog config, ssm-documents, iam-policies, etc.) must manually merge the latest Accelerator provided updates with deployed customizations: it is important customers assess the new defaults and integrate them into their custom configuration, or Accelerator functionality could break or Accelerator deployed features may be unprotected from modification if customers don't take action, we continue to utilize the deployed customized files (without the latest updates) The below release specific considerations need to be cumulatively applied (an upgrade from v1.2.3 to v1.2.5 requires you to follow both v1.2.4 and v1.2.5 considerations) Release Specific Upgrade Considerations: Upgrades to v1.5.0 : Due to the size and complexity of this upgrade, we require all customers to upgrade to v1.3.8 or above before beginning this upgrade While v1.5.0 supports Control Tower for NEW installs, existing Accelerator customers CANNOT add Control Tower to their existing installations at this time (planned enhancement for 22H1) Attempts to install Control Tower on top of the Accelerator will corrupt your environment (both Control Tower and the Accelerator need minor enhancements to enable) The v1.5.0 custom upgrade guide can be found here Upgrades to v1.3.9 and above from v1.3.8-b and below : All interface endpoints containing a period must be removed from the config.json file either before or during the upgrade process i.e. ecr.dkr, ecr.api, transfer.server, sagemaker.api, sagemaker.runtime in the full config.json example If you remove them on a pre-upgrade State Machine execution, you can put them back during the upgrade, if you remove them during the upgrade, you can put them back post upgrade. Upgrades to v1.3.3 and above from v1.3.2 and below : Requires mandatory config file schema changes as documented in the release notes . These updates cause the config file change validation to fail and require running the state machine with the following input to override the validation checks on impacted fields: {\"scope\": \"FULL\", \"mode\": \"APPLY\", \"configOverrides\": {\"ov-ou-vpc\": true, \"ov-ou-subnet\": true, \"ov-acct-vpc\": true }} Tightens VPC interface endpoint security group permissions and enables customization. If you use VPC interface endpoints that requires ports/protocols other than TCP/443 (such as email-smtp), you must customize your config file as described here Upgrades to v1.3.0 and above from v1.2.6 and below : Please note MAJOR changes to state machine behavior, as documented here . Upgrades to v1.2.6 and above from v1.2.5 and below - Ensure you apply the config file changes described in the release notes: Cut-paste the new \"replacements\": {}, section at the top of the example config file into your config file, as-is Enables customers to leverage the repo provided SCP's without customization, simplifying upgrades, while allowing SCP region customization the cloud-cidrX/cloud-maskX variables are examples of customer provided values that can be used to consistently auto-replace values throughout config files, these 4 specific variables are all required for the firewalls to successfully deploy The new ${variable} are auto-replaced across your config files, SCP's and firewall config files. as the variables should resolve to their existing values, you can leave your config file using hardcoded region and Accelerator prefix naming, or you can update them to make subsequent file comparisons easier for future upgrades. These are most useful for new installations in non ca-central-1 regions Some repo provide filenames have changed, where they are referenced within the config file, you must update them to their new filenames We do not delete/cleanup old/unused SCP's, in case they were also used by customers for unmanaged OUs or sub-ou's. After the upgrade, you should manually delete any old/extra SCP's which are no longer required Upgrades to v1.2.5 and above from v1.2.4 and below requires the manual removal of the PBMMAccel-PipelineRole StackSet before beginning your upgrade (we have eliminated all use of StackSets in this release) In the root AWS account, go to: CloudFormation, StackSets Find: ASEA-PipelineRole , and Select the: Stack Instances tab Document all the account numbers, comma separated i.e. 123456789012, 234567890123, 345678901234 Select: Actions, Delete Stacks from StackSets Paste the above account numbers (comma separated) in the Account numbers box Select the Accelerator installation/home region from the Specify Regions Box (should be the only region in the dropdown) Change: Concurrency to: 8, Next, Submit Wait for operation to complete (refresh the browser several times) Select Actions, Delete StackSet, click Delete StackSet Wait for the operation to complete Upgrades to v1.2.4 and above from v1.2.3 and below - Ensure you apply the config file changes described in the release notes: failure to set \"central-endpoint\": true directly on the endpoint VPC (instead of in global-options), will result in the removal of your VPC endpoints failure to move your zone definitions to the endpoint VPC, will result in the removal of you Public and Private hosted zones Summary of Upgrade Steps (all versions) \u00b6 Login to your Organization Management (root) AWS account with administrative privileges Either: a) Ensure a valid Github token is stored in secrets manager (section 2.3.2) b) Ensure the latest release is in a valid branch of CodeCommit in the Organization Management account Review and implement any relevant tasks noted in the upgrade considerations in section 3.1 Update the config file in CodeCommit with new parameters and updated parameter types based on the version you are upgrading to (this is important as features are iterating rapidly) An automated script is available to help convert config files to the new v1.5.0 format Compare your running config file with the sample config file from the latest release Review the Config file changes section of the release notes for all Accelerator versions since your current deployed release If you customized any of the other Accelerator default config files by overriding them in your S3 input bucket, merge the latest defaults with your customizations before beginning your upgrade Download the latest installer template ( AcceleratorInstallerXYZ.template.json or AcceleratorInstallerXXX-CodeCommit.template.json ) from the Assets section of the latest release Do NOT accidentally select the ASEA-InitialSetup CloudFormation stack below If you are replacing your GitHub Token: Take note of the AcceleratorName , AcceleratorPrefix , ConfigS3Bucket and NotificationEmail values from the Parameters tab of your deployed Installer CloudFormation stack ( ASEA-what-you-provided ) Delete the Installer CloudFormation stack ( ASEA-what-you-provided ) Redeploy the Installer CloudFormation stack using the template downloaded in step 6, providing the values you just documented (changes to AcceleratorName or AcceleratorPrefix are not supported) The pipeline will automatically run and trigger the upgraded state machine If you are using a pre-existing GitHub token, or installing from CodeCommit: Update the Installer CloudFormation stack using the template downloaded in step 5, updating the GithubBranch to the latest release (eg. release/v1.5.0 ) Go to AWS CloudFormation and select the stack: ASEA-what-you-provided Select Update, select Replace current template, Select Upload a template file Select Choose File and select the template you downloaded in step 6 ( AcceleratorInstallerXYZ.template.json or AcceleratorInstallerXXX-CodeCommit.template.json ) Select Next, Update GithubBranch parameter to release/vX.Y.Z where X.Y.Z represents the latest release Click Next, Next, I acknowledge, Update Wait for the CloudFormation stack to update ( Update_Complete status) (Requires manual refresh) Go To Code Pipeline and Release the ASEA-InstallerPipeline","title":"Upgrades"},{"location":"installation/upgrades/#upgrades","text":"","title":"Upgrades"},{"location":"installation/upgrades/#considerations","text":"Due to some breaking dependency issues, customers can only upgrade to v1.3.8 or above (older releases continue to function, but cannot be installed). While an upgrade path is planned, customers with a standalone Accelerator installation can upgrade to v1.5.0 but need to continue with a standalone installation until the Control Tower upgrade option becomes available. Always compare your configuration file with the config file from the release you are upgrading to in order to validate new or changed parameters or changes in parameter types / formats. do NOT update to the latest firewall AMI - see the the last bullet in section 5.1. Accelerator Design Constraints / Decisions do NOT update the organization-admin-role - see bullet 2 in section 2.2.7. Other do NOT update account-keys (i.e. existing installations cannot change the internal values to management from master ) do NOT make changes outside those required for the upgrade (those stated in the release notes or found through the comparison with the sample config file(s)). Customers wishing to change existing Accelerator configuration should either do so before their upgrade, ensuring a clean/successful state machine execution, or after a successful upgrade. The Accelerator name and prefix CANNOT be changed after the initial installation Customers which customized any of the Accelerator provided default configuration files (SCPs, rsyslog config, ssm-documents, iam-policies, etc.) must manually merge the latest Accelerator provided updates with deployed customizations: it is important customers assess the new defaults and integrate them into their custom configuration, or Accelerator functionality could break or Accelerator deployed features may be unprotected from modification if customers don't take action, we continue to utilize the deployed customized files (without the latest updates) The below release specific considerations need to be cumulatively applied (an upgrade from v1.2.3 to v1.2.5 requires you to follow both v1.2.4 and v1.2.5 considerations) Release Specific Upgrade Considerations: Upgrades to v1.5.0 : Due to the size and complexity of this upgrade, we require all customers to upgrade to v1.3.8 or above before beginning this upgrade While v1.5.0 supports Control Tower for NEW installs, existing Accelerator customers CANNOT add Control Tower to their existing installations at this time (planned enhancement for 22H1) Attempts to install Control Tower on top of the Accelerator will corrupt your environment (both Control Tower and the Accelerator need minor enhancements to enable) The v1.5.0 custom upgrade guide can be found here Upgrades to v1.3.9 and above from v1.3.8-b and below : All interface endpoints containing a period must be removed from the config.json file either before or during the upgrade process i.e. ecr.dkr, ecr.api, transfer.server, sagemaker.api, sagemaker.runtime in the full config.json example If you remove them on a pre-upgrade State Machine execution, you can put them back during the upgrade, if you remove them during the upgrade, you can put them back post upgrade. Upgrades to v1.3.3 and above from v1.3.2 and below : Requires mandatory config file schema changes as documented in the release notes . These updates cause the config file change validation to fail and require running the state machine with the following input to override the validation checks on impacted fields: {\"scope\": \"FULL\", \"mode\": \"APPLY\", \"configOverrides\": {\"ov-ou-vpc\": true, \"ov-ou-subnet\": true, \"ov-acct-vpc\": true }} Tightens VPC interface endpoint security group permissions and enables customization. If you use VPC interface endpoints that requires ports/protocols other than TCP/443 (such as email-smtp), you must customize your config file as described here Upgrades to v1.3.0 and above from v1.2.6 and below : Please note MAJOR changes to state machine behavior, as documented here . Upgrades to v1.2.6 and above from v1.2.5 and below - Ensure you apply the config file changes described in the release notes: Cut-paste the new \"replacements\": {}, section at the top of the example config file into your config file, as-is Enables customers to leverage the repo provided SCP's without customization, simplifying upgrades, while allowing SCP region customization the cloud-cidrX/cloud-maskX variables are examples of customer provided values that can be used to consistently auto-replace values throughout config files, these 4 specific variables are all required for the firewalls to successfully deploy The new ${variable} are auto-replaced across your config files, SCP's and firewall config files. as the variables should resolve to their existing values, you can leave your config file using hardcoded region and Accelerator prefix naming, or you can update them to make subsequent file comparisons easier for future upgrades. These are most useful for new installations in non ca-central-1 regions Some repo provide filenames have changed, where they are referenced within the config file, you must update them to their new filenames We do not delete/cleanup old/unused SCP's, in case they were also used by customers for unmanaged OUs or sub-ou's. After the upgrade, you should manually delete any old/extra SCP's which are no longer required Upgrades to v1.2.5 and above from v1.2.4 and below requires the manual removal of the PBMMAccel-PipelineRole StackSet before beginning your upgrade (we have eliminated all use of StackSets in this release) In the root AWS account, go to: CloudFormation, StackSets Find: ASEA-PipelineRole , and Select the: Stack Instances tab Document all the account numbers, comma separated i.e. 123456789012, 234567890123, 345678901234 Select: Actions, Delete Stacks from StackSets Paste the above account numbers (comma separated) in the Account numbers box Select the Accelerator installation/home region from the Specify Regions Box (should be the only region in the dropdown) Change: Concurrency to: 8, Next, Submit Wait for operation to complete (refresh the browser several times) Select Actions, Delete StackSet, click Delete StackSet Wait for the operation to complete Upgrades to v1.2.4 and above from v1.2.3 and below - Ensure you apply the config file changes described in the release notes: failure to set \"central-endpoint\": true directly on the endpoint VPC (instead of in global-options), will result in the removal of your VPC endpoints failure to move your zone definitions to the endpoint VPC, will result in the removal of you Public and Private hosted zones","title":"Considerations"},{"location":"installation/upgrades/#summary-of-upgrade-steps-all-versions","text":"Login to your Organization Management (root) AWS account with administrative privileges Either: a) Ensure a valid Github token is stored in secrets manager (section 2.3.2) b) Ensure the latest release is in a valid branch of CodeCommit in the Organization Management account Review and implement any relevant tasks noted in the upgrade considerations in section 3.1 Update the config file in CodeCommit with new parameters and updated parameter types based on the version you are upgrading to (this is important as features are iterating rapidly) An automated script is available to help convert config files to the new v1.5.0 format Compare your running config file with the sample config file from the latest release Review the Config file changes section of the release notes for all Accelerator versions since your current deployed release If you customized any of the other Accelerator default config files by overriding them in your S3 input bucket, merge the latest defaults with your customizations before beginning your upgrade Download the latest installer template ( AcceleratorInstallerXYZ.template.json or AcceleratorInstallerXXX-CodeCommit.template.json ) from the Assets section of the latest release Do NOT accidentally select the ASEA-InitialSetup CloudFormation stack below If you are replacing your GitHub Token: Take note of the AcceleratorName , AcceleratorPrefix , ConfigS3Bucket and NotificationEmail values from the Parameters tab of your deployed Installer CloudFormation stack ( ASEA-what-you-provided ) Delete the Installer CloudFormation stack ( ASEA-what-you-provided ) Redeploy the Installer CloudFormation stack using the template downloaded in step 6, providing the values you just documented (changes to AcceleratorName or AcceleratorPrefix are not supported) The pipeline will automatically run and trigger the upgraded state machine If you are using a pre-existing GitHub token, or installing from CodeCommit: Update the Installer CloudFormation stack using the template downloaded in step 5, updating the GithubBranch to the latest release (eg. release/v1.5.0 ) Go to AWS CloudFormation and select the stack: ASEA-what-you-provided Select Update, select Replace current template, Select Upload a template file Select Choose File and select the template you downloaded in step 6 ( AcceleratorInstallerXYZ.template.json or AcceleratorInstallerXXX-CodeCommit.template.json ) Select Next, Update GithubBranch parameter to release/vX.Y.Z where X.Y.Z represents the latest release Click Next, Next, I acknowledge, Update Wait for the CloudFormation stack to update ( Update_Complete status) (Requires manual refresh) Go To Code Pipeline and Release the ASEA-InstallerPipeline","title":"Summary of Upgrade Steps (all versions)"},{"location":"installation/v150-Upgrade/","text":"1. Accelerator v1.5.0 Custom Upgrade Instructions \u00b6 1. Accelerator v1.5.0 Custom Upgrade Instructions 1.1. Overview 1.2. Upgrade Caveats 1.3. Config File Conversion 1.4. Upgrade process 1.5. \"3.2. Summary of Upgrade Steps (all versions)\" (Copied from installation guide) 1.6. Post Upgrade Follow-up Tasks for v1.5.0 Upgrade 1.1. Overview \u00b6 The upgrade from v1.3.8 to v1.5.0 is generally the same as any previous Accelerator upgrades, with a couple of key differences: the magnitude of this release has resulted in a requirement for significant updates to the config file we have provided a script to assist with this process. A manual verification of the changes and customer custom updates are often still required. we are re-aligning the OU structure with AWS guidance and that of AWS Control Tower (optional, but highly recommended) the core OU is being split into a \"Security\" OU and an \"Infrastructure\" OU we've added the capability to manage your IP addresses in DynamoDB, rather than with the config file this includes the ability to dynamically allocate CIDR ranges to VPCs and subnets more information on this features design can be found on this ticket the config file conversion script will: update your config file in a manner that supports both CIDR management schemes (but continues to leverage the previous mechanism) copy your currently configured CIDR ranges into the appropriate DynamoDB tables (optional, but recommended) you can change your IP address mechanism for any vpc at any time customers can mix and match IP address management mechanisms as they choose ( provided , lookup , and dynamic ) 1.2. Upgrade Caveats \u00b6 While an upgrade path is planned, customers with a Standalone Accelerator installation can upgrade to v1.5.0 but need to continue with a Standalone installation until the Control Tower upgrade option becomes available. The script to assist with config file conversion and DynamoDB population only supports single file json based config files, customers that leverage YAML and/or multi-part config files, have several options: manually update your yaml or multi-part json config file to reflect the config file format for the latest release (similar to all previous upgrades) use the config.json file found in the raw folder of your codecommit repo to run the conversion script this version of the config file has resolved all variables with their final values, all variables will be removed from config.json in this scenario the new config file can be converted back to json/multi-part format before being placed back into your CodeComit repository or it could be used to simply validate the changes you made using option a do not manually update the config file in the raw folder, as it will be overwritten based on the json or yaml file in the root of your repository use a 3rd party tool to manually convert your yaml / multi-part config files to a single file json file to run the conversion script the new config file can be converted back to json/multi-part format before being placed back into your CodeComit repository Config files which are significantly different than the example config files may not be properly converted. This includes config files which use different mandatory account keys or renamed the core OU. This guide and its examples assume the existing accelerator deployment uses the PBMMAccel- accelerator prefix, if a different prefix is used on the existing installation, it is important it is specified when execution section 1.6 below. 1.3. Config File Conversion \u00b6 You must first upgrade to Accelerator v1.3.8 or v1.3.9 Login to your AWS Organization Management account Pull your current config.json file from CodeCommit and save as a text file Locate the python conversion script and review its readme here To convert your configuration file execute: (completely offline process) python update.py --Region ca-central-1 --LoadConfig --ConfigFile config.json This will output a new config file named: update-config.json Save both the original v13.8 and the new v1.5.0 config files for future reference/use After conversion, we recommend running the updated config file back prettier to simplify file comparisons While the conversion script often does much of the heavy lifting, we still require customers to manually verify the changes and make manual adjustments as appropriate: If you use a relatively standard config file you MAY not need to make any changes manually Ensure the value of account-name for the Organization Management account matches the actual account name of the Organization management account (the account key is generally either management or master ). we recommend you change your rdgw-instance-type and rsyslog-instance-type from t2. to t3. (they will auto-replace on the next instance refresh) (Optional). optionally remove the \"API_GW_EXECUTION_LOGGING_ENABLED\" config rule throughout, as it overlaps with an identical Security Hub config rule. we added the capability to deploy a Config aggregator in any of the central services accounts (i.e. Log-archive, Security, Operations), by adding \"config-aggr\": true to either : central-security-services , central-operations-services , or central-log-services . The existing aggregator in the Org management account will remain. Do not set it in all 3 sections, as AWS only supports a maximum of 3 config aggregators. the optional attribute endpoint-port-orverides has been properly renamed to endpoint-port-overrides . If you have the endpoint-port-orverides in your config file you must rename it to endpoint-port-overrides . the new example config files also introduced several new internally resolvable variables ( ${CONFIG::OU_NAME} and ${CONFIG::VPC_NAME} ), which when used thoughtfully along with the new dynamic CIDR feature, enables multi-part config file customers to define the VPCs for multiple OU's in a single shared nested config file. These new variables should be ignored during an upgrade. the accelerator supports 3 types of CIDR ranges provided , lookup , and dynamic . The upgrade script sets the cidr-src to provided , meaning it uses the CIDR ranges provided in the config file, as per the previous release. The upgrade script also adds the additional required fields ( pool and size ) to every CIDR range defined in the config file to leverage the lookup type, but when set to provided these fields are NOT required and could be removed. They were added by the script for the sole purpose of making it easy to switch from provided to lookup in future. Once a customer switches to lookup , the cidr\\value field is no longer used and can be removed from the config file. The cidr-src for should remain set at provided during upgrade. do not add the cidr-pools section to the config file during or before the upgrade, this section is only used for new installations. New description fields have been added to the config file to help provide context to certain objects. These will be used by a future GUI that is under development, and serve no functional purpose at this time. Customers can alter this text as they please. Most of the example config files have been converted to dynamic cidr-src as it provides simplier CIDR management for new customers. Two example config files ending in -oldIP.json have been maintained to aid upgrading customers in config file comparison. Be advised - in v1.5.0 we restructured the SCPs based on a) customer requests, and b) the addition of Control Tower support for new installs. customers are responsible for reviewing the SCPs to ensure they have not been altered in a manner that no longer meets an organizations security requirements; we reorganized and optimized our SCP's from 4 SCP files down to 3 SCP files, without removing any protections or guardrails; these optimizations have resulted in minor enhancements to the SCP protections and in some cases better scoping; the first two SCP files (Part-0 and Part-1) contain the controls which protect the integrity of the Accelerator itself; the third file (Sensitive, Unclass, Sandbox) contains customer data protection specific guardrails, which may change based on workload data classification or customer profiles and requirements; this frees the fourth SCP for use by Control Tower, or for use by customers for custom guardrails (Standalone installs only). As Control Tower leverages 2 SCP files on the Security OU, we have moved some of our SCP's to the account level (Control Tower installations only). 1.4. Upgrade process \u00b6 Before proceeding with your upgrade please review the General Upgrade Considerations in Section 3.1 of the Installation and Upgrade guide upgrades directly from v1.3.8 need to ensure they include the extra step required for v1.3.9 upgrades (removal of endpoints with periods) Login to your AWS Organization Management account, in your home or default region Place your updated and validated config file back in the root folder of your CodeCommit repository json, yaml and multi-part config files continue to be supported Change to the us-east-1 region and open CloudWatch Select Rules under events in the left navigation pane Select the PBMMAccel-MoveAccount_rule , select actions , select Disable Select the PBMMAccel-PolicyChanges_rule , select actions , select Disable Open AWS Organizations Select the core OU and rename it to Security (case sensitive) (i.e. Actions, Rename) In the Root ou, create a new OU named `Infrastructure (case sensitive) (i.e. Actions, Create new) Expand the Security OU, select all EXCEPT the security , log-archive , and Organization Management account (i.e. the Operations , Perimeter , and SharedNetwork accounts) Select Actions, Move, then select the newly created Infrastructure OU (note these accounts temporarily missing guardrails) NOTE: the key to this being a possible move/change, is the new Infrastructure OU is defined identically to the old core OU. Typically you CANNOT move accounts between OU's without breaking something and violating security guardrails. Select Policies from the left navigation pane, then Service COntrol Policies Click the PBMMAccel-Guardrails-Part-2 policy, and Select Targets Detach the policy from ALL OUs Change to the us-east-1 region and open CloudWatch Select Rules under events in the left navigation pane Select the PBMMAccel-MoveAccount_rule , select actions , select Enable Select the PBMMAccel-PolicyChanges_rule , select actions , select Enable Follow the Standard Upgrade instructions in Section 3.2 of the Installation and Upgrade guide, repeated verbatim below for ease of reference 1.5. \"3.2. Summary of Upgrade Steps (all versions)\" (Copied from installation guide) \u00b6 Login to your Organization Management (root) AWS account with administrative privileges Either: a) Ensure a valid Github token is stored in secrets manager, or b) Ensure the latest release is in a valid branch of CodeCommit in the Organization Management account. See (section 2.3.2) for more details. Review and implement any relevant tasks noted in the upgrade considerations in section 3.1 of the Installation and Upgrade guide Update the config file in CodeCommit with new parameters and updated parameter types based on the version you are upgrading to (this is important as features are iterating rapidly) An automated script is available to help convert config files to the new v1.5.0 format Compare your running config file with the sample config file from the latest release Review the Config file changes section of the release notes for all Accelerator versions since your current deployed release If you customized any of the other Accelerator default config files by overriding them in your S3 input bucket, merge the latest defaults with your customizations before beginning your upgrade Download the latest installer template ( AcceleratorInstallerXYZ.template.json or AcceleratorInstallerXXX-CodeCommit.template.json ) from the Assets section of the latest release Do NOT accidentally select the PBMMAccel-InitialSetup CloudFormation stack below If you are replacing your GitHub Token: Take note of the AcceleratorName , AcceleratorPrefix , ConfigS3Bucket and NotificationEmail values from the Parameters tab of your deployed Installer CloudFormation stack ( PBMMAccel-what-you-provided ) Delete the Installer CloudFormation stack ( PBMMAccel-what-you-provided ) Redeploy the Installer CloudFormation stack using the template downloaded in step 6, providing the values you just documented (changes to AcceleratorName or AcceleratorPrefix are not supported) The pipeline will automatically run and trigger the upgraded state machine If you are using a pre-existing GitHub token, or installing from CodeCommit: Update the Installer CloudFormation stack using the template downloaded in step 5, updating the GithubBranch to the latest release (eg. release/v1.5.0 ) Go to AWS CloudFormation and select the stack: PBMMAccel-what-you-provided Select Update, select Replace current template, Select Upload a template file Select Choose File and select the template you downloaded in step 6 ( AcceleratorInstallerXYZ.template.json or AcceleratorInstallerXXX-CodeCommit.template.json ) Select Next, Update GithubBranch parameter to release/vX.Y.Z where X.Y.Z represents the latest release Click Next, Next, I acknowledge, Update Wait for the CloudFormation stack to update ( Update_Complete status) (Requires manual refresh) Go To Code Pipeline and Release the PBMMAccel-InstallerPipeline 1.6. Post Upgrade Follow-up Tasks for v1.5.0 Upgrade \u00b6 Once the State Machine finishes in AWS Organizations, delete the PBMMAccel-Guardrails-Part-2 SCP, as we do not remove policies from Organizations given they could be used by customers for other purposes. Optionally, populate DynamoDB with your allocated CIDR ranges: You must perform the following steps if you plan to: move from \"cidr-src\": \"provided\" to \"cidr-src\": \"lookup\" for your existing cidr blocks; or dynamically assign CIDR ranges to new VPCs using pools that overlap with existing VPC's currently defined in your config file. this step populates the DynamoDB tables ( PBMMAccel-cidr-vpc-assign and PBMMAccel-cidr-subnet-assign ) with your EXISTING utilized CIDR ranges using the upgrade script a second time after the upgrade is complete. Using the converted version of your config file (update-config.json) Validate you are happy with the pool names assigned to each vpc and subnet throughout the config file. Update as appropriate, pool names can be any alpha-numeric string, but a subnets pool must match one of its VPCs pools. Locate the python conversion script and review its readme here To load DynamoDB with your CIDR ranges, execute: (online, requires credentials to the Organization Management account) python update.py --Region ca-central-1 --LoadDB --ConfigFile update-config.json --AcceleratorPrefix PBMMAccel- Note the use of the --LoadDB switch, the UPDATED config filename, and that the Accelerator prefix is specified If you run this command using your v1.3.8 or v1.3.9 config file, the \\cidr\\pool: values will be improperly assigned in DynamoDB. In the Organization Management account, in DynamoDB, select Items, and Verify that the tables have been properly populated. the script should have populated the following two DynamoDB tables: PBMMAccel-cidr-vpc-assign and PBMMAccel-cidr-subnet-assign with all your existing vpc and subnet assignments. if you plan to dynamically assign CIDR ranges for any new VPCs, you need to manually create the CIDR pools by adding new item(s) to the DynamoDB Table PBMMAccel-cidr-pool . The PBMMAccel-cidr-pool table stores CIDR ranges to select from for new CIDR assignments. This table works together with the other two DynamoDB tables to track, assign and maintain non-overlapping CIDR ranges based on a pool name and region. Sample DynamoDB JSON to add an entry to the PBMMAccel-cidr-pool table: { \"id\": { \"S\": \"1\" }, \"cidr\": { \"S\": \"10.0.0.0/13\" }, \"region\": { \"S\": \"ca-central-1\" }, \"pool\": { \"S\": \"main\" } } where id is any unique text, cidr is the main cidr block from which VPC cidrs are taken. region is the AWS region where the pool is used. pool is the name of the pool NOTES: You can populate the cidr-pools section of the config file/DynamoDB with values that overlap with the existing assigned ranges in your config file. In this situation, it is CRITICAL that you execute this entire process, to avoid issueing duplicate or overlapping CIDR ranges with those already issued. Alternatively, leverage new unique ranges when populating the cidr-pools . cidr-pools only needs to be populated when a vpc has a cidr-src set to dynamic . Optionally, change all the cidr-src values throughout your config file to lookup , and remove all the cidr\\value fields. Once changed, CIDR values will be provided by DynamoDB. Switching to lookup requires completion of the previous optional step to first load DynamoDB. run the state machine with the input parameters {\"scope\": \"FULL\",\"mode\": \"APPLY\",\"verbose\": \"0\"} during the state machine execution, the Accelerator will compare the values returned by DynamoDB with the values from the previous successful state machine execution. If the DynamoDB values were incorrectly populated, the state machine will catch it with a comparison failure message and gracefully fail.","title":"1. Accelerator v1.5.0 Custom Upgrade Instructions"},{"location":"installation/v150-Upgrade/#1-accelerator-v150-custom-upgrade-instructions","text":"1. Accelerator v1.5.0 Custom Upgrade Instructions 1.1. Overview 1.2. Upgrade Caveats 1.3. Config File Conversion 1.4. Upgrade process 1.5. \"3.2. Summary of Upgrade Steps (all versions)\" (Copied from installation guide) 1.6. Post Upgrade Follow-up Tasks for v1.5.0 Upgrade","title":"1. Accelerator v1.5.0 Custom Upgrade Instructions"},{"location":"installation/v150-Upgrade/#11-overview","text":"The upgrade from v1.3.8 to v1.5.0 is generally the same as any previous Accelerator upgrades, with a couple of key differences: the magnitude of this release has resulted in a requirement for significant updates to the config file we have provided a script to assist with this process. A manual verification of the changes and customer custom updates are often still required. we are re-aligning the OU structure with AWS guidance and that of AWS Control Tower (optional, but highly recommended) the core OU is being split into a \"Security\" OU and an \"Infrastructure\" OU we've added the capability to manage your IP addresses in DynamoDB, rather than with the config file this includes the ability to dynamically allocate CIDR ranges to VPCs and subnets more information on this features design can be found on this ticket the config file conversion script will: update your config file in a manner that supports both CIDR management schemes (but continues to leverage the previous mechanism) copy your currently configured CIDR ranges into the appropriate DynamoDB tables (optional, but recommended) you can change your IP address mechanism for any vpc at any time customers can mix and match IP address management mechanisms as they choose ( provided , lookup , and dynamic )","title":"1.1. Overview"},{"location":"installation/v150-Upgrade/#12-upgrade-caveats","text":"While an upgrade path is planned, customers with a Standalone Accelerator installation can upgrade to v1.5.0 but need to continue with a Standalone installation until the Control Tower upgrade option becomes available. The script to assist with config file conversion and DynamoDB population only supports single file json based config files, customers that leverage YAML and/or multi-part config files, have several options: manually update your yaml or multi-part json config file to reflect the config file format for the latest release (similar to all previous upgrades) use the config.json file found in the raw folder of your codecommit repo to run the conversion script this version of the config file has resolved all variables with their final values, all variables will be removed from config.json in this scenario the new config file can be converted back to json/multi-part format before being placed back into your CodeComit repository or it could be used to simply validate the changes you made using option a do not manually update the config file in the raw folder, as it will be overwritten based on the json or yaml file in the root of your repository use a 3rd party tool to manually convert your yaml / multi-part config files to a single file json file to run the conversion script the new config file can be converted back to json/multi-part format before being placed back into your CodeComit repository Config files which are significantly different than the example config files may not be properly converted. This includes config files which use different mandatory account keys or renamed the core OU. This guide and its examples assume the existing accelerator deployment uses the PBMMAccel- accelerator prefix, if a different prefix is used on the existing installation, it is important it is specified when execution section 1.6 below.","title":"1.2. Upgrade Caveats"},{"location":"installation/v150-Upgrade/#13-config-file-conversion","text":"You must first upgrade to Accelerator v1.3.8 or v1.3.9 Login to your AWS Organization Management account Pull your current config.json file from CodeCommit and save as a text file Locate the python conversion script and review its readme here To convert your configuration file execute: (completely offline process) python update.py --Region ca-central-1 --LoadConfig --ConfigFile config.json This will output a new config file named: update-config.json Save both the original v13.8 and the new v1.5.0 config files for future reference/use After conversion, we recommend running the updated config file back prettier to simplify file comparisons While the conversion script often does much of the heavy lifting, we still require customers to manually verify the changes and make manual adjustments as appropriate: If you use a relatively standard config file you MAY not need to make any changes manually Ensure the value of account-name for the Organization Management account matches the actual account name of the Organization management account (the account key is generally either management or master ). we recommend you change your rdgw-instance-type and rsyslog-instance-type from t2. to t3. (they will auto-replace on the next instance refresh) (Optional). optionally remove the \"API_GW_EXECUTION_LOGGING_ENABLED\" config rule throughout, as it overlaps with an identical Security Hub config rule. we added the capability to deploy a Config aggregator in any of the central services accounts (i.e. Log-archive, Security, Operations), by adding \"config-aggr\": true to either : central-security-services , central-operations-services , or central-log-services . The existing aggregator in the Org management account will remain. Do not set it in all 3 sections, as AWS only supports a maximum of 3 config aggregators. the optional attribute endpoint-port-orverides has been properly renamed to endpoint-port-overrides . If you have the endpoint-port-orverides in your config file you must rename it to endpoint-port-overrides . the new example config files also introduced several new internally resolvable variables ( ${CONFIG::OU_NAME} and ${CONFIG::VPC_NAME} ), which when used thoughtfully along with the new dynamic CIDR feature, enables multi-part config file customers to define the VPCs for multiple OU's in a single shared nested config file. These new variables should be ignored during an upgrade. the accelerator supports 3 types of CIDR ranges provided , lookup , and dynamic . The upgrade script sets the cidr-src to provided , meaning it uses the CIDR ranges provided in the config file, as per the previous release. The upgrade script also adds the additional required fields ( pool and size ) to every CIDR range defined in the config file to leverage the lookup type, but when set to provided these fields are NOT required and could be removed. They were added by the script for the sole purpose of making it easy to switch from provided to lookup in future. Once a customer switches to lookup , the cidr\\value field is no longer used and can be removed from the config file. The cidr-src for should remain set at provided during upgrade. do not add the cidr-pools section to the config file during or before the upgrade, this section is only used for new installations. New description fields have been added to the config file to help provide context to certain objects. These will be used by a future GUI that is under development, and serve no functional purpose at this time. Customers can alter this text as they please. Most of the example config files have been converted to dynamic cidr-src as it provides simplier CIDR management for new customers. Two example config files ending in -oldIP.json have been maintained to aid upgrading customers in config file comparison. Be advised - in v1.5.0 we restructured the SCPs based on a) customer requests, and b) the addition of Control Tower support for new installs. customers are responsible for reviewing the SCPs to ensure they have not been altered in a manner that no longer meets an organizations security requirements; we reorganized and optimized our SCP's from 4 SCP files down to 3 SCP files, without removing any protections or guardrails; these optimizations have resulted in minor enhancements to the SCP protections and in some cases better scoping; the first two SCP files (Part-0 and Part-1) contain the controls which protect the integrity of the Accelerator itself; the third file (Sensitive, Unclass, Sandbox) contains customer data protection specific guardrails, which may change based on workload data classification or customer profiles and requirements; this frees the fourth SCP for use by Control Tower, or for use by customers for custom guardrails (Standalone installs only). As Control Tower leverages 2 SCP files on the Security OU, we have moved some of our SCP's to the account level (Control Tower installations only).","title":"1.3. Config File Conversion"},{"location":"installation/v150-Upgrade/#14-upgrade-process","text":"Before proceeding with your upgrade please review the General Upgrade Considerations in Section 3.1 of the Installation and Upgrade guide upgrades directly from v1.3.8 need to ensure they include the extra step required for v1.3.9 upgrades (removal of endpoints with periods) Login to your AWS Organization Management account, in your home or default region Place your updated and validated config file back in the root folder of your CodeCommit repository json, yaml and multi-part config files continue to be supported Change to the us-east-1 region and open CloudWatch Select Rules under events in the left navigation pane Select the PBMMAccel-MoveAccount_rule , select actions , select Disable Select the PBMMAccel-PolicyChanges_rule , select actions , select Disable Open AWS Organizations Select the core OU and rename it to Security (case sensitive) (i.e. Actions, Rename) In the Root ou, create a new OU named `Infrastructure (case sensitive) (i.e. Actions, Create new) Expand the Security OU, select all EXCEPT the security , log-archive , and Organization Management account (i.e. the Operations , Perimeter , and SharedNetwork accounts) Select Actions, Move, then select the newly created Infrastructure OU (note these accounts temporarily missing guardrails) NOTE: the key to this being a possible move/change, is the new Infrastructure OU is defined identically to the old core OU. Typically you CANNOT move accounts between OU's without breaking something and violating security guardrails. Select Policies from the left navigation pane, then Service COntrol Policies Click the PBMMAccel-Guardrails-Part-2 policy, and Select Targets Detach the policy from ALL OUs Change to the us-east-1 region and open CloudWatch Select Rules under events in the left navigation pane Select the PBMMAccel-MoveAccount_rule , select actions , select Enable Select the PBMMAccel-PolicyChanges_rule , select actions , select Enable Follow the Standard Upgrade instructions in Section 3.2 of the Installation and Upgrade guide, repeated verbatim below for ease of reference","title":"1.4. Upgrade process"},{"location":"installation/v150-Upgrade/#15-32-summary-of-upgrade-steps-all-versions-copied-from-installation-guide","text":"Login to your Organization Management (root) AWS account with administrative privileges Either: a) Ensure a valid Github token is stored in secrets manager, or b) Ensure the latest release is in a valid branch of CodeCommit in the Organization Management account. See (section 2.3.2) for more details. Review and implement any relevant tasks noted in the upgrade considerations in section 3.1 of the Installation and Upgrade guide Update the config file in CodeCommit with new parameters and updated parameter types based on the version you are upgrading to (this is important as features are iterating rapidly) An automated script is available to help convert config files to the new v1.5.0 format Compare your running config file with the sample config file from the latest release Review the Config file changes section of the release notes for all Accelerator versions since your current deployed release If you customized any of the other Accelerator default config files by overriding them in your S3 input bucket, merge the latest defaults with your customizations before beginning your upgrade Download the latest installer template ( AcceleratorInstallerXYZ.template.json or AcceleratorInstallerXXX-CodeCommit.template.json ) from the Assets section of the latest release Do NOT accidentally select the PBMMAccel-InitialSetup CloudFormation stack below If you are replacing your GitHub Token: Take note of the AcceleratorName , AcceleratorPrefix , ConfigS3Bucket and NotificationEmail values from the Parameters tab of your deployed Installer CloudFormation stack ( PBMMAccel-what-you-provided ) Delete the Installer CloudFormation stack ( PBMMAccel-what-you-provided ) Redeploy the Installer CloudFormation stack using the template downloaded in step 6, providing the values you just documented (changes to AcceleratorName or AcceleratorPrefix are not supported) The pipeline will automatically run and trigger the upgraded state machine If you are using a pre-existing GitHub token, or installing from CodeCommit: Update the Installer CloudFormation stack using the template downloaded in step 5, updating the GithubBranch to the latest release (eg. release/v1.5.0 ) Go to AWS CloudFormation and select the stack: PBMMAccel-what-you-provided Select Update, select Replace current template, Select Upload a template file Select Choose File and select the template you downloaded in step 6 ( AcceleratorInstallerXYZ.template.json or AcceleratorInstallerXXX-CodeCommit.template.json ) Select Next, Update GithubBranch parameter to release/vX.Y.Z where X.Y.Z represents the latest release Click Next, Next, I acknowledge, Update Wait for the CloudFormation stack to update ( Update_Complete status) (Requires manual refresh) Go To Code Pipeline and Release the PBMMAccel-InstallerPipeline","title":"1.5. \"3.2. Summary of Upgrade Steps (all versions)\" (Copied from installation guide)"},{"location":"installation/v150-Upgrade/#16-post-upgrade-follow-up-tasks-for-v150-upgrade","text":"Once the State Machine finishes in AWS Organizations, delete the PBMMAccel-Guardrails-Part-2 SCP, as we do not remove policies from Organizations given they could be used by customers for other purposes. Optionally, populate DynamoDB with your allocated CIDR ranges: You must perform the following steps if you plan to: move from \"cidr-src\": \"provided\" to \"cidr-src\": \"lookup\" for your existing cidr blocks; or dynamically assign CIDR ranges to new VPCs using pools that overlap with existing VPC's currently defined in your config file. this step populates the DynamoDB tables ( PBMMAccel-cidr-vpc-assign and PBMMAccel-cidr-subnet-assign ) with your EXISTING utilized CIDR ranges using the upgrade script a second time after the upgrade is complete. Using the converted version of your config file (update-config.json) Validate you are happy with the pool names assigned to each vpc and subnet throughout the config file. Update as appropriate, pool names can be any alpha-numeric string, but a subnets pool must match one of its VPCs pools. Locate the python conversion script and review its readme here To load DynamoDB with your CIDR ranges, execute: (online, requires credentials to the Organization Management account) python update.py --Region ca-central-1 --LoadDB --ConfigFile update-config.json --AcceleratorPrefix PBMMAccel- Note the use of the --LoadDB switch, the UPDATED config filename, and that the Accelerator prefix is specified If you run this command using your v1.3.8 or v1.3.9 config file, the \\cidr\\pool: values will be improperly assigned in DynamoDB. In the Organization Management account, in DynamoDB, select Items, and Verify that the tables have been properly populated. the script should have populated the following two DynamoDB tables: PBMMAccel-cidr-vpc-assign and PBMMAccel-cidr-subnet-assign with all your existing vpc and subnet assignments. if you plan to dynamically assign CIDR ranges for any new VPCs, you need to manually create the CIDR pools by adding new item(s) to the DynamoDB Table PBMMAccel-cidr-pool . The PBMMAccel-cidr-pool table stores CIDR ranges to select from for new CIDR assignments. This table works together with the other two DynamoDB tables to track, assign and maintain non-overlapping CIDR ranges based on a pool name and region. Sample DynamoDB JSON to add an entry to the PBMMAccel-cidr-pool table: { \"id\": { \"S\": \"1\" }, \"cidr\": { \"S\": \"10.0.0.0/13\" }, \"region\": { \"S\": \"ca-central-1\" }, \"pool\": { \"S\": \"main\" } } where id is any unique text, cidr is the main cidr block from which VPC cidrs are taken. region is the AWS region where the pool is used. pool is the name of the pool NOTES: You can populate the cidr-pools section of the config file/DynamoDB with values that overlap with the existing assigned ranges in your config file. In this situation, it is CRITICAL that you execute this entire process, to avoid issueing duplicate or overlapping CIDR ranges with those already issued. Alternatively, leverage new unique ranges when populating the cidr-pools . cidr-pools only needs to be populated when a vpc has a cidr-src set to dynamic . Optionally, change all the cidr-src values throughout your config file to lookup , and remove all the cidr\\value fields. Once changed, CIDR values will be provided by DynamoDB. Switching to lookup requires completion of the previous optional step to first load DynamoDB. run the state machine with the input parameters {\"scope\": \"FULL\",\"mode\": \"APPLY\",\"verbose\": \"0\"} during the state machine execution, the Accelerator will compare the values returned by DynamoDB with the values from the previous successful state machine execution. If the DynamoDB values were incorrectly populated, the state machine will catch it with a comparison failure message and gracefully fail.","title":"1.6. Post Upgrade Follow-up Tasks for v1.5.0 Upgrade"},{"location":"installation/what-we-do-where/","text":"AWS Secure Environment Accelerator Deployment Capabilities \u00b6 Deploys, creates, manages and updates the following objects across a multi-region, multi-account AWS environment TASK Accelerator - What happens, WHERE, under what condition, on each state machine execution AWS Accounts - Creates mandatory accounts (accounts which other accounts are dependent on) organization management (root) account, global scope - Creates workload accounts (individually or in bulk), base personality determined by ou placement organization management (root) account, global scope - Supports native AWS Organization account and OU activities (OU and account rename, move account between OU's, create accounts, etc.) organization management (root) account, global scope - Applies a Deny All SCP on any newly created account(s) until successfully guardrailed organization management (root) account, new account scope (failure to apply guardrails fails the Accelerator and leaves account blocked until remediated) - Allows bulk parallel* account creation, configuration, updates and guardrail application creates, guardrails and configures new accounts and regions in parallel per defined personas, organization management (root) account. Control Tower account ingestion is sequential at this time. - Performs 'account warming' to establish initial limits, when required state Machine region only, defined accounts (per region potential) - Checks limit increases, when required (complies with initial limits until increased) per account, per region (supported limits only) - Automatically submits limit increases, when required state Machine region only, defined accounts (per region potential) - Leverages AWS Control Tower Accelerator and Control Tower home regions must match, the Accelerator supports all on-by-default regions and will require a standalone install in regions not yet supported by Control Tower Networking - Creates Transit Gateways and TGW route tables incl. static routes and inter-region TGW peering in the defined region(s), defined account(s) - Creates centralized and/or local account (bespoke) VPC's in the defined region(s), defined account(s) ...all completely and individually customizable (per account, VPC, subnet, or OU), Static or Dynamic VPC and subnet CIDR assignments - Creates Subnets, Route tables, NACLs, Security groups, NATGWs, IGWs, VGWs, CGWs (per customer specs) part of any VPC, in the defined region(s), defined account(s) - allows detailed CIDR allocation, and cross-account security group referencing - Deletes default VPC's (worldwide) in all regions, in all accounts, can disable regions (all accounts or specific account) - Creates VPC Endpoints (Gateway and Interface) part of any VPC, in the defined region(s), defined account(s) - Configures centralized endpoints (R53 zones populated, shared and attached to local and cross-account VPC's) configures regional central endpoints (only one 'central' VPC per region) - Creates Route 53 Private and Public Zones in the defined account(s), defined region(s), defined VPC(s), global scope - Creates Resolver Rules and Resolver (inbound/outbound) Endpoints part of a specific VPC(s), in the defined region(s), defined account(s) (i.e. per region possible) ...including MAD R53 DNS resolver rule creation created in same region as MAD only, shared to same region VPC's when use-central-endpoints set - Automatically creates R53 VPC Endpoint Overloaded Zones same region(s), same account(s) as the endpoint and VPC(s) - Deploys and configures AWS Network Firewall on any VPC, any region, any account Cross-Account Object Sharing - VPC and Subnet sharing, including account level retagging/naming (and per account security group 'replication') VPC's are shared to accounts within the SAME REGION as the source VPC only An OU could have additional VPC's defined for additional regions and would be shared to the appropriate accounts in the same additional regions - VPC peering and TGW attachments (local and cross-account) in the defined region, no cross-region attachments or peering supported - Managed Active Directory sharing state machine region only (consider same region as the MAD only)(unshare method not implemented) - Automated TGW inter-region peering cross-region, cross-account or same-account - Shares SSM remediation documents from defined account(s), to defined OU's, in defined regions Zone sharing and VPC associations - Public Hosted Zones no sharing, no association required (any account, any VPC, any region) - Private Hosted Zones - i.e. Cloud DNS domains associated worldwide to all VPCs with use-central-endpoints - Endpoint Private Hosted Zones associate within region, for all VPC use-central-endpoints (including cross-account) - On-premise resolver rules associate within region, for all VPC use-central-endpoints (including cross-account) - MAD resolver rule association same region as the MAD resolver only, assoc. w/all VPC use-central-endpoints Identity - Creates Directory services (Managed Active Directory and Active Directory Connectors) in a specific VPC, in the defined region, defined account - only 1 per account, therefore can't have a second region in the same account (ADC creation only supported in mandatory accounts) - Creates Windows admin bastion host auto-scaling group once per above MAD (once per account), same region as MAD - Set Windows domain password policies (initial installation only) once per above MAD (once per account), same region as MAD - Set IAM account password policies once per account, global scope - Creates Windows domain users and groups (initial installation only) once per above MAD (once per account), same region as MAD - Creates IAM Policies, Roles, Users, and Groups once per account, global scope Cloud Security Services - Enables and configs the following AWS services, worldwide w/central specified admin account: (each service can have specified regions disabled) - Guardduty w/S3 protection enabled all regions, all accounts, admin account per region - Security Hub (Enables specified security standards, and disables specified individual controls) enabled all regions, all accounts, admin account per region - Firewall Manager enabled once per account (global scope), single admin account - CloudTrail w/Insights and S3 data plane logging enabled all regions (using Organization trail, stored in Organization Management account) - Config Recorders/Aggregator enabled all regions, all regions include global events, aggregator set to specified region in Organization Management account - Macie enabled all regions, admin account per region - IAM Access Analyzer enabled once per account (global scope), single admin account - Enables CloudWatch access from central specified admin account enabled once per account (global scope), two admin accounts (Ops & Security) - Deploys customer provided SSM remediation documents (four provided out-of-box today) customized per OU, defined regions, defined accounts ...remediates S3 buckets without KMS CMK encryption and ALB's without centralized logging customized per OU, all regions, integrated w/SSM remediation, when desired - Deploys AWS Config rules (managed and custom) including AWS Conformance packs (NIST 800-53 deployed by default + 2 custom) customized per OU, all regions, all accounts integrated w/SSM remediation, when desired Other Security Capabilities - Creates, deploys and applies Service Control Policies at the top OU level only, sub-ou's managed directly through AWS Organizations - Creates Customer Managed KMS Keys w/automatic key rotation (SSM, EBS, S3) SSM and EBS keys are created if a VPC exists in the region, S3 if we need an Accelerator bucket in the region, per account - Enables account level default EBS KMS CMK encryption set if a VPC exists in the region, per account - Enables S3 Block Public Access once per account, global scope - Configures Systems Manager Session Manager w/KMS CMK encryption and centralized logging set if a VPC exists in the region, per account - Imports or requests certificates into AWS Certificate Manager State Machine region only (per region potential, required for ALB deployments) - Deploys both perimeter and account level ALB's w/Lambda health checks, certs & TLS policies State Machine region only (per region potential) - Deploys & configures 3rd party firewall clusters and management instances in the defined region(s), defined account(s) ...Gateway Load Balancer w/auto-scaling (NEW) and VPN IPSec BGP ECMP deployment options - Configuration is fully managed and maintained in AWS CodeCommit - full multi-account configuration history organization management (root) account ...breaking configuration changes block Accelerator execution Idempotent - extensive error handling and failure cleanup - Accelerator can be stopped, started, and rerun without implication Centralized Logging - Deploys an rsyslog auto-scaling cluster behind an NLB, all syslogs forwarded to CWL State Machine region only (per region potential) - Centralizes logging to a single centralized S3 KMS CMK encrypted bucket (enables, configures and centralizes) incl: Sets S3 ownership flag, sets bucket retentions - VPC Flow logs (w/Enhanced metadata fields and optional CWL destination) part of a specific VPC, in the defined region, defined account (to local account bucket in state machine region, replicated to log-archive primary region) - Organizational Cost and Usage Reports once per organization, global scope (to local account bucket in state machine region, replicated to log-archive primary region) - CloudTrail Logs including S3 Data Plane Logs (also sent to CWL) directly back to log-archive, specified primary region - All CloudWatch Logs (includes rsyslog logs) (and setting Log group retentions) State machine region, plus configured regions - Config History and Snapshots directly back to log-archive account specified primary region - Route 53 Public Zone Logs, DNS Resolver Query Logs to CloudWatch Logs in us-east-1 (which are sent to S3) - GuardDuty Findings directly back to log-archive, specified primary region - Macie Discovery results directly back to security, specified primary region, replicated to log-archive - ALB Logs State Machine region only (same as ALB deployment) - SSM Session Logs (also sent to CWL) All regions currently send back to central region, log-archive account Extensibility - Populates each accounts Parameter Store with the Accelerator deployed objects (allows customer IaC to extend/leverage) each account, defined regions (all ELB's across the environment are populated in specified accounts, i.e. perimeter, to enable automated end-to-end plumbing) - Every execution outputs the execution status and a list of successfully guardrailed accounts to a SNS topic allows 3rd party framework to execute after every Accelerator execution by hooking to SNS topic ...which emails a customer defined email address ...or hooking to the email alert - Deploys roles with customized access (read-only,write) to the log-archive buckets (enabling customer SIEM deployments, SSM, EC2 CWL) defined account, global scope - Designed for Day 1, 2 and day 10. Customers get new features without any customization effort no matter the deployed architecture Upgradable from any version to any version, no customization or professional services required (Customer production proven across multiple releases) Alerting - Deploys global High, Medium, Low, Ignore priority SNS topics and email subscriptions in the defined account, org accessible regional topics, each region subscribed to a single defined central region which has the email subscriptions - Deploys customer defined CloudWatch Log Metrics and Alarms w/prioritized alarms (19 out-of-box) all accounts, home region only, as this is where the Org/account CloudTrail exists - Creates and configures AWS budgets w/alerting (customizable per OU and per account) once per account, global scope - Configures email alerting for CloudTrail Metric Alarms, Firewall Manager Events, Security Hub Findings incl. Guardduty Findings General \u00b6 \"defined\" region, \"defined\" account, means \"customer defined\", either at installation, upgrade, or any time they decide to reconfigure all items are created per customer defined parameters and configurations and are fully customizable without changing a single line of code security services are enabled and deployed globally, but, each service can be disabled per region. A single region deployment is possible. customer can enable/disable features, or change the configuration of each feature in the Accelerator config file customers can evolve their configurations over time, as they evolve and as their requirements change, without the requirement for code changes or professional services Region support \u00b6 All AWS commercial regions are supported. Lack of availability of CodeBuild, CodeCommit, or AWS Organizations in the Accelerator primary or installation region will prevent installation directly in that region. In these cases, customers can select a different installation region and the Accelerator can remotely deploy configurations and guardrails to that unsupported installation region. Prior to v1.2.5, we utilized a single StackSet, which blocked several additional installation regions. The Accelerator no longer leverages any StackSets, unblocking installing directly in several additional regions. As most features can be toggled on/off (per region), we expect most regions should be supportable both as a primary (or installation) region with the three above noted exceptions, and in these cases should still be fully supported as a managed (or secondary) region. Opt-in regions are not yet supported, but given enough demand, could easily be added. ...Return to Accelerator Table of Contents","title":"AWS Secure Environment Accelerator Deployment Capabilities"},{"location":"installation/what-we-do-where/#aws-secure-environment-accelerator-deployment-capabilities","text":"Deploys, creates, manages and updates the following objects across a multi-region, multi-account AWS environment TASK Accelerator - What happens, WHERE, under what condition, on each state machine execution AWS Accounts - Creates mandatory accounts (accounts which other accounts are dependent on) organization management (root) account, global scope - Creates workload accounts (individually or in bulk), base personality determined by ou placement organization management (root) account, global scope - Supports native AWS Organization account and OU activities (OU and account rename, move account between OU's, create accounts, etc.) organization management (root) account, global scope - Applies a Deny All SCP on any newly created account(s) until successfully guardrailed organization management (root) account, new account scope (failure to apply guardrails fails the Accelerator and leaves account blocked until remediated) - Allows bulk parallel* account creation, configuration, updates and guardrail application creates, guardrails and configures new accounts and regions in parallel per defined personas, organization management (root) account. Control Tower account ingestion is sequential at this time. - Performs 'account warming' to establish initial limits, when required state Machine region only, defined accounts (per region potential) - Checks limit increases, when required (complies with initial limits until increased) per account, per region (supported limits only) - Automatically submits limit increases, when required state Machine region only, defined accounts (per region potential) - Leverages AWS Control Tower Accelerator and Control Tower home regions must match, the Accelerator supports all on-by-default regions and will require a standalone install in regions not yet supported by Control Tower Networking - Creates Transit Gateways and TGW route tables incl. static routes and inter-region TGW peering in the defined region(s), defined account(s) - Creates centralized and/or local account (bespoke) VPC's in the defined region(s), defined account(s) ...all completely and individually customizable (per account, VPC, subnet, or OU), Static or Dynamic VPC and subnet CIDR assignments - Creates Subnets, Route tables, NACLs, Security groups, NATGWs, IGWs, VGWs, CGWs (per customer specs) part of any VPC, in the defined region(s), defined account(s) - allows detailed CIDR allocation, and cross-account security group referencing - Deletes default VPC's (worldwide) in all regions, in all accounts, can disable regions (all accounts or specific account) - Creates VPC Endpoints (Gateway and Interface) part of any VPC, in the defined region(s), defined account(s) - Configures centralized endpoints (R53 zones populated, shared and attached to local and cross-account VPC's) configures regional central endpoints (only one 'central' VPC per region) - Creates Route 53 Private and Public Zones in the defined account(s), defined region(s), defined VPC(s), global scope - Creates Resolver Rules and Resolver (inbound/outbound) Endpoints part of a specific VPC(s), in the defined region(s), defined account(s) (i.e. per region possible) ...including MAD R53 DNS resolver rule creation created in same region as MAD only, shared to same region VPC's when use-central-endpoints set - Automatically creates R53 VPC Endpoint Overloaded Zones same region(s), same account(s) as the endpoint and VPC(s) - Deploys and configures AWS Network Firewall on any VPC, any region, any account Cross-Account Object Sharing - VPC and Subnet sharing, including account level retagging/naming (and per account security group 'replication') VPC's are shared to accounts within the SAME REGION as the source VPC only An OU could have additional VPC's defined for additional regions and would be shared to the appropriate accounts in the same additional regions - VPC peering and TGW attachments (local and cross-account) in the defined region, no cross-region attachments or peering supported - Managed Active Directory sharing state machine region only (consider same region as the MAD only)(unshare method not implemented) - Automated TGW inter-region peering cross-region, cross-account or same-account - Shares SSM remediation documents from defined account(s), to defined OU's, in defined regions Zone sharing and VPC associations - Public Hosted Zones no sharing, no association required (any account, any VPC, any region) - Private Hosted Zones - i.e. Cloud DNS domains associated worldwide to all VPCs with use-central-endpoints - Endpoint Private Hosted Zones associate within region, for all VPC use-central-endpoints (including cross-account) - On-premise resolver rules associate within region, for all VPC use-central-endpoints (including cross-account) - MAD resolver rule association same region as the MAD resolver only, assoc. w/all VPC use-central-endpoints Identity - Creates Directory services (Managed Active Directory and Active Directory Connectors) in a specific VPC, in the defined region, defined account - only 1 per account, therefore can't have a second region in the same account (ADC creation only supported in mandatory accounts) - Creates Windows admin bastion host auto-scaling group once per above MAD (once per account), same region as MAD - Set Windows domain password policies (initial installation only) once per above MAD (once per account), same region as MAD - Set IAM account password policies once per account, global scope - Creates Windows domain users and groups (initial installation only) once per above MAD (once per account), same region as MAD - Creates IAM Policies, Roles, Users, and Groups once per account, global scope Cloud Security Services - Enables and configs the following AWS services, worldwide w/central specified admin account: (each service can have specified regions disabled) - Guardduty w/S3 protection enabled all regions, all accounts, admin account per region - Security Hub (Enables specified security standards, and disables specified individual controls) enabled all regions, all accounts, admin account per region - Firewall Manager enabled once per account (global scope), single admin account - CloudTrail w/Insights and S3 data plane logging enabled all regions (using Organization trail, stored in Organization Management account) - Config Recorders/Aggregator enabled all regions, all regions include global events, aggregator set to specified region in Organization Management account - Macie enabled all regions, admin account per region - IAM Access Analyzer enabled once per account (global scope), single admin account - Enables CloudWatch access from central specified admin account enabled once per account (global scope), two admin accounts (Ops & Security) - Deploys customer provided SSM remediation documents (four provided out-of-box today) customized per OU, defined regions, defined accounts ...remediates S3 buckets without KMS CMK encryption and ALB's without centralized logging customized per OU, all regions, integrated w/SSM remediation, when desired - Deploys AWS Config rules (managed and custom) including AWS Conformance packs (NIST 800-53 deployed by default + 2 custom) customized per OU, all regions, all accounts integrated w/SSM remediation, when desired Other Security Capabilities - Creates, deploys and applies Service Control Policies at the top OU level only, sub-ou's managed directly through AWS Organizations - Creates Customer Managed KMS Keys w/automatic key rotation (SSM, EBS, S3) SSM and EBS keys are created if a VPC exists in the region, S3 if we need an Accelerator bucket in the region, per account - Enables account level default EBS KMS CMK encryption set if a VPC exists in the region, per account - Enables S3 Block Public Access once per account, global scope - Configures Systems Manager Session Manager w/KMS CMK encryption and centralized logging set if a VPC exists in the region, per account - Imports or requests certificates into AWS Certificate Manager State Machine region only (per region potential, required for ALB deployments) - Deploys both perimeter and account level ALB's w/Lambda health checks, certs & TLS policies State Machine region only (per region potential) - Deploys & configures 3rd party firewall clusters and management instances in the defined region(s), defined account(s) ...Gateway Load Balancer w/auto-scaling (NEW) and VPN IPSec BGP ECMP deployment options - Configuration is fully managed and maintained in AWS CodeCommit - full multi-account configuration history organization management (root) account ...breaking configuration changes block Accelerator execution Idempotent - extensive error handling and failure cleanup - Accelerator can be stopped, started, and rerun without implication Centralized Logging - Deploys an rsyslog auto-scaling cluster behind an NLB, all syslogs forwarded to CWL State Machine region only (per region potential) - Centralizes logging to a single centralized S3 KMS CMK encrypted bucket (enables, configures and centralizes) incl: Sets S3 ownership flag, sets bucket retentions - VPC Flow logs (w/Enhanced metadata fields and optional CWL destination) part of a specific VPC, in the defined region, defined account (to local account bucket in state machine region, replicated to log-archive primary region) - Organizational Cost and Usage Reports once per organization, global scope (to local account bucket in state machine region, replicated to log-archive primary region) - CloudTrail Logs including S3 Data Plane Logs (also sent to CWL) directly back to log-archive, specified primary region - All CloudWatch Logs (includes rsyslog logs) (and setting Log group retentions) State machine region, plus configured regions - Config History and Snapshots directly back to log-archive account specified primary region - Route 53 Public Zone Logs, DNS Resolver Query Logs to CloudWatch Logs in us-east-1 (which are sent to S3) - GuardDuty Findings directly back to log-archive, specified primary region - Macie Discovery results directly back to security, specified primary region, replicated to log-archive - ALB Logs State Machine region only (same as ALB deployment) - SSM Session Logs (also sent to CWL) All regions currently send back to central region, log-archive account Extensibility - Populates each accounts Parameter Store with the Accelerator deployed objects (allows customer IaC to extend/leverage) each account, defined regions (all ELB's across the environment are populated in specified accounts, i.e. perimeter, to enable automated end-to-end plumbing) - Every execution outputs the execution status and a list of successfully guardrailed accounts to a SNS topic allows 3rd party framework to execute after every Accelerator execution by hooking to SNS topic ...which emails a customer defined email address ...or hooking to the email alert - Deploys roles with customized access (read-only,write) to the log-archive buckets (enabling customer SIEM deployments, SSM, EC2 CWL) defined account, global scope - Designed for Day 1, 2 and day 10. Customers get new features without any customization effort no matter the deployed architecture Upgradable from any version to any version, no customization or professional services required (Customer production proven across multiple releases) Alerting - Deploys global High, Medium, Low, Ignore priority SNS topics and email subscriptions in the defined account, org accessible regional topics, each region subscribed to a single defined central region which has the email subscriptions - Deploys customer defined CloudWatch Log Metrics and Alarms w/prioritized alarms (19 out-of-box) all accounts, home region only, as this is where the Org/account CloudTrail exists - Creates and configures AWS budgets w/alerting (customizable per OU and per account) once per account, global scope - Configures email alerting for CloudTrail Metric Alarms, Firewall Manager Events, Security Hub Findings incl. Guardduty Findings","title":"AWS Secure Environment Accelerator Deployment Capabilities"},{"location":"installation/what-we-do-where/#general","text":"\"defined\" region, \"defined\" account, means \"customer defined\", either at installation, upgrade, or any time they decide to reconfigure all items are created per customer defined parameters and configurations and are fully customizable without changing a single line of code security services are enabled and deployed globally, but, each service can be disabled per region. A single region deployment is possible. customer can enable/disable features, or change the configuration of each feature in the Accelerator config file customers can evolve their configurations over time, as they evolve and as their requirements change, without the requirement for code changes or professional services","title":"General"},{"location":"installation/what-we-do-where/#region-support","text":"All AWS commercial regions are supported. Lack of availability of CodeBuild, CodeCommit, or AWS Organizations in the Accelerator primary or installation region will prevent installation directly in that region. In these cases, customers can select a different installation region and the Accelerator can remotely deploy configurations and guardrails to that unsupported installation region. Prior to v1.2.5, we utilized a single StackSet, which blocked several additional installation regions. The Accelerator no longer leverages any StackSets, unblocking installing directly in several additional regions. As most features can be toggled on/off (per region), we expect most regions should be supportable both as a primary (or installation) region with the three above noted exceptions, and in these cases should still be fully supported as a managed (or secondary) region. Opt-in regions are not yet supported, but given enough demand, could easily be added. ...Return to Accelerator Table of Contents","title":"Region support"},{"location":"operations/","text":"Operations & Troubleshooting Guide \u00b6 This document is targeted at individuals installing or executing the AWS Secure Environment Accelerator. It is intended to guide individuals who are executing the Accelerator by providing an understanding as to what happens at each point throughout execution and to assist in troubleshooting state machine failures and/or errors. This is one component of the provided documenation package and should be read after the Installation Guide, but before the Developer Guide. System Overview Troubleshooting Common Tasks Import Existing ALZ","title":"Operations & Troubleshooting Guide"},{"location":"operations/#operations-troubleshooting-guide","text":"This document is targeted at individuals installing or executing the AWS Secure Environment Accelerator. It is intended to guide individuals who are executing the Accelerator by providing an understanding as to what happens at each point throughout execution and to assist in troubleshooting state machine failures and/or errors. This is one component of the provided documenation package and should be read after the Installation Guide, but before the Developer Guide. System Overview Troubleshooting Common Tasks Import Existing ALZ","title":"Operations &amp; Troubleshooting Guide"},{"location":"operations/common-tasks/","text":"Common Tasks \u00b6 Restart the State Machine \u00b6 The state machine can be stopped and restarted at any time. The Accelerator has been design to be able to rollback to a stable state, such that should the state machine be stopped or fail for any reason, subsequent state machine executions can simply proceed through the failed step without manual cleanup or issues (assuming the failure scenario has been resolved). An extensive amount of effort was placed on ensuring seamless customer recovery in failure situations. The Accelerator is idempotent - it can be run as many or as few times as desired with no negative effect. On each state machine execution, the state machine, primarily leveraging the capabilities of CDK, will evaluate the delta's between the old previously deployed configuration and the new configuration and update the environment as appropriate. The state machine will execute: automatically after each execution of the Code Pipeline (new installs, code upgrades, or manual pipeline executions) automatically when new AWS accounts are moved into any Accelerator controller OU in AWS Organizations when someone manual starts it: Step Functions , ASEA-MainStateMachine_sm , Start Execution , Start Execution (leave default values in name and json box) The state machine prevents users from accidentally performing certain major breaking changes, specifically unsupported AWS platform changes, changes that will fail to deploy, or changes that could be catastrophic to users. If someone knows exactly what they are doing and the full implications of these changes, we provide the option to override these checks. Customers should expect that items we have blocked CANNOT be changed after the Accelerator installation. These flags should be used with extreme caution. Specifying any of these flags without proper guidance will likely leave your Accelerator in a state of disrepair. These flags were added for internal purposes only - we do NOT support customers providing these flags. Providing this parameter to the state machine overrides all checks: { \"overrideComparison\": true } Providing any one or more of the following flags will only override the specified check(s): { \"configOverrides\": { 'ov-global-options': true, 'ov-del-accts': true, 'ov-ren-accts': true, 'ov-acct-email': true, 'ov-acct-ou': true, 'ov-acct-vpc': true, 'ov-acct-subnet': true, 'ov-tgw': true, 'ov-mad': true, 'ov-ou-vpc': true, 'ov-ou-subnet': true, 'ov-share-to-ou': true, 'ov-share-to-accounts': true, 'ov-nacl': true, 'ov-nfw': true } } Providing this value allows for the forced rebuilding of the DynamoDB Outputs table: { \"storeAllOutputs\": true } Switch To a Managed Account \u00b6 To switch from the root account to a managed account you can click on your account name in the AWS Console. Then choose Switch Role in the menu. In the page that appears next you need to fill out the account ID of the managed account you want to switch to. Next, you need to enter the role name defined in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ). And lastly, you need to enter a relevant name so you can later switch roles by using this name. TBD: This role may be locked down starting in v1.2.5 - Update process once direction finalized Caution: This mechanism is ONLY to be used for troubleshooting Accelerator problems. This role is outside the Accelerator governance process and bypasses all the preventative guardrails that protect the Accelerator contructs and prevent users from performing activities in violation of the security guardrails. This role should NOT be used outside this context, all users should be authenticating and logging into the environment through AWS SSO. After switching to the managed account, the AWS Console header will look like the following image. You can switch to the same account again quickly by clicking the name you entered previously in the menu. [1] : https://docs.aws.amazon.com/cdk/latest/guide/home.html","title":"Common Tasks"},{"location":"operations/common-tasks/#common-tasks","text":"","title":"Common Tasks"},{"location":"operations/common-tasks/#restart-the-state-machine","text":"The state machine can be stopped and restarted at any time. The Accelerator has been design to be able to rollback to a stable state, such that should the state machine be stopped or fail for any reason, subsequent state machine executions can simply proceed through the failed step without manual cleanup or issues (assuming the failure scenario has been resolved). An extensive amount of effort was placed on ensuring seamless customer recovery in failure situations. The Accelerator is idempotent - it can be run as many or as few times as desired with no negative effect. On each state machine execution, the state machine, primarily leveraging the capabilities of CDK, will evaluate the delta's between the old previously deployed configuration and the new configuration and update the environment as appropriate. The state machine will execute: automatically after each execution of the Code Pipeline (new installs, code upgrades, or manual pipeline executions) automatically when new AWS accounts are moved into any Accelerator controller OU in AWS Organizations when someone manual starts it: Step Functions , ASEA-MainStateMachine_sm , Start Execution , Start Execution (leave default values in name and json box) The state machine prevents users from accidentally performing certain major breaking changes, specifically unsupported AWS platform changes, changes that will fail to deploy, or changes that could be catastrophic to users. If someone knows exactly what they are doing and the full implications of these changes, we provide the option to override these checks. Customers should expect that items we have blocked CANNOT be changed after the Accelerator installation. These flags should be used with extreme caution. Specifying any of these flags without proper guidance will likely leave your Accelerator in a state of disrepair. These flags were added for internal purposes only - we do NOT support customers providing these flags. Providing this parameter to the state machine overrides all checks: { \"overrideComparison\": true } Providing any one or more of the following flags will only override the specified check(s): { \"configOverrides\": { 'ov-global-options': true, 'ov-del-accts': true, 'ov-ren-accts': true, 'ov-acct-email': true, 'ov-acct-ou': true, 'ov-acct-vpc': true, 'ov-acct-subnet': true, 'ov-tgw': true, 'ov-mad': true, 'ov-ou-vpc': true, 'ov-ou-subnet': true, 'ov-share-to-ou': true, 'ov-share-to-accounts': true, 'ov-nacl': true, 'ov-nfw': true } } Providing this value allows for the forced rebuilding of the DynamoDB Outputs table: { \"storeAllOutputs\": true }","title":"Restart the State Machine"},{"location":"operations/common-tasks/#switch-to-a-managed-account","text":"To switch from the root account to a managed account you can click on your account name in the AWS Console. Then choose Switch Role in the menu. In the page that appears next you need to fill out the account ID of the managed account you want to switch to. Next, you need to enter the role name defined in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ). And lastly, you need to enter a relevant name so you can later switch roles by using this name. TBD: This role may be locked down starting in v1.2.5 - Update process once direction finalized Caution: This mechanism is ONLY to be used for troubleshooting Accelerator problems. This role is outside the Accelerator governance process and bypasses all the preventative guardrails that protect the Accelerator contructs and prevent users from performing activities in violation of the security guardrails. This role should NOT be used outside this context, all users should be authenticating and logging into the environment through AWS SSO. After switching to the managed account, the AWS Console header will look like the following image. You can switch to the same account again quickly by clicking the name you entered previously in the menu. [1] : https://docs.aws.amazon.com/cdk/latest/guide/home.html","title":"Switch To a Managed Account"},{"location":"operations/operations-import-ALZAccount/","text":"How to migrate an AWS Landing Zone (ALZ) account \"as is\" into an AWS Secure Environment Accelerator (ASEA) \u00b6 This document describes the steps to migrate an existing linked account from an AWS Landing Zone (ALZ) to an AWS Secure Environment Accelerator (ASEA). Prerequisites / Setup \u00b6 Confirm ASEA SSO and OU configuration \u00b6 On the ASEA, setup and run initial tests with SSO and permission sets with an account under the OU where the linked account will be migrated to. Confirm that SSO is properly configured with permissions required for the team members whose account is being migrated. This would include configuration of the ASEA\u2019s AWS Managed Active Directory (MAD) which should align with how the team migrating their account has their AWS SSO and MAD configured today. Switch the ALZ linked account payment method to invoicing \u00b6 If working with your AWS account team (TAM/SA) they will reach out to an internal team within AWS to have the linked account payment method switched to invoicing. This way the customer doesn\u2019t have to enter a credit card when making the account standalone in the upcoming steps. Confirm console access to the ALZ linked account and also to the email account \u00b6 Confirm you have access to login as root to the ALZ linked account AWS console. Confirm you have access to the email account associated to the ALZ linked account. The upcoming steps will first make the account standalone (remove from ALZ organizations) so you need to make sure you have root access to the account. If required, you can reset the password following: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_change-root.html If an Enterprise Support (ES) customer, then confirm ES is enabled on the ALZ linked account \u00b6 If the ALZ management account is on Enterprise Support (ES), then make sure ES is enabled on the linked account being migrated to the ASEA. If its not, then raise a support case to activate ES on the linked account. This is to make sure an ES support case can be created and escalated during step 2 if any unforeseen issue occurs. Confirm the ALZ CodePipeline is executing successfully. \u00b6 Make sure the ALZ CodePipeline is still running successfully. Execute the ALZ CodePipeline from the management account to make sure it runs successfully. - AWS Console -> CodePipeline - Select \u201cAWS-Landing-Zone-CodePipeline\u201d - Select \u201cRelease Change\u201d - Click on the pipeline and confirm it successfully runs through to completion Confirm CLI access and setup Python and the AWS Python SDK (boto3) \u00b6 Confirm SSO temporary command line access from the management account with AdminAccess. - SSO login \u2192 Select linked account \u2192 \u201cCommand line or programmatic access\u201d - Select Option 2 and add to your AWS credentials file under \u201c[default]\u201c - This is required as the python script in step 3 takes a \u201cprofile\u201d parameter - Confirm you have the AWS CLI tool installed. - https://aws.amazon.com/cli/ - Confirm by running a command such as \u201caws s3 ls\u201d - Confirm you have python3 and the AWS python library (boto3) installed which is required in step 2 to confirm the account has been disassociated from the landing zone correctly. - https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html Landing Zone - Disassociate the account from the ALZ \u00b6 Login to the ALZ management account, and go to \u201cService Catalog\u201d -> \u201cProvisioned products\u201d Select \u201cAccess Filter\u201d -> \u201cAccount\u201d to see a list of the account products Select the product for the specific linked account \u00b6 Put the linked account name in the provisioned products search bar This will narrow down the list and show a product name \u201cAWS-Landing-Zone-Account-Vending-Machine\u201d with a name \u201clz_applicaitons_ _ \u201d Select that product and then \u201cActions->Terminate\u201d Confirm the product successfully terminates \u00b6 The provisioned product entry will show a status of \u201cUnder change\u201d You can also verify by going to CloudFormation\u2192Stacks and you will see \u201cDELETE IN PROGRESS\u201d for the AVM Template stack being deleted. Go to the Resources tab to see the deleted resources associated to this stack. Once the provisioned product no longer says \u201cUnder change\u201d move to the next step. Please note, this can take 1-2 hours. Go to the linked account (assume role) \u00b6 From the management account, assume the role \u201cAWSCloudFormationStackSetExecutionRole\u201d to the linked account or optionally, SSO with console access to that account Under \u201cCloudFormation\u201d verify that the ALZ Stacks (StackSets from ALZ mgmt) were deleted \u00b6 There should be no stack left in the linked account with the prefix \u201cStackSet-AWS-Landing-Zone-Baseline*\". For example: StackSet-AWS-Landing-Zone-Baseline-CentralizedLoggingSpoke- StackSet-AWS-Landing-Zone-Baseline-EnableConfigRules- StackSet-AWS-Landing-Zone-Baseline-EnableNotifications- StackSet-AWS-Landing-Zone-Baseline-EnableConfigRulesGlobal- StackSet-AWS-Landing-Zone-Baseline-EnableConfig- StackSet-AWS-Landing-Zone-Baseline-ConfigRole- StackSet-AWS-Landing-Zone-Baseline-IamPasswordPolicy- StackSet-AWS-Landing-Zone-Baseline-SecurityRoles- StackSet-AWS-Landing-Zone-Baseline-EnableCloudTrail- Verify that the account is ready to be invited and baselined by the ASEA \u00b6 You need to ensure that resources don\u2019t exist in the default VPC, there is no config recorder channel, no CloudTrail Trail and STS is active in all regions. This can be done manually, but ideally use this python script that can be run as well to automate the verification https://github.com/paulbayer/Inventory_Scripts/blob/mainline/ALZ_CheckAccount.py mkdir test; cd test git clone https://github.com/paulbayer/Inventory_Scripts.git python3 ALZ_CheckAccount.py -a LINKED ACCOUNT_HERE -p default It will run through 5 steps and output the following. If you were to run this script before the \u201cterminate\u201d step above is complete you would have warnings in steps 2 and 3 below. Step 0 completed without issues Checking account 111122223333 for default VPCs in any region Step 1 completed with no issues Checking account 111122223333 for a Config Recorders and Delivery Channels in any region Step 2 completed with no issues Checking account 111122223333 for a specially named CloudTrail in all regions Step 3 completed with no issues Checking account 111122223333 for any GuardDuty invites Step 4 completed with no issues Checking that the account is part of the AWS Organization. Step 5 completed with no issues We've found NO issues that would hinder the adoption of this account **** Landing Zone (ALZ) - Remove the account from the ALZ organizations and make standalone \u00b6 Removing the account from the ALZ organizations and making it standalone is required so it can be invited into the ASEA organization. Read the following summary/considerations \u00b6 https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/ Verify access \u00b6 As stated in the previous sections, verify you have a mechanism to access the account post leaving the ALZ organization Former SSO roles will no longer function nor will the \u201cAWSCloudFormationStackSetExecutionRole\u201d role as it will have a trust relationship to the ALZ management account. Confirm the root credentials have been recovered and are usable As an alternative, confirm access with a new role/IAM user with Admin permissions on the account Verify billing flipped to invoicing \u00b6 As stated in the previous sections, verify the account payment method has been flipped to \u201cinvoicing\u201d to avoid having to enter a Credit Card when going standalone. This can be done working with your AWS account team who will coordinate internally, or by raising a support case describing the use case. Remove the account from the organizations and make standalone \u00b6 Follow the instructions on the following link to remove the account The short version is select the account from the ALZ mgmt account Organizations and select \"remove\" https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.html https://aws.amazon.com/blogs/security/aws-organizations-now-supports-self-service-removal-of-accounts-from-an-organization Note, when moving the account standalone do not select Enterprise Support. You shouldn't get a popup dialog asking for a Credit Card and the Support level since the account should have been moved to invoicing. Support can be reenabled on the linked account once it\u2019s invited into the ASEA organization. Accelerator - Invite the account into its organization \u00b6 From the ASEA mgmt account, send an invite to the standalone account \u00b6 Follow the instructions on the following link to invite the account The short version is go to the ASEA mgmt account organizations and select \"Add an account\" -> \"Invite existing account\" -> \"enter the linked account account ID\" https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html In the former ALZ account, Accept the invitation \u00b6 https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html#orgs_manage_accounts_accept-decline-invite Keep the linked account at the root level of the Organizations \u00b6 Verify access to the linked account using your root login credentials If you had created an IAM role/user with Admin permissions, then verify access as well Activate Enterprise Support (ES) on this linked account \u00b6 If ES is enabled on the ASEA management account, open a support case to enable ES on this linked account Go to the Support center and create a billing support case with \"Account\" and \"Activation\" Subject \"Requesting ES enablement on linked account\" Body \"Requesting ES enablement on linked account \" Your AWS TAM can escalate the case with the support team if it\u2019s time sensitive. This is to make sure an ES support case can be created and escalated during the next steps if any unforeseen issue occurs. Update (or add) the Organization Adming Role so one can assume the role into the linked account \u00b6 Login to the linked account which just joined the organization. Create a new Organization Admin role, as defined in the customers config file: \"organization-admin-role\": \"OrganizationAccountAccessRole\". With newer customers the default is \"OrganizationAccountAccessRole, with older customers it is \"AWSCloudFormationStackSetExecutionRole\". If \"AWSCloudFormationStackSetExecutionRole\" then you can edit the trust relationship directly Go to IAM -> Role -> AWSCloudFormationStackSetExecutionRole Update the trust relationship to have the management account ID of the ASEA (instead of the account ID of the previous ALZ) Verify that you can assume this role from the management account into the linked account Accelerator - Move the linked account from the top level root OU into the appropriate OU managed by the ASEA \u00b6 Plan what OU this account will be moved into \u00b6 Option 1 - Create a new OU and move the account into that OU Before the migration, the team would have created a new OU (ie-similar to the sandbox OU). This would be needed if they need to isolate this account from TGW attachments/Networking and want to keep it isolated. The state machine will run and start to baseline the account. It will create a new VPC and deploy resources using CFN such as Config, CloudTrail, etc. Note, if the OU is setup similar to the sandbox OU it does not provide access to the shared VPCs that have the TGW attachments. Creating a new OU also requires adding that new OU and the OU persona to the config file in advance of the next state machine execution. Option 2 - Move account into an existing OU (ie-prod) The state machine will run and start to baseline the account. It will create a new VPC and deploy resources using CFN such as Config, CloudTrail, etc. The customers existing VPC will remain, as a 2nd DETACHED VPC. Mote. if it is non-compliant to security rules, it remains non-compliant and needs to be cleaned up and brought into compliance If the VPC is compliant and it has unique IP addresses, it could be attached to the TGW. Move the account from the root OU to the correct OU \u00b6 THIS CANNOT BE EASILY UNDONE - MAKE SURE YOU MOVE TO THE CORRECT OU Follow the instructions on the following link to move the account to the correct OU The short version is go to the ASEA management account organizations and \"select the account\" -> \"actions\" -> \"move\" -> \"select the correct OU\" NOTE: The ASEA state machine will automatically start within 1-2 minutes of the account being moved into the OU https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html#move_account_to_ou Verify that the ASEA main state machine (under AWS->Step Functions) is triggered and runs cleanly (~30-45 minutes) Accelerator (ASEA) - Verify access control with roles, SSO, etc \u00b6 Update and verify SSO and permission sets for the linked account now part of the ASEA Verify you still have access to the linked account via root (or other mechanisms) Verify you still can assume the operations role into the linked account Landing Zone - Close down the ALZ core accounts and then the management account \u00b6 Once all workloads have been migrated from the ALZ to the ASEA, then you may decide to shutdown your ALZ. Close down the ALZ linked accounts \u00b6 Close all the linked accounts \u201cas is\u201d without making them standalone This will be the ALZ core linked accounts, but you might have some remaining workload accounts you decided not to migrate to the ASEA. https://aws.amazon.com/premiumsupport/knowledge-center/close-aws-account The management account will remain with organizations and the core accounts will show as suspended for 90 days. Close down the ALZ management account \u00b6 After 90 days, the suspended linked accounts will be completely closed Go to the root account and turn off Organizations and then close the root account","title":"Import Existing ALZ"},{"location":"operations/operations-import-ALZAccount/#how-to-migrate-an-aws-landing-zone-alz-account-as-is-into-an-aws-secure-environment-accelerator-asea","text":"This document describes the steps to migrate an existing linked account from an AWS Landing Zone (ALZ) to an AWS Secure Environment Accelerator (ASEA).","title":"How to migrate an AWS Landing Zone (ALZ) account \"as is\" into an AWS Secure Environment Accelerator (ASEA)"},{"location":"operations/operations-import-ALZAccount/#prerequisites-setup","text":"","title":"Prerequisites / Setup"},{"location":"operations/operations-import-ALZAccount/#confirm-asea-sso-and-ou-configuration","text":"On the ASEA, setup and run initial tests with SSO and permission sets with an account under the OU where the linked account will be migrated to. Confirm that SSO is properly configured with permissions required for the team members whose account is being migrated. This would include configuration of the ASEA\u2019s AWS Managed Active Directory (MAD) which should align with how the team migrating their account has their AWS SSO and MAD configured today.","title":"Confirm ASEA SSO and OU configuration"},{"location":"operations/operations-import-ALZAccount/#switch-the-alz-linked-account-payment-method-to-invoicing","text":"If working with your AWS account team (TAM/SA) they will reach out to an internal team within AWS to have the linked account payment method switched to invoicing. This way the customer doesn\u2019t have to enter a credit card when making the account standalone in the upcoming steps.","title":"Switch the ALZ linked account payment method to invoicing"},{"location":"operations/operations-import-ALZAccount/#confirm-console-access-to-the-alz-linked-account-and-also-to-the-email-account","text":"Confirm you have access to login as root to the ALZ linked account AWS console. Confirm you have access to the email account associated to the ALZ linked account. The upcoming steps will first make the account standalone (remove from ALZ organizations) so you need to make sure you have root access to the account. If required, you can reset the password following: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_change-root.html","title":"Confirm console access to the ALZ linked account and also to the email account"},{"location":"operations/operations-import-ALZAccount/#if-an-enterprise-support-es-customer-then-confirm-es-is-enabled-on-the-alz-linked-account","text":"If the ALZ management account is on Enterprise Support (ES), then make sure ES is enabled on the linked account being migrated to the ASEA. If its not, then raise a support case to activate ES on the linked account. This is to make sure an ES support case can be created and escalated during step 2 if any unforeseen issue occurs.","title":"If an Enterprise Support (ES) customer, then confirm ES is enabled on the ALZ linked account"},{"location":"operations/operations-import-ALZAccount/#confirm-the-alz-codepipeline-is-executing-successfully","text":"Make sure the ALZ CodePipeline is still running successfully. Execute the ALZ CodePipeline from the management account to make sure it runs successfully. - AWS Console -> CodePipeline - Select \u201cAWS-Landing-Zone-CodePipeline\u201d - Select \u201cRelease Change\u201d - Click on the pipeline and confirm it successfully runs through to completion","title":"Confirm the ALZ CodePipeline is executing successfully."},{"location":"operations/operations-import-ALZAccount/#confirm-cli-access-and-setup-python-and-the-aws-python-sdk-boto3","text":"Confirm SSO temporary command line access from the management account with AdminAccess. - SSO login \u2192 Select linked account \u2192 \u201cCommand line or programmatic access\u201d - Select Option 2 and add to your AWS credentials file under \u201c[default]\u201c - This is required as the python script in step 3 takes a \u201cprofile\u201d parameter - Confirm you have the AWS CLI tool installed. - https://aws.amazon.com/cli/ - Confirm by running a command such as \u201caws s3 ls\u201d - Confirm you have python3 and the AWS python library (boto3) installed which is required in step 2 to confirm the account has been disassociated from the landing zone correctly. - https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html","title":"Confirm CLI access and setup Python and the AWS Python SDK (boto3)"},{"location":"operations/operations-import-ALZAccount/#landing-zone-disassociate-the-account-from-the-alz","text":"Login to the ALZ management account, and go to \u201cService Catalog\u201d -> \u201cProvisioned products\u201d Select \u201cAccess Filter\u201d -> \u201cAccount\u201d to see a list of the account products","title":"Landing Zone - Disassociate the account from the ALZ"},{"location":"operations/operations-import-ALZAccount/#select-the-product-for-the-specific-linked-account","text":"Put the linked account name in the provisioned products search bar This will narrow down the list and show a product name \u201cAWS-Landing-Zone-Account-Vending-Machine\u201d with a name \u201clz_applicaitons_ _ \u201d Select that product and then \u201cActions->Terminate\u201d","title":"Select the product for the specific linked account"},{"location":"operations/operations-import-ALZAccount/#confirm-the-product-successfully-terminates","text":"The provisioned product entry will show a status of \u201cUnder change\u201d You can also verify by going to CloudFormation\u2192Stacks and you will see \u201cDELETE IN PROGRESS\u201d for the AVM Template stack being deleted. Go to the Resources tab to see the deleted resources associated to this stack. Once the provisioned product no longer says \u201cUnder change\u201d move to the next step. Please note, this can take 1-2 hours.","title":"Confirm the product successfully terminates"},{"location":"operations/operations-import-ALZAccount/#go-to-the-linked-account-assume-role","text":"From the management account, assume the role \u201cAWSCloudFormationStackSetExecutionRole\u201d to the linked account or optionally, SSO with console access to that account","title":"Go to the linked account (assume role)"},{"location":"operations/operations-import-ALZAccount/#under-cloudformation-verify-that-the-alz-stacks-stacksets-from-alz-mgmt-were-deleted","text":"There should be no stack left in the linked account with the prefix \u201cStackSet-AWS-Landing-Zone-Baseline*\". For example: StackSet-AWS-Landing-Zone-Baseline-CentralizedLoggingSpoke- StackSet-AWS-Landing-Zone-Baseline-EnableConfigRules- StackSet-AWS-Landing-Zone-Baseline-EnableNotifications- StackSet-AWS-Landing-Zone-Baseline-EnableConfigRulesGlobal- StackSet-AWS-Landing-Zone-Baseline-EnableConfig- StackSet-AWS-Landing-Zone-Baseline-ConfigRole- StackSet-AWS-Landing-Zone-Baseline-IamPasswordPolicy- StackSet-AWS-Landing-Zone-Baseline-SecurityRoles- StackSet-AWS-Landing-Zone-Baseline-EnableCloudTrail-","title":"Under \u201cCloudFormation\u201d verify that the ALZ Stacks (StackSets from ALZ mgmt) were deleted"},{"location":"operations/operations-import-ALZAccount/#verify-that-the-account-is-ready-to-be-invited-and-baselined-by-the-asea","text":"You need to ensure that resources don\u2019t exist in the default VPC, there is no config recorder channel, no CloudTrail Trail and STS is active in all regions. This can be done manually, but ideally use this python script that can be run as well to automate the verification https://github.com/paulbayer/Inventory_Scripts/blob/mainline/ALZ_CheckAccount.py mkdir test; cd test git clone https://github.com/paulbayer/Inventory_Scripts.git python3 ALZ_CheckAccount.py -a LINKED ACCOUNT_HERE -p default It will run through 5 steps and output the following. If you were to run this script before the \u201cterminate\u201d step above is complete you would have warnings in steps 2 and 3 below. Step 0 completed without issues Checking account 111122223333 for default VPCs in any region Step 1 completed with no issues Checking account 111122223333 for a Config Recorders and Delivery Channels in any region Step 2 completed with no issues Checking account 111122223333 for a specially named CloudTrail in all regions Step 3 completed with no issues Checking account 111122223333 for any GuardDuty invites Step 4 completed with no issues Checking that the account is part of the AWS Organization. Step 5 completed with no issues We've found NO issues that would hinder the adoption of this account ****","title":"Verify that the account is ready to be invited and baselined by the ASEA"},{"location":"operations/operations-import-ALZAccount/#landing-zone-alz-remove-the-account-from-the-alz-organizations-and-make-standalone","text":"Removing the account from the ALZ organizations and making it standalone is required so it can be invited into the ASEA organization.","title":"Landing Zone (ALZ) - Remove the account from the ALZ organizations and make standalone"},{"location":"operations/operations-import-ALZAccount/#read-the-following-summaryconsiderations","text":"https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/","title":"Read the following summary/considerations"},{"location":"operations/operations-import-ALZAccount/#verify-access","text":"As stated in the previous sections, verify you have a mechanism to access the account post leaving the ALZ organization Former SSO roles will no longer function nor will the \u201cAWSCloudFormationStackSetExecutionRole\u201d role as it will have a trust relationship to the ALZ management account. Confirm the root credentials have been recovered and are usable As an alternative, confirm access with a new role/IAM user with Admin permissions on the account","title":"Verify access"},{"location":"operations/operations-import-ALZAccount/#verify-billing-flipped-to-invoicing","text":"As stated in the previous sections, verify the account payment method has been flipped to \u201cinvoicing\u201d to avoid having to enter a Credit Card when going standalone. This can be done working with your AWS account team who will coordinate internally, or by raising a support case describing the use case.","title":"Verify billing flipped to invoicing"},{"location":"operations/operations-import-ALZAccount/#remove-the-account-from-the-organizations-and-make-standalone","text":"Follow the instructions on the following link to remove the account The short version is select the account from the ALZ mgmt account Organizations and select \"remove\" https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.html https://aws.amazon.com/blogs/security/aws-organizations-now-supports-self-service-removal-of-accounts-from-an-organization Note, when moving the account standalone do not select Enterprise Support. You shouldn't get a popup dialog asking for a Credit Card and the Support level since the account should have been moved to invoicing. Support can be reenabled on the linked account once it\u2019s invited into the ASEA organization.","title":"Remove the account from the organizations and make standalone"},{"location":"operations/operations-import-ALZAccount/#accelerator-invite-the-account-into-its-organization","text":"","title":"Accelerator - Invite the account into its organization"},{"location":"operations/operations-import-ALZAccount/#from-the-asea-mgmt-account-send-an-invite-to-the-standalone-account","text":"Follow the instructions on the following link to invite the account The short version is go to the ASEA mgmt account organizations and select \"Add an account\" -> \"Invite existing account\" -> \"enter the linked account account ID\" https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html","title":"From the ASEA mgmt account, send an invite to the standalone account"},{"location":"operations/operations-import-ALZAccount/#in-the-former-alz-account-accept-the-invitation","text":"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html#orgs_manage_accounts_accept-decline-invite","title":"In the former ALZ account, Accept the invitation"},{"location":"operations/operations-import-ALZAccount/#keep-the-linked-account-at-the-root-level-of-the-organizations","text":"Verify access to the linked account using your root login credentials If you had created an IAM role/user with Admin permissions, then verify access as well","title":"Keep the linked account at the root level of the Organizations"},{"location":"operations/operations-import-ALZAccount/#activate-enterprise-support-es-on-this-linked-account","text":"If ES is enabled on the ASEA management account, open a support case to enable ES on this linked account Go to the Support center and create a billing support case with \"Account\" and \"Activation\" Subject \"Requesting ES enablement on linked account\" Body \"Requesting ES enablement on linked account \" Your AWS TAM can escalate the case with the support team if it\u2019s time sensitive. This is to make sure an ES support case can be created and escalated during the next steps if any unforeseen issue occurs.","title":"Activate Enterprise Support (ES) on this linked account"},{"location":"operations/operations-import-ALZAccount/#update-or-add-the-organization-adming-role-so-one-can-assume-the-role-into-the-linked-account","text":"Login to the linked account which just joined the organization. Create a new Organization Admin role, as defined in the customers config file: \"organization-admin-role\": \"OrganizationAccountAccessRole\". With newer customers the default is \"OrganizationAccountAccessRole, with older customers it is \"AWSCloudFormationStackSetExecutionRole\". If \"AWSCloudFormationStackSetExecutionRole\" then you can edit the trust relationship directly Go to IAM -> Role -> AWSCloudFormationStackSetExecutionRole Update the trust relationship to have the management account ID of the ASEA (instead of the account ID of the previous ALZ) Verify that you can assume this role from the management account into the linked account","title":"Update (or add) the Organization Adming Role so one can assume the role into the linked account"},{"location":"operations/operations-import-ALZAccount/#accelerator-move-the-linked-account-from-the-top-level-root-ou-into-the-appropriate-ou-managed-by-the-asea","text":"","title":"Accelerator - Move the linked account from the top level root OU into the appropriate OU managed by the ASEA"},{"location":"operations/operations-import-ALZAccount/#plan-what-ou-this-account-will-be-moved-into","text":"Option 1 - Create a new OU and move the account into that OU Before the migration, the team would have created a new OU (ie-similar to the sandbox OU). This would be needed if they need to isolate this account from TGW attachments/Networking and want to keep it isolated. The state machine will run and start to baseline the account. It will create a new VPC and deploy resources using CFN such as Config, CloudTrail, etc. Note, if the OU is setup similar to the sandbox OU it does not provide access to the shared VPCs that have the TGW attachments. Creating a new OU also requires adding that new OU and the OU persona to the config file in advance of the next state machine execution. Option 2 - Move account into an existing OU (ie-prod) The state machine will run and start to baseline the account. It will create a new VPC and deploy resources using CFN such as Config, CloudTrail, etc. The customers existing VPC will remain, as a 2nd DETACHED VPC. Mote. if it is non-compliant to security rules, it remains non-compliant and needs to be cleaned up and brought into compliance If the VPC is compliant and it has unique IP addresses, it could be attached to the TGW.","title":"Plan what OU this account will be moved into"},{"location":"operations/operations-import-ALZAccount/#move-the-account-from-the-root-ou-to-the-correct-ou","text":"THIS CANNOT BE EASILY UNDONE - MAKE SURE YOU MOVE TO THE CORRECT OU Follow the instructions on the following link to move the account to the correct OU The short version is go to the ASEA management account organizations and \"select the account\" -> \"actions\" -> \"move\" -> \"select the correct OU\" NOTE: The ASEA state machine will automatically start within 1-2 minutes of the account being moved into the OU https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html#move_account_to_ou Verify that the ASEA main state machine (under AWS->Step Functions) is triggered and runs cleanly (~30-45 minutes)","title":"Move the account from the root OU to the correct OU"},{"location":"operations/operations-import-ALZAccount/#accelerator-asea-verify-access-control-with-roles-sso-etc","text":"Update and verify SSO and permission sets for the linked account now part of the ASEA Verify you still have access to the linked account via root (or other mechanisms) Verify you still can assume the operations role into the linked account","title":"Accelerator (ASEA) - Verify access control with roles, SSO, etc"},{"location":"operations/operations-import-ALZAccount/#landing-zone-close-down-the-alz-core-accounts-and-then-the-management-account","text":"Once all workloads have been migrated from the ALZ to the ASEA, then you may decide to shutdown your ALZ.","title":"Landing Zone - Close down the ALZ core accounts and then the management account"},{"location":"operations/operations-import-ALZAccount/#close-down-the-alz-linked-accounts","text":"Close all the linked accounts \u201cas is\u201d without making them standalone This will be the ALZ core linked accounts, but you might have some remaining workload accounts you decided not to migrate to the ASEA. https://aws.amazon.com/premiumsupport/knowledge-center/close-aws-account The management account will remain with organizations and the core accounts will show as suspended for 90 days.","title":"Close down the ALZ linked accounts"},{"location":"operations/operations-import-ALZAccount/#close-down-the-alz-management-account","text":"After 90 days, the suspended linked accounts will be completely closed Go to the root account and turn off Organizations and then close the root account","title":"Close down the ALZ management account"},{"location":"operations/system-overview/","text":"System Overview \u00b6 The system can be thought of in two levels. The first level of the system consists of Accelerator stacks and resources. Let's call these the Accelerator-management resource. The second level of the system consists of stacks and resources that are deployed by the Accelerator-management resource. Let's call these the Accelerator-managed resources. The Accelerator-management resources are responsible for deploying the Accelerator-managed resources. There are two Accelerator-management stacks: the Installer stack that is responsible for creating the next listed stack; the Initial Setup stack. This stack is responsible for reading configuration file and creating Accelerator-managed resources in the relevant accounts. There are multiple Accelerator-managed stacks. Currently there are as many as twelve Accelerator-managed stacks per managed account. The figure below shows a zoomed-out overview of the Accelerator. The top of the overview shows the Accelerator-management resources, i.e. the Installer stack and the Initial Setup stack. The bottom of the overview shows the Accelerator-managed resources in the different accounts. Installer Stack \u00b6 The Accelerator-management Installer stack contains the necessary resources to deploy the Accelerator-management Initial Setup stack in an AWS account. This AWS account will be referred to as the 'root' account in this document. The Installer stack consists of the following resources: ASEA-InstallerPipeline : this is a AWS::CodePipeline::Pipeline that pulls the latest Accelerator code from GitHub. It launches the CodeBuild project ASEA-InstallerProject_pl , executes the ASEA-Installer-SaveApplicationVersion Lambda and launches the Accelerator state machine. ASEA-InstallerProject_pl : this is a AWS::CodeBuild::Project that installs the Accelerator in AWS account. ASEA-Installer-SaveApplicationVersion : this is a AWS::Lambda::Function that stores the current Accelerator version into Parameter Store. ASEA-Installer-StartExecution : this is a AWS::Lambda::Function that launches the Accelerator after CodeBuild deploys the Accelerator. Creation of AWS::DynamoDB::Table - ASEA-Parameters and ASEA-Outputs which are used for the internal operation of the Accelerator. ASEA-Outputs is used to share CloudFormation stack outputs between regions, stacks and phases. ASEA-Parameters is used to various configuration items like managed accounts, organizations structure, and limits. The ASEA-InstallerPipeline starts when first installed using the CloudFormation template. The administrator can also start the pipeline manually by clicking the Release Change button in the AWS Console. This starts the ASEA-InstallerProject_pl CodeBuild project. The CodeBuild project uses the GitHub source artifact. The CodeBuild projects spins up a new Linux instances and installs the Accelerator dependencies and starts the deployment of the Accelerator using the AWS Cloud Development Kit (CDK 1 ). CDK bootstraps its environment and creates the CDKToolkit stack in the AWS account. It creates the S3 bucket cdktoolkit-stagingbucket-* and the ECR repository aws-cdk/assets . CDK copies assets to the bootstrap bucket and bootstrap repository that are used by the Accelerator. The assets that are stored on S3 include default IAM policies, default SCPs, default firewall configuration. The assets that are pushed to ECR include the Accelerator Docker build image. This Docker image is responsible for deploying Accelerator resources using the CDK. CDK finally deploys the Initial Setup stack. The Accelerator state machine is described in the next section. This diagram depicts the Accelerator Installer CodePipeline as of v1.2.1: Once the Code Pipeline completes successfully: the Accelerator codebase was pulled from GitHub the Accelerator codebase was deployed/installed in the Organization Management (root) AWS account parameter store /accelerator/version was updated with the new version information this provides a full history of all Accelerator versions and upgrades the newly installed Accelerator state machine is started At this time the resources deployed by the Installer Stack are no longer required. The Installer stack could be removed (which would remove the Code Pipeline) with no impact on Accelerator functionality. If the Installer Stack was removed, it would need to be re-installed to upgrade the Accelerator. If the stack was not removed, an Accelerator codebase upgrade often only requires updating a single stack parameter to point to the latest Accelerator code branch, and re-releasing the pipeline. No files to manually copy, change or update, an upgrade can be initiated with a simple variable update. Initial Setup Stack \u00b6 The Accelerator-management Initial Setup stack, named ASEA-InitialSetup , consists of a state machine, named ASEA-MainStateMachine_sm , that executes various steps to create the Accelerator-managed stacks and resources in the Accelerator-managed accounts. Using a state machine, we can clearly define the deployment process and systematically control branches of execution and handle exceptions. The Accelerator comprises a primary state machine ASEA-MainStateMachine_sm , and nine supporting state machines (as of v1.2.1). Customer will only ever Execute the ASEA-MainStateMachine_sm . All troubleshooting will also typically begin with the ASEA-MainStateMachine_sm . The image below depicts the latest state ASEA-MainStateMachine_sm machine. Each green or white square in the image represents a step in the state machine. This all green diagram represents a successul Accelerator state machine execution. Notice the extremely linear state machine design. This was done to reduce complexity and ease troubleshooting. You may notice a small fork in the state machine. The left path is executed if the Accelerator is deployed on top of an ALZ, The right path is executed if the Accelerator is deployed as a standalone solution. If we eventually add a Control Tower deployment option, it is likely it could also leverage the existing ALZ path. The state machine contains three different types of steps: steps that execute a Lambda function; steps that start another state machine, e.g. Create Accounts step; steps that start another state machine that starts a CodeBuild project, e.g. the Phase {-1,0,1,2,3,4,5} steps. The stack additionally consists of the following resources: AWS::CodeBuild::Project ASEA-Deploy or ASEA-DeployPrebuilt AWS::CodeCommit::Repository ASEA-Config-Repo AWS::IAM::Role ASEA-L-SFN-MasterRole ASEA-L-SFN-Execution AWS::Lambda::Function A Lambda function for every Lambda function step in the state machine. AWS::StepFunctions::StateMachine ASEA-ALZCreateAccount_sm : See Create Landing Zone Account ; ASEA-OrgCreateAccount_sm : See Create Organization Account ; ASEA-InstallCfnRoleMaster_sm : See Install CloudFormation Execution Role ; ASEA-InstallRoles_sm : See Install Execution Roles ; ASEA-DeleteDefaultVpcs_sfn : See Delete Default VPCs ; ASEA-CodeBuild_sm : See Deploy Phase 0 ; ASEA-CreateConfigRecorder_sfn : See Create Config Recorders ; ASEA-CreateAdConnector_sm : See Create AD Connector ; ASEA-StoreOutputs_sm : See Share Outputs - new in v1.2.1. Note: Most resources have a random suffix to their name. This is because we use CDK to deploy the resources. See https://docs.aws.amazon.com/cdk/latest/guide/identifiers.html#identifiers_logical_ids Get or Create Configuration from S3 \u00b6 This step calls a Lambda function that finds or creates the configuration repository. Finds the configuration file(s) in the CodeCommit repository. If the configuration file cannot be found in the repository it is copied from the customer's S3 configuration bucket. If the copy is successful then the configuration file(s) in the S3 bucket will be removed. The configuration file config.json or config.yaml is parsed and validated. This step will fail if both file types exist, the configuration file is not valid JSON or YAML or does not adhere to the configuration file specification. Internally the Accelerator always leverages JSON, but accepts JSON or YAML as the source input file and converts it to JSON prior to each execution, storing the converted and fully expanded file if in the raw folder. Get Baseline from Configuration \u00b6 This step calls a Lambda function that gets the alz-baseline of the configuration file to decide which path in the state machine will be taken. Compare Configurations \u00b6 This step calls a Lambda function that compares the previous version of the configuration file with the current version of the configuration file. The previous configuration file CodeCommit commit id is stored in the secret accelerator/config/last-successful-commit in AWS Secrets Manager in the root account. The following configuration file changes are not allowed: changing ALZ baseline; changing root account or region; changing central log services account or region; changing the organizational unit, name or email address of an account; removing an account; changing the name, CIDR or region of a VPC; disabling a VPC; changing the name, availability zone, CIDR of a subnet; disabling or removing a subnet; changing the name, ASN, region or features of a transit gateway; changing the ID, VPC, subnet, region, size, DNS, Netbios of a Managed Active Directory; disabling a Managed Active Directory; changing the ASN of a virtual private gateway; changing the sharing to accounts of a VPC; changing the NACLs of a subnet. It is possible to ignore certain configuration file changes. See Restart the State Machine how to pass these options to the state machine. Load Landing Zone Configuration \u00b6 Executed only when using AWS Landing Zone baseline This step calls a Lambda function that validates the ALZ deployment and configuration. This step fails when an existing ALZ deployment in the root account can not be found; an organizational unit that is used by ALZ or the Accelerator can not be found; an account is present in the Accelerator configuration but does not exist in AWS Organizations; an account has a name, email address or organizational unit that is different from the ALZ configuration; an account has a name, email address or organizational unit that is different from the Accelerator configuration; the organizational units in the ALZ configuration are different from the ones in AWS Organizations; the account type of ALZ configuration accounts can not be detected; the accounts of type primary ', security , log-archive , shared-services are missing from the ALZ configuration. Add Execution Role to Service Catalog \u00b6 Executed only when using AWS Landing Zone baseline This step calls a Lambda function that adds the state machine's IAM role to the ALZ Account Vending Machine (AVM) service catalog portfolio. Create Landing Zone Account \u00b6 Executed only when using AWS Landing Zone baseline This step starts the ASEA-ALZCreateAccount_sm state machine. This state machine is responsible for creating an account using the AVM and waits for the account to be created and configured. Organizational Unit (OU) Validation \u00b6 Executed only when using AWS Organizations baseline This step validates that the OU structure defined in the Accelerator configuration file matches the OU and account structure defined in AWS Organizations. Missing OUs are created. If any AWS Accounts or OUs have been renamed, this step updates the configuration file with the latest information. Accounts located in the incorrect top-level OU cause a state machine failure. Load Organization Configuration \u00b6 Executed only when using AWS Organizations baseline This step calls a Lambda function that validates the Accelerator deployment and configuration. This step fails when an organizational unit that is used by the Accelerator can not be found; an account is present in the Accelerator configuration but does not exist in AWS Organizations; an account has a name, email address or organizational unit that is different from the Accelerator configuration. returns accounts that have not been created duplicates some validation functionality Install CloudFormation Role in root \u00b6 Executed only when using AWS Organizations baseline This step starts the ASEA-InstallCfnRoleMaster_sm state machine. This state machine is responsible for creating the IAM role defined in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ) in the root account. You can read more about why this role is created here . Create Organization Account \u00b6 Executed only when using AWS Organizations baseline This step starts the ASEA-OrgCreateAccount_sm state machine. This state machine is responsible for creating an account using the AWS Organizations and waits for the account to be created and configured. The newly created account will be moved to the correct organizational unit and will have an the ASEA-Quarantine-New-Object Service Control Policy (SCP) attached to it. The SCP will only be removed after the Accelerator state machine has completed. Application of this SCP ensures that new AWS accounts cannot be used until they have been properly configured and had all Accelerator guardrails applied per the configuration file. It is important that guardrails be consistently in place from the time a user first accesses a new AWS account, until the account is decommissioned. Should the state machine fail, still be in-progress, or has not yet processed the account (for whatever reason), users who access it will be prevented from doing anything . Load Organizational Units \u00b6 This step calls a Lambda function that loads all Organizational Units. The Organizational Units are stored in the DynamoDB table ASEA-Parameters in the root account. The Organizational Units will later be used to determine which configuration file options apply to an AWS account. Load Accounts \u00b6 This step calls a Lambda function that loads all accounts. The accounts are stored in the DynamoDB table ASEA-Parameters in the root account. The accounts will later be used to deploy stacks in the correct accounts. Install Execution Roles \u00b6 This step calls a Lambda function that creates stack sets in all Accelerator accounts. This stack sets contains a single resource, i.e. an IAM role ASEA-PipelineRole that can be assumed by the ASEA-L-SFN-MasterRole . This IAM role allows the root account to administer the Accelerator accounts. Delete Default VPCs \u00b6 This step starts the ASEA-DeleteDefaultVpcs_sfn state machine. This state machine is responsible for deleting default subnets, internet gateways and VPCs for all regions and accounts in the Accelerator configuration. This step fails when one or more default VPCs cannot be deleted. This step does not fail on the first error, it executes on all accounts/regions and then fails with a complete list of errors. Load Limits \u00b6 This step calls a Lambda function that loads service quotas and requests a service quota increase according to the configuration file. When a service quota increase request has been closed but not increased, then the service quota request will be issued again when the creation of the last request was at least two days ago. Enable Trusted Access for Services \u00b6 This step calls a Lambda function that is responsible for enabling AWS service access in the organization; enabling AWS Resource Access Manager sharing in the organization; creating a service-linked role for AWS IAM Access Analyzer; setting the security account as delegated administrator for AWS Firewall Manager; setting the security account as delegated administrator for AWS IAM Access Analyzer; setting the security account as delegated administrator for Amazon GuardDuty. Store All Phase Outputs \u00b6 This step only executes on the first run of the state machine after it has been upgraded to v1.2.0 or above. This step exists solely to support upgrades from Accelerator versions prior to v1.2.0 and can be removed when no existing customers are running versions older than v1.2.0. This steps populates the DynamoDB Outputs table with the outputs from previous executions which were previously stored in S3 (and at one time even stored in secrets manager). Deploy Phase -1 (Negative one) \u00b6 The following resources are deployed in phase -1: Creating required roles for macie custom resources Creating required roles for guardDuty custom resources Creating required roles for securityHub custom resources Creating required roles for IamCreateRole custom resource Creating required roles for createSSMDocument custom resource Creating required roles for createLogGroup custom resource Creating required roles for CWLCentralLoggingSubscriptionFilterRole custom resource Creating required roles for TransitGatewayCreatePeeringAttachment custom resource Creating required roles for TransitGatewayAcceptPeeringAttachment custom resource Creating required roles for createLogsMetricFilter custom resource Creating required roles for SnsSubscriberLambda custom resource Store Phase -1 Output \u00b6 This step calls a Lambda function that stores the outputs from the deployed stacks in sub-accounts in the DynamoDB table ASEA-Outputs in AWS Secrets Manager in the root account. Deploy Phase 0 \u00b6 This step starts the ASEA-CodeBuild_sm state machine. This state machine is responsible for starting a CodeBuild execution that deploys CloudFormation stacks in Accelerator-managed accounts using CDK. The same CodeBuild project is used to deploy all phases. The environment variable ACCELERATOR_PHASE in the CodeBuild execution decides which phase to deploy. The following resources are deployed in phase 0: create default EBS encryption key; create an AWS log bucket with encryption key; create the central log services bucket with encryption key; create the Accelerator configuration bucket with encryption key; copy artifacts to the Accelerator configuration bucket: SCPs; firewall configuration; account warming (step 1); set password policy (step 1); create IAM users (step 1): create passwords and store in Secrets Manager; create MAD deployment (step 1): create passwords and store in Secrets Manager; create service-linked role; create rsyslog deployment (step 1); create firewalls (step 1); create budgets (step 1); create transit gateways (step 1); create Route53 DNS logging log group; enable Macie (step 1); enable GuardDuty; enable Access Analyzer; Store Phase 0 Output \u00b6 This step calls a Lambda function that stores the outputs from the deployed stacks in sub-accounts in the secret ASEA-Outputs in AWS Secrets Manager in the root account. Verify Files \u00b6 This step verifies that all required files to complete the installation have been provided by the user. If any required files have not been provided, this step will fail and provide a list of all missing files. Why do we wait until so late in the state machine execution to perform this task? we do not want to add or delete files from a customer supplied bucket we do not want to force customers to need to copy and supply standard configuration files for 3. prescriptive installation we want to allow customers to override the sample or standard configuration files the config bucket is only created in Phase 0 we copy all the relevant sample files from reference-artifacts folders to our config bucket, and then, we copy any customer supplied config files over top of our sample files This allows customers to override the sample configs, with customer custom configs without needing to either fork the repo, or supply the files on a vanilla install Until we do this file copy, we cannot be sure that the combination of customer supplied and reference-artifact sample files provides all the required files. Create Config Recorders \u00b6 This step starts the ASEA-CreateConfigRecorder_sfn state machine. This state machine is responsible for creating Config recorders in all accounts and regions. Add SCPs to Organization \u00b6 This step calls a Lambda function that creates and attaches the SCPs listed in the Accelerator configuration. The SCP policies are loaded from the Accelerator configuration bucket. This step fails when an SCP policy cannot be found in the Accelerator configuration bucket; an SCP could not be attached to an organizational unit or account, e.g. when the maximum number of attached SCPs is exceeded Deploy Phase 1 \u00b6 Create S3 Bucket in all accounts and replicate to Log Account Bucket Deploy VPC: Vpc Subnets Subnet sharing (RAM) Route tables Internet gateways NAT gateways Interface endpoints Gateway endpoints Transit Gateway Attachments IAM Role required for VPC Peering Auto accept Firewall images subscription check Creates the customer gateways for the EIPs of the firewall Create IAM Roles, Users in account based on configuration Creates the additional budgets for the account stacks. Import Certificates Setup SSMSessionManagerDocument Create Cost and Usage reports Enable Macie in root Account GuardDuty setup in Security Account Setup CWL Central Logging Create Roles required for Flow Logs Transit Gateway Peering Create LogGroup required for DNS Logging Store Phase 1 Output \u00b6 See Deploy Phase 0 . Account Default Settings \u00b6 This step calls a Lambda function that enables and sets EBS default encryption for all accounts in the Accelerator configuration; enables S3 object level ALZ Cloudtrail logging; enables Log Insight events; enables KMS encryption using the CMK from the central logging account; sets AWS Systems Manager Session Manager default configuration in every Accelerator-managed account in every region with a VPC. Deploy Phase 2 \u00b6 Create CloudTrail in root account Create VPC Peering Connection Create Security Groups for shared VPC in sub accounts Setup Security Hub in Security Account Setup Cross Account CloudWatch logs sharing by creating roles in sub accounts Enable VPC FlowLogs Create Active Directory (MAD) Create Firewall clusters Create Firewall Management instance Create Transit Gateway Routes, Association and Propagation Enable Macie in Security account and Create Members, Update Config GuardDuty - Add existing Org accounts as members and allow new accounts to be members and Publish Create SNS Topics in Log Account TGW Peering Attachments Store Phase 2 Output \u00b6 See Deploy Phase 0 . Deploy Phase 3 \u00b6 create peering connection routes; create ALB (step 1); create rsyslog deployment (step 2); create hosted zones, resolver rules and resolver endpoints and Share; Enable Security Hub and Invite Sub accounts as members; TransitGateway Peering attachment and routes; Macie update Session; Store Phase 3 Output \u00b6 See Deploy Phase 0 . Deploy Phase 4 \u00b6 SecurityHub Disable Controls Creates CloudWatch Metrics on LogGroups Associate Shared Resolver Rules to VPC Associate Hosted Zones to VPC Store Phase 4 Output \u00b6 See Deploy Phase 0 . Associate Hosted Zones (Step removed in v1.2.1) \u00b6 This step calls a Lambda function that associates the private zones, all the interface endpoint zones, and the resolver rules with each VPC that leverages endpoint services. This step was removed in v1.2.1 of the Accelerator codebase. Add Tags to Shared Resources \u00b6 This step calls a Lambda function that adds tags to shared resources in the share destination account. For example, when a subnet is shared into another account, this step will add the Name tag to the subnet in the shared account. The supported resources are VPCs; subnets; security groups; transit gateway attachments. Enable Directory Sharing \u00b6 This step calls a Lambda function that shares Managed Active Directory according to the Accelerator configuration. The directory is shared from the source account to the target account. The directory will be accepted in the target account. Deploy Phase 5 \u00b6 create Remote Desktop Gateway; create launch configuration; create autoscaling group; enable central logging to S3 (step 2); Create CloudWatch Events for moveAccount, policyChanges and createAccount Creates CloudWatch Alarms Create AD Connector \u00b6 This step starts the ASEA-DeleteDefaultVpcs_sfn state machine. This state machine is responsible for creating AD connectors according to the Accelerator configuration. This step fails when one or more AD connectors failed to be created. Store Commit ID \u00b6 This step calls a Lambda function that stores the commit ID of the configuration file for which the state machine ran. Detach Quarantine SCP \u00b6 Executed only when using AWS Organizations baseline This step calls a Lambda function that stores the commit ID for which the state machine just ran.","title":"System Overview"},{"location":"operations/system-overview/#system-overview","text":"The system can be thought of in two levels. The first level of the system consists of Accelerator stacks and resources. Let's call these the Accelerator-management resource. The second level of the system consists of stacks and resources that are deployed by the Accelerator-management resource. Let's call these the Accelerator-managed resources. The Accelerator-management resources are responsible for deploying the Accelerator-managed resources. There are two Accelerator-management stacks: the Installer stack that is responsible for creating the next listed stack; the Initial Setup stack. This stack is responsible for reading configuration file and creating Accelerator-managed resources in the relevant accounts. There are multiple Accelerator-managed stacks. Currently there are as many as twelve Accelerator-managed stacks per managed account. The figure below shows a zoomed-out overview of the Accelerator. The top of the overview shows the Accelerator-management resources, i.e. the Installer stack and the Initial Setup stack. The bottom of the overview shows the Accelerator-managed resources in the different accounts.","title":"System Overview"},{"location":"operations/system-overview/#installer-stack","text":"The Accelerator-management Installer stack contains the necessary resources to deploy the Accelerator-management Initial Setup stack in an AWS account. This AWS account will be referred to as the 'root' account in this document. The Installer stack consists of the following resources: ASEA-InstallerPipeline : this is a AWS::CodePipeline::Pipeline that pulls the latest Accelerator code from GitHub. It launches the CodeBuild project ASEA-InstallerProject_pl , executes the ASEA-Installer-SaveApplicationVersion Lambda and launches the Accelerator state machine. ASEA-InstallerProject_pl : this is a AWS::CodeBuild::Project that installs the Accelerator in AWS account. ASEA-Installer-SaveApplicationVersion : this is a AWS::Lambda::Function that stores the current Accelerator version into Parameter Store. ASEA-Installer-StartExecution : this is a AWS::Lambda::Function that launches the Accelerator after CodeBuild deploys the Accelerator. Creation of AWS::DynamoDB::Table - ASEA-Parameters and ASEA-Outputs which are used for the internal operation of the Accelerator. ASEA-Outputs is used to share CloudFormation stack outputs between regions, stacks and phases. ASEA-Parameters is used to various configuration items like managed accounts, organizations structure, and limits. The ASEA-InstallerPipeline starts when first installed using the CloudFormation template. The administrator can also start the pipeline manually by clicking the Release Change button in the AWS Console. This starts the ASEA-InstallerProject_pl CodeBuild project. The CodeBuild project uses the GitHub source artifact. The CodeBuild projects spins up a new Linux instances and installs the Accelerator dependencies and starts the deployment of the Accelerator using the AWS Cloud Development Kit (CDK 1 ). CDK bootstraps its environment and creates the CDKToolkit stack in the AWS account. It creates the S3 bucket cdktoolkit-stagingbucket-* and the ECR repository aws-cdk/assets . CDK copies assets to the bootstrap bucket and bootstrap repository that are used by the Accelerator. The assets that are stored on S3 include default IAM policies, default SCPs, default firewall configuration. The assets that are pushed to ECR include the Accelerator Docker build image. This Docker image is responsible for deploying Accelerator resources using the CDK. CDK finally deploys the Initial Setup stack. The Accelerator state machine is described in the next section. This diagram depicts the Accelerator Installer CodePipeline as of v1.2.1: Once the Code Pipeline completes successfully: the Accelerator codebase was pulled from GitHub the Accelerator codebase was deployed/installed in the Organization Management (root) AWS account parameter store /accelerator/version was updated with the new version information this provides a full history of all Accelerator versions and upgrades the newly installed Accelerator state machine is started At this time the resources deployed by the Installer Stack are no longer required. The Installer stack could be removed (which would remove the Code Pipeline) with no impact on Accelerator functionality. If the Installer Stack was removed, it would need to be re-installed to upgrade the Accelerator. If the stack was not removed, an Accelerator codebase upgrade often only requires updating a single stack parameter to point to the latest Accelerator code branch, and re-releasing the pipeline. No files to manually copy, change or update, an upgrade can be initiated with a simple variable update.","title":"Installer Stack"},{"location":"operations/system-overview/#initial-setup-stack","text":"The Accelerator-management Initial Setup stack, named ASEA-InitialSetup , consists of a state machine, named ASEA-MainStateMachine_sm , that executes various steps to create the Accelerator-managed stacks and resources in the Accelerator-managed accounts. Using a state machine, we can clearly define the deployment process and systematically control branches of execution and handle exceptions. The Accelerator comprises a primary state machine ASEA-MainStateMachine_sm , and nine supporting state machines (as of v1.2.1). Customer will only ever Execute the ASEA-MainStateMachine_sm . All troubleshooting will also typically begin with the ASEA-MainStateMachine_sm . The image below depicts the latest state ASEA-MainStateMachine_sm machine. Each green or white square in the image represents a step in the state machine. This all green diagram represents a successul Accelerator state machine execution. Notice the extremely linear state machine design. This was done to reduce complexity and ease troubleshooting. You may notice a small fork in the state machine. The left path is executed if the Accelerator is deployed on top of an ALZ, The right path is executed if the Accelerator is deployed as a standalone solution. If we eventually add a Control Tower deployment option, it is likely it could also leverage the existing ALZ path. The state machine contains three different types of steps: steps that execute a Lambda function; steps that start another state machine, e.g. Create Accounts step; steps that start another state machine that starts a CodeBuild project, e.g. the Phase {-1,0,1,2,3,4,5} steps. The stack additionally consists of the following resources: AWS::CodeBuild::Project ASEA-Deploy or ASEA-DeployPrebuilt AWS::CodeCommit::Repository ASEA-Config-Repo AWS::IAM::Role ASEA-L-SFN-MasterRole ASEA-L-SFN-Execution AWS::Lambda::Function A Lambda function for every Lambda function step in the state machine. AWS::StepFunctions::StateMachine ASEA-ALZCreateAccount_sm : See Create Landing Zone Account ; ASEA-OrgCreateAccount_sm : See Create Organization Account ; ASEA-InstallCfnRoleMaster_sm : See Install CloudFormation Execution Role ; ASEA-InstallRoles_sm : See Install Execution Roles ; ASEA-DeleteDefaultVpcs_sfn : See Delete Default VPCs ; ASEA-CodeBuild_sm : See Deploy Phase 0 ; ASEA-CreateConfigRecorder_sfn : See Create Config Recorders ; ASEA-CreateAdConnector_sm : See Create AD Connector ; ASEA-StoreOutputs_sm : See Share Outputs - new in v1.2.1. Note: Most resources have a random suffix to their name. This is because we use CDK to deploy the resources. See https://docs.aws.amazon.com/cdk/latest/guide/identifiers.html#identifiers_logical_ids","title":"Initial Setup Stack"},{"location":"operations/system-overview/#get-or-create-configuration-from-s3","text":"This step calls a Lambda function that finds or creates the configuration repository. Finds the configuration file(s) in the CodeCommit repository. If the configuration file cannot be found in the repository it is copied from the customer's S3 configuration bucket. If the copy is successful then the configuration file(s) in the S3 bucket will be removed. The configuration file config.json or config.yaml is parsed and validated. This step will fail if both file types exist, the configuration file is not valid JSON or YAML or does not adhere to the configuration file specification. Internally the Accelerator always leverages JSON, but accepts JSON or YAML as the source input file and converts it to JSON prior to each execution, storing the converted and fully expanded file if in the raw folder.","title":"Get or Create Configuration from S3"},{"location":"operations/system-overview/#get-baseline-from-configuration","text":"This step calls a Lambda function that gets the alz-baseline of the configuration file to decide which path in the state machine will be taken.","title":"Get Baseline from Configuration"},{"location":"operations/system-overview/#compare-configurations","text":"This step calls a Lambda function that compares the previous version of the configuration file with the current version of the configuration file. The previous configuration file CodeCommit commit id is stored in the secret accelerator/config/last-successful-commit in AWS Secrets Manager in the root account. The following configuration file changes are not allowed: changing ALZ baseline; changing root account or region; changing central log services account or region; changing the organizational unit, name or email address of an account; removing an account; changing the name, CIDR or region of a VPC; disabling a VPC; changing the name, availability zone, CIDR of a subnet; disabling or removing a subnet; changing the name, ASN, region or features of a transit gateway; changing the ID, VPC, subnet, region, size, DNS, Netbios of a Managed Active Directory; disabling a Managed Active Directory; changing the ASN of a virtual private gateway; changing the sharing to accounts of a VPC; changing the NACLs of a subnet. It is possible to ignore certain configuration file changes. See Restart the State Machine how to pass these options to the state machine.","title":"Compare Configurations"},{"location":"operations/system-overview/#load-landing-zone-configuration","text":"Executed only when using AWS Landing Zone baseline This step calls a Lambda function that validates the ALZ deployment and configuration. This step fails when an existing ALZ deployment in the root account can not be found; an organizational unit that is used by ALZ or the Accelerator can not be found; an account is present in the Accelerator configuration but does not exist in AWS Organizations; an account has a name, email address or organizational unit that is different from the ALZ configuration; an account has a name, email address or organizational unit that is different from the Accelerator configuration; the organizational units in the ALZ configuration are different from the ones in AWS Organizations; the account type of ALZ configuration accounts can not be detected; the accounts of type primary ', security , log-archive , shared-services are missing from the ALZ configuration.","title":"Load Landing Zone Configuration"},{"location":"operations/system-overview/#add-execution-role-to-service-catalog","text":"Executed only when using AWS Landing Zone baseline This step calls a Lambda function that adds the state machine's IAM role to the ALZ Account Vending Machine (AVM) service catalog portfolio.","title":"Add Execution Role to Service Catalog"},{"location":"operations/system-overview/#create-landing-zone-account","text":"Executed only when using AWS Landing Zone baseline This step starts the ASEA-ALZCreateAccount_sm state machine. This state machine is responsible for creating an account using the AVM and waits for the account to be created and configured.","title":"Create Landing Zone Account"},{"location":"operations/system-overview/#organizational-unit-ou-validation","text":"Executed only when using AWS Organizations baseline This step validates that the OU structure defined in the Accelerator configuration file matches the OU and account structure defined in AWS Organizations. Missing OUs are created. If any AWS Accounts or OUs have been renamed, this step updates the configuration file with the latest information. Accounts located in the incorrect top-level OU cause a state machine failure.","title":"Organizational Unit (OU) Validation"},{"location":"operations/system-overview/#load-organization-configuration","text":"Executed only when using AWS Organizations baseline This step calls a Lambda function that validates the Accelerator deployment and configuration. This step fails when an organizational unit that is used by the Accelerator can not be found; an account is present in the Accelerator configuration but does not exist in AWS Organizations; an account has a name, email address or organizational unit that is different from the Accelerator configuration. returns accounts that have not been created duplicates some validation functionality","title":"Load Organization Configuration"},{"location":"operations/system-overview/#install-cloudformation-role-in-root","text":"Executed only when using AWS Organizations baseline This step starts the ASEA-InstallCfnRoleMaster_sm state machine. This state machine is responsible for creating the IAM role defined in organization-admin-role (default: AWSCloudFormationStackSetAdministrationRole ) in the root account. You can read more about why this role is created here .","title":"Install CloudFormation Role in root"},{"location":"operations/system-overview/#create-organization-account","text":"Executed only when using AWS Organizations baseline This step starts the ASEA-OrgCreateAccount_sm state machine. This state machine is responsible for creating an account using the AWS Organizations and waits for the account to be created and configured. The newly created account will be moved to the correct organizational unit and will have an the ASEA-Quarantine-New-Object Service Control Policy (SCP) attached to it. The SCP will only be removed after the Accelerator state machine has completed. Application of this SCP ensures that new AWS accounts cannot be used until they have been properly configured and had all Accelerator guardrails applied per the configuration file. It is important that guardrails be consistently in place from the time a user first accesses a new AWS account, until the account is decommissioned. Should the state machine fail, still be in-progress, or has not yet processed the account (for whatever reason), users who access it will be prevented from doing anything .","title":"Create Organization Account"},{"location":"operations/system-overview/#load-organizational-units","text":"This step calls a Lambda function that loads all Organizational Units. The Organizational Units are stored in the DynamoDB table ASEA-Parameters in the root account. The Organizational Units will later be used to determine which configuration file options apply to an AWS account.","title":"Load Organizational Units"},{"location":"operations/system-overview/#load-accounts","text":"This step calls a Lambda function that loads all accounts. The accounts are stored in the DynamoDB table ASEA-Parameters in the root account. The accounts will later be used to deploy stacks in the correct accounts.","title":"Load Accounts"},{"location":"operations/system-overview/#install-execution-roles","text":"This step calls a Lambda function that creates stack sets in all Accelerator accounts. This stack sets contains a single resource, i.e. an IAM role ASEA-PipelineRole that can be assumed by the ASEA-L-SFN-MasterRole . This IAM role allows the root account to administer the Accelerator accounts.","title":"Install Execution Roles"},{"location":"operations/system-overview/#delete-default-vpcs","text":"This step starts the ASEA-DeleteDefaultVpcs_sfn state machine. This state machine is responsible for deleting default subnets, internet gateways and VPCs for all regions and accounts in the Accelerator configuration. This step fails when one or more default VPCs cannot be deleted. This step does not fail on the first error, it executes on all accounts/regions and then fails with a complete list of errors.","title":"Delete Default VPCs"},{"location":"operations/system-overview/#load-limits","text":"This step calls a Lambda function that loads service quotas and requests a service quota increase according to the configuration file. When a service quota increase request has been closed but not increased, then the service quota request will be issued again when the creation of the last request was at least two days ago.","title":"Load Limits"},{"location":"operations/system-overview/#enable-trusted-access-for-services","text":"This step calls a Lambda function that is responsible for enabling AWS service access in the organization; enabling AWS Resource Access Manager sharing in the organization; creating a service-linked role for AWS IAM Access Analyzer; setting the security account as delegated administrator for AWS Firewall Manager; setting the security account as delegated administrator for AWS IAM Access Analyzer; setting the security account as delegated administrator for Amazon GuardDuty.","title":"Enable Trusted Access for Services"},{"location":"operations/system-overview/#store-all-phase-outputs","text":"This step only executes on the first run of the state machine after it has been upgraded to v1.2.0 or above. This step exists solely to support upgrades from Accelerator versions prior to v1.2.0 and can be removed when no existing customers are running versions older than v1.2.0. This steps populates the DynamoDB Outputs table with the outputs from previous executions which were previously stored in S3 (and at one time even stored in secrets manager).","title":"Store All Phase Outputs"},{"location":"operations/system-overview/#deploy-phase-1-negative-one","text":"The following resources are deployed in phase -1: Creating required roles for macie custom resources Creating required roles for guardDuty custom resources Creating required roles for securityHub custom resources Creating required roles for IamCreateRole custom resource Creating required roles for createSSMDocument custom resource Creating required roles for createLogGroup custom resource Creating required roles for CWLCentralLoggingSubscriptionFilterRole custom resource Creating required roles for TransitGatewayCreatePeeringAttachment custom resource Creating required roles for TransitGatewayAcceptPeeringAttachment custom resource Creating required roles for createLogsMetricFilter custom resource Creating required roles for SnsSubscriberLambda custom resource","title":"Deploy Phase -1 (Negative one)"},{"location":"operations/system-overview/#store-phase-1-output","text":"This step calls a Lambda function that stores the outputs from the deployed stacks in sub-accounts in the DynamoDB table ASEA-Outputs in AWS Secrets Manager in the root account.","title":"Store Phase -1 Output"},{"location":"operations/system-overview/#deploy-phase-0","text":"This step starts the ASEA-CodeBuild_sm state machine. This state machine is responsible for starting a CodeBuild execution that deploys CloudFormation stacks in Accelerator-managed accounts using CDK. The same CodeBuild project is used to deploy all phases. The environment variable ACCELERATOR_PHASE in the CodeBuild execution decides which phase to deploy. The following resources are deployed in phase 0: create default EBS encryption key; create an AWS log bucket with encryption key; create the central log services bucket with encryption key; create the Accelerator configuration bucket with encryption key; copy artifacts to the Accelerator configuration bucket: SCPs; firewall configuration; account warming (step 1); set password policy (step 1); create IAM users (step 1): create passwords and store in Secrets Manager; create MAD deployment (step 1): create passwords and store in Secrets Manager; create service-linked role; create rsyslog deployment (step 1); create firewalls (step 1); create budgets (step 1); create transit gateways (step 1); create Route53 DNS logging log group; enable Macie (step 1); enable GuardDuty; enable Access Analyzer;","title":"Deploy Phase 0"},{"location":"operations/system-overview/#store-phase-0-output","text":"This step calls a Lambda function that stores the outputs from the deployed stacks in sub-accounts in the secret ASEA-Outputs in AWS Secrets Manager in the root account.","title":"Store Phase 0 Output"},{"location":"operations/system-overview/#verify-files","text":"This step verifies that all required files to complete the installation have been provided by the user. If any required files have not been provided, this step will fail and provide a list of all missing files. Why do we wait until so late in the state machine execution to perform this task? we do not want to add or delete files from a customer supplied bucket we do not want to force customers to need to copy and supply standard configuration files for 3. prescriptive installation we want to allow customers to override the sample or standard configuration files the config bucket is only created in Phase 0 we copy all the relevant sample files from reference-artifacts folders to our config bucket, and then, we copy any customer supplied config files over top of our sample files This allows customers to override the sample configs, with customer custom configs without needing to either fork the repo, or supply the files on a vanilla install Until we do this file copy, we cannot be sure that the combination of customer supplied and reference-artifact sample files provides all the required files.","title":"Verify Files"},{"location":"operations/system-overview/#create-config-recorders","text":"This step starts the ASEA-CreateConfigRecorder_sfn state machine. This state machine is responsible for creating Config recorders in all accounts and regions.","title":"Create Config Recorders"},{"location":"operations/system-overview/#add-scps-to-organization","text":"This step calls a Lambda function that creates and attaches the SCPs listed in the Accelerator configuration. The SCP policies are loaded from the Accelerator configuration bucket. This step fails when an SCP policy cannot be found in the Accelerator configuration bucket; an SCP could not be attached to an organizational unit or account, e.g. when the maximum number of attached SCPs is exceeded","title":"Add SCPs to Organization"},{"location":"operations/system-overview/#deploy-phase-1","text":"Create S3 Bucket in all accounts and replicate to Log Account Bucket Deploy VPC: Vpc Subnets Subnet sharing (RAM) Route tables Internet gateways NAT gateways Interface endpoints Gateway endpoints Transit Gateway Attachments IAM Role required for VPC Peering Auto accept Firewall images subscription check Creates the customer gateways for the EIPs of the firewall Create IAM Roles, Users in account based on configuration Creates the additional budgets for the account stacks. Import Certificates Setup SSMSessionManagerDocument Create Cost and Usage reports Enable Macie in root Account GuardDuty setup in Security Account Setup CWL Central Logging Create Roles required for Flow Logs Transit Gateway Peering Create LogGroup required for DNS Logging","title":"Deploy Phase 1"},{"location":"operations/system-overview/#store-phase-1-output_1","text":"See Deploy Phase 0 .","title":"Store Phase 1 Output"},{"location":"operations/system-overview/#account-default-settings","text":"This step calls a Lambda function that enables and sets EBS default encryption for all accounts in the Accelerator configuration; enables S3 object level ALZ Cloudtrail logging; enables Log Insight events; enables KMS encryption using the CMK from the central logging account; sets AWS Systems Manager Session Manager default configuration in every Accelerator-managed account in every region with a VPC.","title":"Account Default Settings"},{"location":"operations/system-overview/#deploy-phase-2","text":"Create CloudTrail in root account Create VPC Peering Connection Create Security Groups for shared VPC in sub accounts Setup Security Hub in Security Account Setup Cross Account CloudWatch logs sharing by creating roles in sub accounts Enable VPC FlowLogs Create Active Directory (MAD) Create Firewall clusters Create Firewall Management instance Create Transit Gateway Routes, Association and Propagation Enable Macie in Security account and Create Members, Update Config GuardDuty - Add existing Org accounts as members and allow new accounts to be members and Publish Create SNS Topics in Log Account TGW Peering Attachments","title":"Deploy Phase 2"},{"location":"operations/system-overview/#store-phase-2-output","text":"See Deploy Phase 0 .","title":"Store Phase 2 Output"},{"location":"operations/system-overview/#deploy-phase-3","text":"create peering connection routes; create ALB (step 1); create rsyslog deployment (step 2); create hosted zones, resolver rules and resolver endpoints and Share; Enable Security Hub and Invite Sub accounts as members; TransitGateway Peering attachment and routes; Macie update Session;","title":"Deploy Phase 3"},{"location":"operations/system-overview/#store-phase-3-output","text":"See Deploy Phase 0 .","title":"Store Phase 3 Output"},{"location":"operations/system-overview/#deploy-phase-4","text":"SecurityHub Disable Controls Creates CloudWatch Metrics on LogGroups Associate Shared Resolver Rules to VPC Associate Hosted Zones to VPC","title":"Deploy Phase 4"},{"location":"operations/system-overview/#store-phase-4-output","text":"See Deploy Phase 0 .","title":"Store Phase 4 Output"},{"location":"operations/system-overview/#associate-hosted-zones-step-removed-in-v121","text":"This step calls a Lambda function that associates the private zones, all the interface endpoint zones, and the resolver rules with each VPC that leverages endpoint services. This step was removed in v1.2.1 of the Accelerator codebase.","title":"Associate Hosted Zones (Step removed in v1.2.1)"},{"location":"operations/system-overview/#add-tags-to-shared-resources","text":"This step calls a Lambda function that adds tags to shared resources in the share destination account. For example, when a subnet is shared into another account, this step will add the Name tag to the subnet in the shared account. The supported resources are VPCs; subnets; security groups; transit gateway attachments.","title":"Add Tags to Shared Resources"},{"location":"operations/system-overview/#enable-directory-sharing","text":"This step calls a Lambda function that shares Managed Active Directory according to the Accelerator configuration. The directory is shared from the source account to the target account. The directory will be accepted in the target account.","title":"Enable Directory Sharing"},{"location":"operations/system-overview/#deploy-phase-5","text":"create Remote Desktop Gateway; create launch configuration; create autoscaling group; enable central logging to S3 (step 2); Create CloudWatch Events for moveAccount, policyChanges and createAccount Creates CloudWatch Alarms","title":"Deploy Phase 5"},{"location":"operations/system-overview/#create-ad-connector","text":"This step starts the ASEA-DeleteDefaultVpcs_sfn state machine. This state machine is responsible for creating AD connectors according to the Accelerator configuration. This step fails when one or more AD connectors failed to be created.","title":"Create AD Connector"},{"location":"operations/system-overview/#store-commit-id","text":"This step calls a Lambda function that stores the commit ID of the configuration file for which the state machine ran.","title":"Store Commit ID"},{"location":"operations/system-overview/#detach-quarantine-scp","text":"Executed only when using AWS Organizations baseline This step calls a Lambda function that stores the commit ID for which the state machine just ran.","title":"Detach Quarantine SCP"},{"location":"operations/troubleshooting/","text":"Troubleshooting \u00b6 Issues could occur in different parts of the Accelerator. We'll guide you through troubleshooting these issues in this section. Components \u00b6 State Machine \u00b6 Viewing the step function Graph inspector (depicted above in 2.2), the majority of the main state machine has a large colored box around which is the functionality to catch state machine failures Main Try Catch block to Notify users . This large outer box will be blue while the state machine is still executing, it will be green upon a successful state machine execution and will turn orange/yellow on a state machine failure. What if my State Machine fails? Why? Previous solutions had complex recovery processes, what's involved? If your main state machine fails, review the error(s), resolve the problem and simply re-run the state machine. We've put a huge focus on ensuring the solution is idempotent and to ensure recovery is a smooth and easy process. Ensuring the integrity of deployed guardrails is critical in operating and maintaining an environment hosting protected data. Based on customer feedback and security best practices, we purposely fail the state machine if we cannot successfully deploy guardrails. Additionally, with millions of active customers each supporting different and diverse use cases and with the rapid rate of evolution of the AWS platform, sometimes we will encounter unexpected circumstances and the state machine might fail. We've spent a lot of time over the course of the Accelerator development process ensuring the solution can roll forward, roll backward, be stopped, restarted, and rerun without issues. A huge focus was placed on dealing with and writing custom code to manage and deal with non-idempotent resources (like S3 buckets, log groups, KMS keys, etc.). We've spent a lot of time ensuring that any failed artifacts are automatically cleaned up and don't cause subsequent executions to fail. We've put a strong focus on ensuring you do not need to go into your various AWS sub-accounts and manually remove or cleanup resources or deployment failures. We've also tried to provide usable error messages that are easy to understand and troubleshoot. As new scenario's are brought to our attention, we continue to adjust the codebase to better handle these situations. Will your state machine fail at some point in time, likely. Will you be able to easily recover and move forward without extensive time and effort, YES! As the state machine executes, each step will turn from white (not started), to blue (executing), to green (Success), or grey/red (failure). To diagnose the problem select the grey/red step that failed. If you miss the step and select the outer box, you will have selected the Main Try Catch block to Notify users . You need to carefully select the failed step. As stated in section 2.2, the state machine contains 3 different types of states, which are each diagnosed differently. a. If the step is calling a Lambda function then you will see the following after clicking the failed step. In this case, you can see that the Cause section contains a useful message. This message will differ between Lambda functions. In case this message does not make the issue clear, you can click on the CloudWatch Logs link in the Resource section to view the output of the Lambda function that was called by the step. See the section CloudWatch Logs . Note: The Resource section contains two links that blend together. You need to click the second link ( CloudWatch Logs ), not the first link which will open the actual resource/Lambda. b. In case the failed step started another state machine, you will see the following after clicking the failed step. To view the state machine execution that failed you can click the link in the Resource section. In case the failed step started the CodeBuild state machine, ASEA-CodeBuild_sm , you will be able to see the CodeBuild project and execution ID that failed by looking at the output of the Start Build step in the ASEA-CodeBuild_sm state machine. . In the image above the execution of CodeBuild project ASEA-DeployPrebuilt with ID ASEA-DeployPrebuilt:717584a9-c406-4569-9cc2-0d23e9ff9ef0 failed. See the CodeBuild section to troubleshoot. CodeBuild \u00b6 The Accelerator deploys and leverages two CodeBuild projects. The ASEA-InstallerProject_pl project is used by the Code Pipeline/Installer stack and ASEA-DeployPrebuilt which is used throughout the Accelerator state machine. Both are similar in that they use CDK to deploy stacks. The installer project will not exist, if the installer has been removed. After a successful installation you will see the following in Codebuild, for the ASEA-DeployPrebuilt project: When an error occurs you will see that the CodeBuild project execution fails when looking in the execution overview. You can click on the name of the CodeBuild execution and then look inside the logs what caused the failure. These logs can be confusing. We are deploying multiple stacks in parallel and all the messages for all the parallel deployments are interleaved together, so make sure you are correlating the events back to the correct event source. Because we are deploying to 16 regions in parallel, you will also see messages for the same stack deployment interleaved. Even though a task may indicate it is complete and then another seemingly identical task indicates in-progress, the second message is coming from one of the alternate regions. You can for example see the error message The stack named ASEA-Perimeter-Phase2 is in a failed state: UPDATE_ROLLBACK_COMPLETE . This means the stack ASEA-Perimeter-Phase2 failed to update and it had to rollback. The error indicated at the bottom of the Codebuild screen is typically NOT the cause of the failure, just the end result. You need to scroll up and find the FIRST occurrence of an error in the log file. Often starting at the top of the log file and searching for the text FAIL (case sensitive), will allow you to find the relevant error message(s) quickly. The failure is typically listed in the CloudFormation update logs. In this example we can see that the resource FirewallManager failed to create through CloudFormation. One way to solve this issue is to deprovision the firewall manager in the configuration file and then run the state machine. Next, provision the firewall manager and run the state machine again. If the error message is not clear, or the error occurred in a nested stack, then a more detailed error will be available in the CloudFormation stack events. See the CloudFormation section below. CloudFormation \u00b6 In case you want to troubleshoot errors that occurred in CloudFormation, the best way is to look in the CloudFormation stack's events. This requires you to assume a role into the relevant sub-account, and to locate the relevant failed, rolled-back, or deleted stack. Unfortunately, we are unable to log the region of the error message, so depending on what's being deployed, you may need to search all 16 regions for the failed stack. When a native resource fails to create or update there are no additional logs available except what is displayed in the Status reason column. When a custom resource fails to create or update -- i.e. not a native CloudFormation resource but a resource backed by a custom Lambda function -- then we can find additional logs in CloudWatch. Often the stack failure occurrs in a managed account instead of the root account. See Switch To a Managed Account to switch to the CloudFormation console in the managed account. Custom Resource \u00b6 Custom resources are backed by a Lambda function that implements the creation, modification or deletion or the resource. Every Lambda function has a CloudWatch log group that contains logs about the custom resource creation. To troubleshoot errors in custom resource, you need to check the custom resource's log group. Example custom resource log group names: /aws/lambda/ASEA-Master-Phase1-CustomCurReportDefinitionL-14IHLQCC1LY8L /aws/lambda/ASEA-Master-Phase2-AWS679f53fac002430cb0da5b7-Z75Q4GG9LIV5 /aws/lambda/ASEA-Operations-Phas-AWS679f53fac002430cb0da5-HMV2YF6OKJET /aws/lambda/ASEA-Operations-Phas-CustomGetDetectorIdLambd-HEM07DR0DOOJ CloudWatch \u00b6 When you arrived in CloudWatch logs by clicking on the state machine's step CloudWatch Logs link you will immediately see the list of log streams. Every log stream represents an instance of the Lambda function. You can find errors in multiple log groups using CloudWatch Log Insights. fields @timestamp, @message | sort @timestamp desc | filter strcontains(@message, 'ERROR') | limit 100 CodePipeline \u00b6 \"Internal Failure\" incorrect Github token, repo or branch Examples \u00b6 Lets walk through a couple of example: Example 1 \u00b6 State Machine failed (Lambda), click on the grey box, then click on the Resource object: Click on the red failed box, click on Step Input . The error is clearly indicated, we could not delete a Default VPC because the default VPC had dependencies, in a specified account and region. In this case several dependencies exist and need to be cleaned up to proceed (EIP's and something less obvious like security groups). Example 2 \u00b6 In the next example the state machine failed (sub-state machine) on the create accounts step. In this case rather than clicking on the Graph inspector we are going to scroll down through the Execution event history underneath the Graph inspector. We are going to find the FIRST failed task from the top of the list and then select the state machine from the prior task: We will then click onn the red failed box, select Exception and we can see a clear error message - we have exceeded the maximum number of AWS accounts allowed in your organization: Alternatively, in case the Exception error is not clear, we can select Details and then select CloudWatch logs for the end of the Resource section: If you open the latest log stream in the opened log group ( /aws/lambda/ASEA-InitialSetup-StartAccountCreationHandler-1IZ2N4EP29D72 ) and review the last several messages in the stream, the following clear message also appears: Example 3 \u00b6 In the next example the state machine failed in one of the CodeBuild state machine steps, based on the Resource name of the failed step. Rather than tracing this failure through the sub-state machine and then into the failed CodeBuild task, we are simply going to open AWS CodeBuild, and open the ASEA-DeployPrebuilt task. The failed task should be on the top of the Codebuild build run list. Open the build job. Using your browser, from the top of the page, search for \"FAIL\", and we are immediately brought to the error. In this particular case we had an issue with the creation of VPC endpoints. We defined something not supported by the current configuration file. The solution was to simply remove the offending endpoints from the config file and re-run the state machine.","title":"Troubleshooting"},{"location":"operations/troubleshooting/#troubleshooting","text":"Issues could occur in different parts of the Accelerator. We'll guide you through troubleshooting these issues in this section.","title":"Troubleshooting"},{"location":"operations/troubleshooting/#components","text":"","title":"Components"},{"location":"operations/troubleshooting/#state-machine","text":"Viewing the step function Graph inspector (depicted above in 2.2), the majority of the main state machine has a large colored box around which is the functionality to catch state machine failures Main Try Catch block to Notify users . This large outer box will be blue while the state machine is still executing, it will be green upon a successful state machine execution and will turn orange/yellow on a state machine failure. What if my State Machine fails? Why? Previous solutions had complex recovery processes, what's involved? If your main state machine fails, review the error(s), resolve the problem and simply re-run the state machine. We've put a huge focus on ensuring the solution is idempotent and to ensure recovery is a smooth and easy process. Ensuring the integrity of deployed guardrails is critical in operating and maintaining an environment hosting protected data. Based on customer feedback and security best practices, we purposely fail the state machine if we cannot successfully deploy guardrails. Additionally, with millions of active customers each supporting different and diverse use cases and with the rapid rate of evolution of the AWS platform, sometimes we will encounter unexpected circumstances and the state machine might fail. We've spent a lot of time over the course of the Accelerator development process ensuring the solution can roll forward, roll backward, be stopped, restarted, and rerun without issues. A huge focus was placed on dealing with and writing custom code to manage and deal with non-idempotent resources (like S3 buckets, log groups, KMS keys, etc.). We've spent a lot of time ensuring that any failed artifacts are automatically cleaned up and don't cause subsequent executions to fail. We've put a strong focus on ensuring you do not need to go into your various AWS sub-accounts and manually remove or cleanup resources or deployment failures. We've also tried to provide usable error messages that are easy to understand and troubleshoot. As new scenario's are brought to our attention, we continue to adjust the codebase to better handle these situations. Will your state machine fail at some point in time, likely. Will you be able to easily recover and move forward without extensive time and effort, YES! As the state machine executes, each step will turn from white (not started), to blue (executing), to green (Success), or grey/red (failure). To diagnose the problem select the grey/red step that failed. If you miss the step and select the outer box, you will have selected the Main Try Catch block to Notify users . You need to carefully select the failed step. As stated in section 2.2, the state machine contains 3 different types of states, which are each diagnosed differently. a. If the step is calling a Lambda function then you will see the following after clicking the failed step. In this case, you can see that the Cause section contains a useful message. This message will differ between Lambda functions. In case this message does not make the issue clear, you can click on the CloudWatch Logs link in the Resource section to view the output of the Lambda function that was called by the step. See the section CloudWatch Logs . Note: The Resource section contains two links that blend together. You need to click the second link ( CloudWatch Logs ), not the first link which will open the actual resource/Lambda. b. In case the failed step started another state machine, you will see the following after clicking the failed step. To view the state machine execution that failed you can click the link in the Resource section. In case the failed step started the CodeBuild state machine, ASEA-CodeBuild_sm , you will be able to see the CodeBuild project and execution ID that failed by looking at the output of the Start Build step in the ASEA-CodeBuild_sm state machine. . In the image above the execution of CodeBuild project ASEA-DeployPrebuilt with ID ASEA-DeployPrebuilt:717584a9-c406-4569-9cc2-0d23e9ff9ef0 failed. See the CodeBuild section to troubleshoot.","title":"State Machine"},{"location":"operations/troubleshooting/#codebuild","text":"The Accelerator deploys and leverages two CodeBuild projects. The ASEA-InstallerProject_pl project is used by the Code Pipeline/Installer stack and ASEA-DeployPrebuilt which is used throughout the Accelerator state machine. Both are similar in that they use CDK to deploy stacks. The installer project will not exist, if the installer has been removed. After a successful installation you will see the following in Codebuild, for the ASEA-DeployPrebuilt project: When an error occurs you will see that the CodeBuild project execution fails when looking in the execution overview. You can click on the name of the CodeBuild execution and then look inside the logs what caused the failure. These logs can be confusing. We are deploying multiple stacks in parallel and all the messages for all the parallel deployments are interleaved together, so make sure you are correlating the events back to the correct event source. Because we are deploying to 16 regions in parallel, you will also see messages for the same stack deployment interleaved. Even though a task may indicate it is complete and then another seemingly identical task indicates in-progress, the second message is coming from one of the alternate regions. You can for example see the error message The stack named ASEA-Perimeter-Phase2 is in a failed state: UPDATE_ROLLBACK_COMPLETE . This means the stack ASEA-Perimeter-Phase2 failed to update and it had to rollback. The error indicated at the bottom of the Codebuild screen is typically NOT the cause of the failure, just the end result. You need to scroll up and find the FIRST occurrence of an error in the log file. Often starting at the top of the log file and searching for the text FAIL (case sensitive), will allow you to find the relevant error message(s) quickly. The failure is typically listed in the CloudFormation update logs. In this example we can see that the resource FirewallManager failed to create through CloudFormation. One way to solve this issue is to deprovision the firewall manager in the configuration file and then run the state machine. Next, provision the firewall manager and run the state machine again. If the error message is not clear, or the error occurred in a nested stack, then a more detailed error will be available in the CloudFormation stack events. See the CloudFormation section below.","title":"CodeBuild"},{"location":"operations/troubleshooting/#cloudformation","text":"In case you want to troubleshoot errors that occurred in CloudFormation, the best way is to look in the CloudFormation stack's events. This requires you to assume a role into the relevant sub-account, and to locate the relevant failed, rolled-back, or deleted stack. Unfortunately, we are unable to log the region of the error message, so depending on what's being deployed, you may need to search all 16 regions for the failed stack. When a native resource fails to create or update there are no additional logs available except what is displayed in the Status reason column. When a custom resource fails to create or update -- i.e. not a native CloudFormation resource but a resource backed by a custom Lambda function -- then we can find additional logs in CloudWatch. Often the stack failure occurrs in a managed account instead of the root account. See Switch To a Managed Account to switch to the CloudFormation console in the managed account.","title":"CloudFormation"},{"location":"operations/troubleshooting/#custom-resource","text":"Custom resources are backed by a Lambda function that implements the creation, modification or deletion or the resource. Every Lambda function has a CloudWatch log group that contains logs about the custom resource creation. To troubleshoot errors in custom resource, you need to check the custom resource's log group. Example custom resource log group names: /aws/lambda/ASEA-Master-Phase1-CustomCurReportDefinitionL-14IHLQCC1LY8L /aws/lambda/ASEA-Master-Phase2-AWS679f53fac002430cb0da5b7-Z75Q4GG9LIV5 /aws/lambda/ASEA-Operations-Phas-AWS679f53fac002430cb0da5-HMV2YF6OKJET /aws/lambda/ASEA-Operations-Phas-CustomGetDetectorIdLambd-HEM07DR0DOOJ","title":"Custom Resource"},{"location":"operations/troubleshooting/#cloudwatch","text":"When you arrived in CloudWatch logs by clicking on the state machine's step CloudWatch Logs link you will immediately see the list of log streams. Every log stream represents an instance of the Lambda function. You can find errors in multiple log groups using CloudWatch Log Insights. fields @timestamp, @message | sort @timestamp desc | filter strcontains(@message, 'ERROR') | limit 100","title":"CloudWatch"},{"location":"operations/troubleshooting/#codepipeline","text":"\"Internal Failure\" incorrect Github token, repo or branch","title":"CodePipeline"},{"location":"operations/troubleshooting/#examples","text":"Lets walk through a couple of example:","title":"Examples"},{"location":"operations/troubleshooting/#example-1","text":"State Machine failed (Lambda), click on the grey box, then click on the Resource object: Click on the red failed box, click on Step Input . The error is clearly indicated, we could not delete a Default VPC because the default VPC had dependencies, in a specified account and region. In this case several dependencies exist and need to be cleaned up to proceed (EIP's and something less obvious like security groups).","title":"Example 1"},{"location":"operations/troubleshooting/#example-2","text":"In the next example the state machine failed (sub-state machine) on the create accounts step. In this case rather than clicking on the Graph inspector we are going to scroll down through the Execution event history underneath the Graph inspector. We are going to find the FIRST failed task from the top of the list and then select the state machine from the prior task: We will then click onn the red failed box, select Exception and we can see a clear error message - we have exceeded the maximum number of AWS accounts allowed in your organization: Alternatively, in case the Exception error is not clear, we can select Details and then select CloudWatch logs for the end of the Resource section: If you open the latest log stream in the opened log group ( /aws/lambda/ASEA-InitialSetup-StartAccountCreationHandler-1IZ2N4EP29D72 ) and review the last several messages in the stream, the following clear message also appears:","title":"Example 2"},{"location":"operations/troubleshooting/#example-3","text":"In the next example the state machine failed in one of the CodeBuild state machine steps, based on the Resource name of the failed step. Rather than tracing this failure through the sub-state machine and then into the failed CodeBuild task, we are simply going to open AWS CodeBuild, and open the ASEA-DeployPrebuilt task. The failed task should be on the top of the Codebuild build run list. Open the build job. Using your browser, from the top of the page, search for \"FAIL\", and we are immediately brought to the error. In this particular case we had an issue with the creation of VPC endpoints. We defined something not supported by the current configuration file. The solution was to simply remove the offending endpoints from the config file and re-run the state machine.","title":"Example 3"},{"location":"pricing/sample_pricing/","text":"AWS Secure Environment Accelerator Pricing \u00b6 The AWS Secure Environment Accelerator (ASEA) is available free of charge as an open source solution on GitHub. You are responsible for the cost of the AWS services enabled, configured, and deployed by the solution. The ASEA solution enables, configures and deploys two types of AWS services : services leveraged by the ASEA itself to deliver its capabilities; and services orchestrated by the ASEA to help create a secure multi-account AWS foundation for your users and workloads. The pricing for services leveraged by the ASEA are relatively consistent and small. The pricing for services orchestrated by the ASEA can vary dramatically based on the underlying architecture, services and features selected by a customer through the customizable configuration file. Most of the provided example ASEA configuration files (except ultra-lite) build a highly available and scalable multi-datacenter environment with hyperscale routing and enterprise grade security worldwide, something that would cost tens of millions of dollars on-premises and still not achieve the same results. As shown below, different configuration files can dramatically change the monthly cost of running the solution from $30/month, to $1500/month, to $2400/month, to over $3700/month. The price of the deployed solution is 100% dependent on what the customer deploys, and not on the Accelerator automation engine itself. While the example deployment(s) may appear expensive when used solely for testing in a personal account, they typically only represent a very small percentage of a production customers AWS spend. The examples were designed to minimize costs as a customer scales. This document is designed to assist customers in understanding the pricing associated with operating the example ASEA configuration files. For full pricing details, please refer to each services pricing page . Example Configuration File Pricing \u00b6 The pricing found in this document is provided as an example only. Pricing represents reasonably steady state, minimal activity or traffic flows, and only includes sample workload accounts when they exist in the example config files. Pricing is based on the ca-central-1 region, a month with 31 days (744 hours), on-demand pricing and Bring Your Own Licensing (BYOL) for any 3rd party firewalls. This is estimated pricing, the solution is regularly updated and pricing is dependent on the actual version and configuration used to implement the solution. Any changes to the example configuration file will impact the pricing. These estimates do not include any customer workloads, workloads must be independently priced. Pricing by Configuration file \u00b6 The following table provides the estimated monthly pricing based on the example configuration. Additional information on each of the example config files can be found here . Example Configuration Description Estimated Monthly Pricing Ultra-Lite This configuration file was created to represent an extremely minimalistic Accelerator deployment, to demonstrate the art of the possible for an extremely simple config. This example is NOT recommended as it violates many AWS best practices. $30 Test Designed to reduce solution costs, while demonstrating full solution functionality (Use for testing Full/Lite configurations or Low Security Profiles). Based on Lite Config w/AWS Network Firewall. $1,500 Lite Same as Full Config with the following changes: 1) Reduces the FortiGate instance sizes from c5n.2xl to c5n.xl (VM08 to VM04); 2) Only deploys the 9 required centralized Interface Endpoints (removes 50). All services remain accessible using the AWS public endpoints, but require traversing the perimeter firewalls; 3) Removes the perimeter VPC Interface Endpoints; 4) Removes the Unclass ou and VPC. Four variants of the lite configuration file are provided: - AWS Control Tower w/AWS Network Firewall instead of IPSEC VPN Firewalls (recommended starting point) - AWS Network Firewall instead of IPSEC VPN Firewalls - IPSEC VPN integrated 3rd party firewalls - AWS Gateway Load Balancer integrated 3rd party firewalls $2,575 $2,550 $2,450 +FW lic. $2475 +FW lic. Full Large IPSEC VPN Firewalls w/Endpoints - The full configuration file was based on feedback from customers moving into AWS at scale and at a rapid pace. Customers of this nature have indicated that they do not want to have to upsize their perimeter firewalls or add Interface endpoints as their developers start to use new AWS services. These are the two most expensive components of the deployed architecture solution. $4,200 Pricing by AWS Account (All Configurations) \u00b6 The following table provides the estimated monthly pricing per AWS account for each of the example configuration files. AWS Account Description Ultra Lite Test Lite Full Management This is the organization management or root account. This account aggregates organization wide billing, and is used to manage the Accelerator, AWS SSO and SCPs. Access to this account must be highly restricted. This account should not contain any customer resources or workloads. $10 $75 $140 $140 Operations This Account is used for centralized IT operational resources (MAD, rsyslog, ITSM, etc.) which need to made available to all accounts in the organization and would generally be used and managed by the Cloud Operations team. - $275 $680 $680 Security The security account is generally used and managed by the customers security and compliance teams, and contains an organizations security tooling and consoles. This account functions as the organization administrative account for Security Hub, GuardDuty, Macie, Firewall Manager, and Access Analyzer. This account also has the ability to assume a view-only role in every account in the organization to conduct security investigations. $5 $10 $25 $25 Log Archive The log archive account provides a central aggregation and secure long-term storage location for all logs created within the AWS organization. Logs created in every account in the organization are centralized to an S3 bucket in this account. $15 $35 $55 $55 Perimeter This account is used as the centralized internet facing ingress/egress point and contains edge security services for the organizations IaaS based workloads. - $590 $410-$700 $1,200 Shared Network This account is used for centralized or shared networking resources and will typically contain a transit gateway to enable routing between different AWS based and on-premises networks. If a centralized or shared VPC architecture is deployed, this account will also contain VPCs (i.e. Dev, Test, Prod) which are shared via RAM sharing to accounts within designated OUs in the organization. If a spoke architecture is used, the Transit gateway is instead shared to the accounts within the organization. - $515 $825-$995 $1,950 MyDev1 This is an optional sample workload account which lives in the Dev organizational unit. Dev accounts have a full set of security guardrails similar to a production accounts and are designed to be used by developers. These accounts leverage either local or centralized networking and are connected to the organizations network via the centralized transit gateway, which is used to access the internet via the perimeter security account or on-premises networks. - - $80 $80 TheFunAccount This is an optional sample workload account that is created in Sandbox organizational unit. Sandbox accounts are designed for experimentation only, as they have the fewest guardrails, and provide the most cloud native experience. These accounts leverage localized networking and are fully isolated from all other organization networks, with no transit gateway connectivity and direct internet access via a local internet gateway. - - $70 $70 TOTAL Estimated Monthly Pricing $30 $1500 $2,450 - $2,575 $4,200 Detailed Pricing by AWS Service (Lite Config \u2013 IPSec VPN Active/Active Firewalls) \u00b6 We picked a single example configuration file to provide detailed pricing per service. The following table provides the estimated monthly pricing per AWS services provisioned by the Accelerator, across all accounts, for the Lite \u2013 IPSec VPN configuration. AWS service Quantity Estimated Monthly Pricing CloudTrail (All Regions) $28 CloudWatch (All Regions) $35 CloudWatch Events (All Regions) $0 CodeBuild $2 CodeCommit $0 CodePipeline $0 Config (All Regions) $85 Data Transfer $0 Directory Service - Managed Active Directory (2 domain controllers) - Shared Directory (2 accounts) - Small AD Connector (1) $444 DynamoDB $0 EC2 Container Registry (ECR) $0.2 Elastic Compute Cloud (EC2) - NAT Gateway (1) - Remote Desktop Gateway (1 x Windows t3.large) - rsyslog Servers (2 x Linux t3.large) - Fortinet Firewalls (2 x Linux c5n.xlarge) - EBS Volumes (30 GB x 3 instances, 100 GB x 2 instances) $669 Elastic Load Balancing - Application Load Balancing (2) - Network Load Balancing (rsyslog) (1) $55 GuardDuty (All Regions) $41 Key Management Service (All Regions) $44 Kinesis $12 Kinesis Firehose $2 Lambda (All Regions) $0 Macie (All Regions) $4 Route 53 - HostedZones (11) - Resolver Network Interfaces (4) $378 Secrets Manager $5 Security Hub (All Regions) $97 Simple Notification Service (All regions) $0 Simple Queue Service (All Regions) $0 Simple Storage Service (All regions) $6 Step Functions $1 Systems Manager $0 Virtual Private Cloud - VPC Endpoints (18) - VPN Connections (2) - Transit Gateway VPC Attachments (5) - Transit Gateway VPN Attachments (2) $542 TOTAL Estimated Monthly Pricing $2,450","title":"Pricing"},{"location":"pricing/sample_pricing/#aws-secure-environment-accelerator-pricing","text":"The AWS Secure Environment Accelerator (ASEA) is available free of charge as an open source solution on GitHub. You are responsible for the cost of the AWS services enabled, configured, and deployed by the solution. The ASEA solution enables, configures and deploys two types of AWS services : services leveraged by the ASEA itself to deliver its capabilities; and services orchestrated by the ASEA to help create a secure multi-account AWS foundation for your users and workloads. The pricing for services leveraged by the ASEA are relatively consistent and small. The pricing for services orchestrated by the ASEA can vary dramatically based on the underlying architecture, services and features selected by a customer through the customizable configuration file. Most of the provided example ASEA configuration files (except ultra-lite) build a highly available and scalable multi-datacenter environment with hyperscale routing and enterprise grade security worldwide, something that would cost tens of millions of dollars on-premises and still not achieve the same results. As shown below, different configuration files can dramatically change the monthly cost of running the solution from $30/month, to $1500/month, to $2400/month, to over $3700/month. The price of the deployed solution is 100% dependent on what the customer deploys, and not on the Accelerator automation engine itself. While the example deployment(s) may appear expensive when used solely for testing in a personal account, they typically only represent a very small percentage of a production customers AWS spend. The examples were designed to minimize costs as a customer scales. This document is designed to assist customers in understanding the pricing associated with operating the example ASEA configuration files. For full pricing details, please refer to each services pricing page .","title":"AWS Secure Environment Accelerator Pricing"},{"location":"pricing/sample_pricing/#example-configuration-file-pricing","text":"The pricing found in this document is provided as an example only. Pricing represents reasonably steady state, minimal activity or traffic flows, and only includes sample workload accounts when they exist in the example config files. Pricing is based on the ca-central-1 region, a month with 31 days (744 hours), on-demand pricing and Bring Your Own Licensing (BYOL) for any 3rd party firewalls. This is estimated pricing, the solution is regularly updated and pricing is dependent on the actual version and configuration used to implement the solution. Any changes to the example configuration file will impact the pricing. These estimates do not include any customer workloads, workloads must be independently priced.","title":"Example Configuration File Pricing"},{"location":"pricing/sample_pricing/#pricing-by-configuration-file","text":"The following table provides the estimated monthly pricing based on the example configuration. Additional information on each of the example config files can be found here . Example Configuration Description Estimated Monthly Pricing Ultra-Lite This configuration file was created to represent an extremely minimalistic Accelerator deployment, to demonstrate the art of the possible for an extremely simple config. This example is NOT recommended as it violates many AWS best practices. $30 Test Designed to reduce solution costs, while demonstrating full solution functionality (Use for testing Full/Lite configurations or Low Security Profiles). Based on Lite Config w/AWS Network Firewall. $1,500 Lite Same as Full Config with the following changes: 1) Reduces the FortiGate instance sizes from c5n.2xl to c5n.xl (VM08 to VM04); 2) Only deploys the 9 required centralized Interface Endpoints (removes 50). All services remain accessible using the AWS public endpoints, but require traversing the perimeter firewalls; 3) Removes the perimeter VPC Interface Endpoints; 4) Removes the Unclass ou and VPC. Four variants of the lite configuration file are provided: - AWS Control Tower w/AWS Network Firewall instead of IPSEC VPN Firewalls (recommended starting point) - AWS Network Firewall instead of IPSEC VPN Firewalls - IPSEC VPN integrated 3rd party firewalls - AWS Gateway Load Balancer integrated 3rd party firewalls $2,575 $2,550 $2,450 +FW lic. $2475 +FW lic. Full Large IPSEC VPN Firewalls w/Endpoints - The full configuration file was based on feedback from customers moving into AWS at scale and at a rapid pace. Customers of this nature have indicated that they do not want to have to upsize their perimeter firewalls or add Interface endpoints as their developers start to use new AWS services. These are the two most expensive components of the deployed architecture solution. $4,200","title":"Pricing by Configuration file"},{"location":"pricing/sample_pricing/#pricing-by-aws-account-all-configurations","text":"The following table provides the estimated monthly pricing per AWS account for each of the example configuration files. AWS Account Description Ultra Lite Test Lite Full Management This is the organization management or root account. This account aggregates organization wide billing, and is used to manage the Accelerator, AWS SSO and SCPs. Access to this account must be highly restricted. This account should not contain any customer resources or workloads. $10 $75 $140 $140 Operations This Account is used for centralized IT operational resources (MAD, rsyslog, ITSM, etc.) which need to made available to all accounts in the organization and would generally be used and managed by the Cloud Operations team. - $275 $680 $680 Security The security account is generally used and managed by the customers security and compliance teams, and contains an organizations security tooling and consoles. This account functions as the organization administrative account for Security Hub, GuardDuty, Macie, Firewall Manager, and Access Analyzer. This account also has the ability to assume a view-only role in every account in the organization to conduct security investigations. $5 $10 $25 $25 Log Archive The log archive account provides a central aggregation and secure long-term storage location for all logs created within the AWS organization. Logs created in every account in the organization are centralized to an S3 bucket in this account. $15 $35 $55 $55 Perimeter This account is used as the centralized internet facing ingress/egress point and contains edge security services for the organizations IaaS based workloads. - $590 $410-$700 $1,200 Shared Network This account is used for centralized or shared networking resources and will typically contain a transit gateway to enable routing between different AWS based and on-premises networks. If a centralized or shared VPC architecture is deployed, this account will also contain VPCs (i.e. Dev, Test, Prod) which are shared via RAM sharing to accounts within designated OUs in the organization. If a spoke architecture is used, the Transit gateway is instead shared to the accounts within the organization. - $515 $825-$995 $1,950 MyDev1 This is an optional sample workload account which lives in the Dev organizational unit. Dev accounts have a full set of security guardrails similar to a production accounts and are designed to be used by developers. These accounts leverage either local or centralized networking and are connected to the organizations network via the centralized transit gateway, which is used to access the internet via the perimeter security account or on-premises networks. - - $80 $80 TheFunAccount This is an optional sample workload account that is created in Sandbox organizational unit. Sandbox accounts are designed for experimentation only, as they have the fewest guardrails, and provide the most cloud native experience. These accounts leverage localized networking and are fully isolated from all other organization networks, with no transit gateway connectivity and direct internet access via a local internet gateway. - - $70 $70 TOTAL Estimated Monthly Pricing $30 $1500 $2,450 - $2,575 $4,200","title":"Pricing by AWS Account (All Configurations)"},{"location":"pricing/sample_pricing/#detailed-pricing-by-aws-service-lite-config-ipsec-vpn-activeactive-firewalls","text":"We picked a single example configuration file to provide detailed pricing per service. The following table provides the estimated monthly pricing per AWS services provisioned by the Accelerator, across all accounts, for the Lite \u2013 IPSec VPN configuration. AWS service Quantity Estimated Monthly Pricing CloudTrail (All Regions) $28 CloudWatch (All Regions) $35 CloudWatch Events (All Regions) $0 CodeBuild $2 CodeCommit $0 CodePipeline $0 Config (All Regions) $85 Data Transfer $0 Directory Service - Managed Active Directory (2 domain controllers) - Shared Directory (2 accounts) - Small AD Connector (1) $444 DynamoDB $0 EC2 Container Registry (ECR) $0.2 Elastic Compute Cloud (EC2) - NAT Gateway (1) - Remote Desktop Gateway (1 x Windows t3.large) - rsyslog Servers (2 x Linux t3.large) - Fortinet Firewalls (2 x Linux c5n.xlarge) - EBS Volumes (30 GB x 3 instances, 100 GB x 2 instances) $669 Elastic Load Balancing - Application Load Balancing (2) - Network Load Balancing (rsyslog) (1) $55 GuardDuty (All Regions) $41 Key Management Service (All Regions) $44 Kinesis $12 Kinesis Firehose $2 Lambda (All Regions) $0 Macie (All Regions) $4 Route 53 - HostedZones (11) - Resolver Network Interfaces (4) $378 Secrets Manager $5 Security Hub (All Regions) $97 Simple Notification Service (All regions) $0 Simple Queue Service (All Regions) $0 Simple Storage Service (All regions) $6 Step Functions $1 Systems Manager $0 Virtual Private Cloud - VPC Endpoints (18) - VPN Connections (2) - Transit Gateway VPC Attachments (5) - Transit Gateway VPN Attachments (2) $542 TOTAL Estimated Monthly Pricing $2,450","title":"Detailed Pricing by AWS Service (Lite Config \u2013 IPSec VPN Active/Active Firewalls)"},{"location":"schema/","text":"Configuration File Schema Documentation \u00b6 English Documentation: https://awssea.github.io/schema/en/index.html Documentation Fran\u00e7aise: https://awssea.github.io/schema/fr/index.html","title":"Configuration File Schema Documentation"},{"location":"schema/#configuration-file-schema-documentation","text":"English Documentation: https://awssea.github.io/schema/en/index.html Documentation Fran\u00e7aise: https://awssea.github.io/schema/fr/index.html","title":"Configuration File Schema Documentation"},{"location":"workshops/","text":"ASEA Workshops \u00b6 Accelerator Administrator Immersion Day \u00b6 The Accelerator Administrator Immersion Day workshop is focused on helping administrators who will be administering the landing zone understand how they can design, build and operate the components in ASEA. Click here for an overview of the topics covered. Accelerator Workload/Application Team Immersion Day \u00b6 The Accelerator Workload/Application Team Immersion Day workshop is focused on helping project teams understand what it means to operate within an ASEA managed environment. Click here for an overview of the topics covered.","title":"ASEA Workshops"},{"location":"workshops/#asea-workshops","text":"","title":"ASEA Workshops"},{"location":"workshops/#accelerator-administrator-immersion-day","text":"The Accelerator Administrator Immersion Day workshop is focused on helping administrators who will be administering the landing zone understand how they can design, build and operate the components in ASEA. Click here for an overview of the topics covered.","title":"Accelerator Administrator Immersion Day"},{"location":"workshops/#accelerator-workloadapplication-team-immersion-day","text":"The Accelerator Workload/Application Team Immersion Day workshop is focused on helping project teams understand what it means to operate within an ASEA managed environment. Click here for an overview of the topics covered.","title":"Accelerator Workload/Application Team Immersion Day"}]}