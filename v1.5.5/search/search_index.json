{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"1. AWS Secure Environment Accelerator","text":""},{"location":"#11-overview","title":"1.1. Overview","text":"<p>The AWS Accelerator is a tool designed to help deploy and operate secure multi-account, multi-region AWS environments on an ongoing basis. The power of the solution is the configuration file that drives the architecture deployed by the tool. This enables extensive flexibility and for the completely automated deployment of a customized architecture within AWS without changing a single line of code.</p> <p>While flexible, the AWS Accelerator is delivered with a sample configuration file which deploys an opinionated and prescriptive architecture designed to help meet the security and operational requirements of many governments around the world. Tuning the parameters within the configuration file allows for the deployment of customized architectures and enables the solution to help meet the multitude of requirements of a broad range of governments and public sector organizations.</p> <p>The installation of the provided prescriptive architecture is reasonably simple, deploying a customized architecture does require extensive understanding of the AWS platform. The sample deployment specifically helps customers meet NIST 800-53 and/or CCCS Medium Cloud Control Profile (formerly PBMM).</p> <p></p>"},{"location":"#12-what-specifically-does-the-accelerator-deploy-and-manage","title":"1.2. What specifically does the Accelerator deploy and manage?","text":"<p>A common misconception is that the AWS Secure Environment Accelerator only deploys security services, not true. The Accelerator is capable of deploying a complete end-to-end hybrid enterprise multi-region cloud environment.</p> <p>Additionally, while the Accelerator is initially responsible for deploying a prescribed architecture, it more importantly allows for organizations to operate, evolve, and maintain their cloud architecture and security controls over time and as they grow, with minimal effort, often using native AWS tools. While the Accelerator helps with the deployment of technical security controls, it\u2019s important to understand that the Accelerator is only part of your security and compliance effort.  We encourage customers to work with their AWS account team, AWS Professional Services or an AWS Partner to determine how to best meet the remainder of your compliance requirements.</p> <p>The Accelerator is designed to enable customers to upgrade across Accelerator versions while maintaining a customer\u2019s specific configuration and customizations, and without the need for any coding expertise or for professional services. Customers have been able to seamlessly upgrade their AWS multi-account environment from the very first Accelerator beta release to the latest release (across more than 50 releases), gaining the benefits of bug fixes and enhancements while having the option to enable new features, without any loss of existing customization or functionality.</p> <p>Specifically the accelerator deploys and manages the following functionality, both at initial accelerator deployment and as new accounts are created, added, or onboarded in a completely automated but customizable manner:</p>"},{"location":"#121-creates-aws-account","title":"1.2.1. Creates AWS Account","text":"<ul> <li>Core Accounts - as many or as few as your organization requires, using the naming you desire. These accounts are used to centralize core capabilities across the organization and provide <code>Control Panel</code> like capabilities across the environment. Common core accounts include:<ul> <li>Shared Network</li> <li>Operations</li> <li>Perimeter</li> <li>Log Archive</li> <li>Security Tooling</li> </ul> </li> <li>Workload Accounts - automated concurrent mass account creation or use AWS organizations to scale one account at a time. These accounts are used to host a customer's workloads and applications.</li> <li>Scalable to 1000's of AWS accounts</li> <li>Supports AWS Organizations nested OU's and importing existing AWS accounts</li> <li>Performs 'account warming' to establish initial limits, when required</li> <li>Automatically submits limit increases, when required (complies with initial limits until increased)</li> <li>Leverages AWS Control Tower</li> </ul>"},{"location":"#122-creates-networking","title":"1.2.2. Creates Networking","text":"<ul> <li>Transit Gateways and TGW route tables (incl. inter-region TGW peering)</li> <li>Centralized and/or Local (bespoke) VPC's</li> <li>Subnets, Route tables, NACLs, Security groups, NATGWs, IGWs, VGWs, CGWs</li> <li>NEW Outpost, Local Zone and Wavelength support</li> <li>VPC Endpoints (Gateway and Interface, Centralized or Local)</li> <li>Route 53 Private and Public Zones, Resolver Rules and Endpoints, VPC Endpoint Overloaded Zones</li> <li>All completely and individually customizable (per account, VPC, subnet, or OU)</li> <li>Layout and customize your VPCs, subnets, CIDRs and connectivity the way you want</li> <li>Static or Dynamic VPC and subnet CIDR assignments</li> <li>Deletes default VPC's (worldwide)</li> <li>AWS Network Firewall</li> </ul>"},{"location":"#123-cross-account-object-sharing","title":"1.2.3. Cross-Account Object Sharing","text":"<ul> <li>VPC and Subnet sharing, including account level re-tagging (Per account security group 'replication')</li> <li>VPC attachments and peering (local and cross-account)</li> <li>Zone sharing and VPC associations</li> <li>Managed Active Directory sharing, including R53 DNS resolver rule creation/sharing</li> <li>Automated TGW inter-region peering</li> <li>Populate Parameter Store with all <code>user</code> objects to be used by customers' IaC</li> <li>Deploy and share SSM documents (4 provided out-of-box, ELB Logging, S3 Encryption, Instance Profile remediation, Role remediation)<ul> <li>customer can provide their own SSM documents for automated deployment and sharing</li> </ul> </li> </ul>"},{"location":"#124-identity","title":"1.2.4. Identity","text":"<ul> <li>Creates Directory services (Managed Active Directory and Active Directory Connectors)</li> <li>Creates Windows admin bastion host auto-scaling group</li> <li>Set Windows domain password policies</li> <li>Set IAM account password policies</li> <li>Creates Windows domain users and groups (initial installation only)</li> <li>Creates IAM Policies, Roles, Users, and Groups</li> <li>Fully integrates with and leverages AWS SSO for centralized and federated login</li> </ul>"},{"location":"#125-cloud-security-services","title":"1.2.5. Cloud Security Services","text":"<ul> <li>Enables and configures the following AWS services, worldwide w/central designated admin account:<ul> <li>GuardDuty w/S3 protection</li> <li>Security Hub (Enables designated security standards, and disables individual controls)</li> <li>Firewall Manager</li> <li>CloudTrail w/Insights and S3 data plane logging</li> <li>Config Recorders/Aggregator</li> <li>Conformance Packs and Config rules (95 out-of-box NIST 800-53 rules, 2 custom rules, customizable per OU)</li> <li>Macie</li> <li>IAM Access Analyzer</li> <li>CloudWatch access from central designated admin account (and setting Log group retentions)</li> </ul> </li> </ul>"},{"location":"#126-other-security-capabilities","title":"1.2.6. Other Security Capabilities","text":"<ul> <li>Creates, deploys and applies Service Control Policies</li> <li>Creates Customer Managed KMS Keys (SSM, EBS, S3), EC2 key pairs, and secrets</li> <li>Enables account level default EBS encryption and S3 Block Public Access</li> <li>Configures Systems Manager Session Manager w/KMS encryption and centralized logging</li> <li>Configures Systems Manager Inventory w/centralized logging</li> <li>Creates and configures AWS budgets (customizable per OU and per account)</li> <li>Imports or requests certificates into AWS Certificate Manager</li> <li>Deploys both perimeter and account level ALB's w/Lambda health checks, certificates and TLS policies</li> <li>Deploys &amp; configures 3rd party firewall clusters and management instances (leverages marketplace)<ul> <li>Gateway Load Balancer w/auto-scaling and VPN IPSec BGP ECMP deployment options</li> </ul> </li> <li>Protects Accelerator deployed and managed objects</li> <li>Sets Up SNS Alerting topics (High, Medium, Low, Blackhole priorities)</li> <li>Deploys CloudWatch Log Metrics and Alarms</li> <li>Deploys customer provided custom config rules (2 provided out-of-box, no EC2 Instance Profile/Permissions)</li> </ul>"},{"location":"#127-centralized-logging-and-alerting","title":"1.2.7. Centralized Logging and Alerting","text":"<ul> <li>Deploys an rsyslog auto-scaling cluster behind a NLB, all syslogs forwarded to CloudWatch Logs</li> <li>Centralized access to \"Cloud Security Service\" Consoles from designated AWS account</li> <li>Centralizes logging to a single centralized S3 bucket (enables, configures and centralizes)<ul> <li>VPC Flow logs w/Enhanced metadata fields (also sent to CWL)</li> <li>Organizational Cost and Usage Reports</li> <li>CloudTrail Logs including S3 Data Plane Logs (also sent to CWL)</li> <li>All CloudWatch Logs (includes rsyslog logs)</li> <li>Config History and Snapshots</li> <li>Route 53 Public Zone Logs (also sent to CWL)</li> <li>GuardDuty Findings</li> <li>Macie Discovery results</li> <li>ALB Logs</li> <li>SSM Inventory</li> <li>Security Hub findings</li> <li>SSM Session Logs (also sent to CWL)</li> <li>Resolver Query Logs (also sent to CWL)</li> </ul> </li> <li>Email alerting for CloudTrail Metric Alarms, Firewall Manager Events, Security Hub Findings incl. GuardDuty Findings</li> <li>NEW Optionally collect Organization and ASEA configuration and metadata in a new restricted log archive bucket</li> </ul>"},{"location":"#13-relationship-with-aws-landing-zone-solution-alz","title":"1.3. Relationship with AWS Landing Zone Solution (ALZ)","text":"<p>The ALZ was an AWS Solution designed to deploy a multi-account AWS architecture for customers based on best practices and lessons learned from some of AWS' largest customers. The AWS Accelerator draws on design patterns from the Landing Zone, and re-uses several concepts and nomenclature, but it is not directly derived from it, nor does it leverage any code from the ALZ. The Accelerator is a standalone solution with no dependence on ALZ.</p>"},{"location":"#14-relationship-with-aws-control-tower","title":"1.4. Relationship with AWS Control Tower","text":"<p>The AWS Secure Environment Accelerator now leverages AWS Control Tower!</p> <p>With the release of v1.5.0, the AWS Accelerator adds the capability to be deployed on top of AWS Control Tower. Customers get the benefits of the fully managed capabilities of AWS Control Tower combined with the power and flexibility of the Accelerators Networking and Security orchestration.</p>"},{"location":"#15-accelerator-installation-process-summary","title":"1.5. Accelerator Installation Process (Summary)","text":"<p>This summarizes the installation process, the full installation document can be found in the documentation section below.</p> <ul> <li>Create a config.json (or config.yaml) file to represent your organizations requirements (several samples provided)</li> <li>Create a Secrets Manager Secret which contains a GitHub token that provides access to the Accelerator code repository</li> <li>Create a unique S3 input bucket in the management account of the region you wish to deploy the solution and place your config.json and any additional custom config files in the bucket</li> <li>Download and execute the latest release installer CloudFormation template in your management accounts preferred 'primary' / 'home' region</li> <li>Wait for:<ul> <li>CloudFormation to deploy and start the Code Pipeline (~5 mins)</li> <li>Code Pipeline to download the Accelerator codebase and install the Accelerator State Machine (~10 mins)</li> <li>The Accelerator State Machine to finish execution (~1.25 hrs Standalone version, ~2.25 hrs Control Tower Version)</li> </ul> </li> <li>Perform required one-time post installation activities (configure AWS SSO, set firewall passwords, etc.)</li> <li>On an ongoing basis:<ul> <li>Use AWS Organizations to create new AWS accounts, which will automatically be guardrailed by the Accelerator</li> <li>Update the config file in CodeCommit and run the Accelerator State Machine to:<ul> <li>deploy, configure and guardrail multiple accounts at the same time (~25 min Standalone, ~50 min/account Control Tower)</li> <li>change Accelerator configuration settings (~25 min)</li> </ul> </li> </ul> </li> </ul>"},{"location":"toc-generation/","title":"Table of Contents Generation","text":"<p>Easy and automated ToC generation:</p> <ul> <li>Visual Studio Code Plugin - Markdown All in One - https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one</li> </ul> <p>Note: The PDF engine (<code>pandoc/latex</code>) used in the <code>Release Documentation</code> action does not like deeply nested markdown lists. Accordingly, use <code>Depth From: 2</code> and <code>Depth to: 4</code> in Table of Contents generation.</p> <p>Other alternatives:</p> <ul> <li>Table of Contents can be generated for Markdown documents using the following tool: https://github.com/AlanWalk/Markdown-TOC</li> </ul>"},{"location":"architectures/","title":"Accelerator Sample Sensitive Architecture","text":"<ul> <li>Overview</li> <li>Account Structure</li> <li>Authentication &amp; Authorization</li> <li>Logging and Monitoring</li> <li>Networking</li> <li>Architecture Diagrams</li> </ul>"},{"location":"architectures/sensitive/","title":"1. AWS Secure Environment Accelerator Reference Architecture","text":""},{"location":"architectures/sensitive/#11-overview","title":"1.1. Overview","text":"<p>The AWS Secure Environment Accelerator (ASEA) Reference Architecture is a comprehensive, multi-account AWS cloud architecture, which was designed working backwards from AWS customers with high compliance requirements like federal, provincial and municipal governments. The ASEA Reference Architecture has been designed to address central identity and access management, governance, data security, comprehensive logging, and network design/segmentation per security frameworks like NIST 800-53, ITSG-33, FEDRAMP Moderate, IRAP and other Sensitive or Medium level security profiles.</p> <p>This document is solely focused on the deployed or resulting reference architecture and does NOT talk about the tooling, mechanisms, or automation engine used to deploy the architecture. The AWS Secure Environment Accelerator (ASEA) is one tool capable of deploying this architecture (along with many other architectures), but customers are free to choose whichever mechanism they deem appropriate to deploy it. Readers should refer to the ASEA documentation for references to the ASEA architecture, design, operation, and troubleshooting. If using the ASEA automation engine, this document reflects the resulting architecture deployed using one of the Medium ASEA sample configuration files. This architecture document should stand on its own in depicting the <code>deployed</code> architecture.</p> <p>The ASEA Architecture is a standalone architecture, irrespective of how it was delivered into a customer AWS environment. It is nonetheless anticipated that most customers will choose to realize their ASEA Architecture via the delivery mechanism of the ASEA automation engine. Except where absolutely necessary, this document will refrain from referencing the ASEA automation engine further.</p>"},{"location":"architectures/sensitive/#12-introduction","title":"1.2. Introduction","text":"<p>The AWS Secure Environment Accelerator (ASEA) Reference Architecture has been built with the following design principles:</p> <ol> <li>Deliver security outcomes aligned with a medium level security control profile;</li> <li>Maximize agility, scalability, and availability, while minimizing cost;</li> <li>Enable the full capabilities of the AWS cloud;</li> <li>Be adaptable to evolving technological capabilities in the underlying platform being used in the AWS Secure Environment Accelerator Architecture;</li> <li>Allow for seamless auto-scaling and provide unbounded bandwidth as bandwidth requirements increase (or decrease) based on actual customer load (a key aspect of the value proposition of cloud computing);</li> <li>Architect for high availability: the design stretches across two physical AWS Availability Zones (AZ), such that the loss of any one AZ does not impact application availability. The design can be easily extended to a third availability zone;</li> <li>Operate as least privilege: all principals in the accounts are intended to operate with the lowest-feasible permission set.</li> </ol>"},{"location":"architectures/sensitive/#121-purpose-of-document","title":"1.2.1. Purpose of Document","text":"<p>This document is intended to outline the technical measures that are delivered by the AWS Secure Environment Accelerator Reference Architecture. This includes a summary of the major architectural decisions.</p>"},{"location":"architectures/sensitive/#122-architecture-summary","title":"1.2.2. Architecture Summary","text":"<p>The central features of the AWS Secure Environment Accelerator Architecture are as follows:</p> <ul> <li> <p>AWS Organization with multiple-accounts: An AWS Organization is a grouping construct for a number of separate AWS accounts that are controlled by a single customer entity. This provides consolidated billing, account grouping using organizational units, and facilitates the deployment of pan-organizational guardrails such as API logging with CloudTrail and preventative controls using AWS Service Control Policies (SCPs). Separate AWS accounts provide strong control-plane and data-plane isolation between workloads and/or environments, as if they were owned by different AWS customers. The solution provides a prescriptive AWS account structure, giving different accounts different security personas based on its grouping.</p> </li> <li> <p>Preventative security controls: Protect the architecture, prevent the disablement of guardrails, and block undesirable user behavior. These are implemented using AWS Service Control Policies (SCPs). SCPs provide a guardrail mechanism principally used to deny specific or entire categories of API operations at an AWS account, OU, or organization level. These can be used to ensure workloads are deployed only in prescribed regions, or deny access to specific AWS services. The solution provides prescriptive SCPs.</p> </li> <li> <p>Encryption: AWS Key Management Service (KMS) with customer-managed keys is used to encrypt data stored at rest using FIPS 140-2 validated encryption, whether in S3 buckets, EBS volumes, RDS databases, or other AWS services storing data. Data in-transit is protected using TLS 1.2 or higher encryption.</p> </li> <li> <p>Centralized, isolated networking: AWS Virtual Private Clouds (VPCs) are used to create data-plane isolation between workloads, centralized in a shared-network account. Centralization enables strong segregation of duties and cost optimization. Connectivity to on-premises environments, internet egress, shared resources and AWS APIs are mediated at a central point of ingress/egress via the use of Transit Gateway, Site-to-Site VPN, Next-Gen Firewalls, and AWS Direct Connect (where applicable). The centralized VPC architecture is not for all customers; for customers less concerned with cost optimization, an option exists for local account based VPCs interconnected via the Transit Gateway in the central shared-network account. Under both options, the architecture prescribes moving AWS public API endpoints into the customer's private VPC address space, using centralized endpoints for cost efficiency.</p> </li> <li> <p>Segmentation and Separation: Not only does the architecture provide strong segmentation and separation between workloads belonging to different stages of the SDLC cycle, or between different IT administrative roles (like between networking, ingress/egress firewalls, and workloads), it offers a strong network zoning architecture, micro-segmenting the environment by wrapping every instance or component in a stateful firewall which is enforced in the hardware of the AWS Nitro System. All flows are tightly enforced, with lateral movement prevented between applications, tiers within an application, and nodes in a tier of an application unless explicitly allowed. Further, routing is prevented between Dev, Test, and Prod with recommendations on a CI/CD architecture to enable developer agility and ease code promotion between environments with appropriate approvals.</p> </li> <li> <p>Centralized DNS management: Amazon Route 53 is used to provide unified public and private hosted zones across the cloud environment. Inbound and Outbound Route 53 Resolvers extend this unified view of DNS to on-premises networks.</p> </li> <li> <p>Centralized logging: CloudTrail logs are enabled organization-wide to provide full control plane auditability across the cloud environment. CloudWatch Logs, AWS' cloud natice logging service is used to capture a wide variety of logs including OS and application logs, VPC flow logs, and DNS logs which are then centralized and deletion is prevented using SCPs. The architecture prescribes comprehensive log collection and centralization across AWS services and accounts.</p> </li> <li> <p>Centralized security monitoring: Compliance drift and security threats are surfaced across the customer's AWS organization via the automatic deployment of a multitude of different types of detective security controls. This includes enabling the multitude of AWS security services in every account in the customers AWS organization including Amazon GuardDuty, AWS Security Hub, AWS Config, AWS Firewall Manager, Amazon Macie, Access Analyzer, CloudWatch Alarms with control and visibility delagated across the multi-account environment to a single central security tooling account for easy organization-wide visibility to all security findings and compliance drift. In addition, the security account has been provided View-Only access across the organization (including access to each account's CloudWatch console) to enable investigation during an incident. View-Only access is different from Read-Only access in that it does not provide any access to any data. Finally, an optional add-on is available to consume the comprehensive set of centralized logs making them searchable, providing correlation and basic dashboards.</p> </li> <li> <p>Single Sign-On: AWS SSO is used to provide centralized IAM role assumption into AWS accounts across the organization for authorized principals. An organization's existing identities can be sourced from a customer's existing Active Directory identity store or other 3rd party identity provider (IdP). AWS enables MFA enforcement using Authenticator apps, Security keys and built-in authenticators, supporting WebAuthn, FIDO2, and U2F based authentication and devices.</p> </li> <li> <p>Centralized ingress/egress IaaS inspection: It is common to see centralized ingress/egress requirements for IaaS based workloads. The architecture provides said functionality, enabling customers to decide if native AWS ingress/egress firewall inspection services meet their requirements, or to augment those capabilities with 3rd party firewall appliances. The architecture supports starting with an AWS firewall, switching to a 3rd party firewall, or a combination of ingress/egress firewall technologies.</p> </li> <li> <p>Automation: Automation is a critical component of the architecture. It ensures guardrails are consistently applied as new AWS accounts are added to the organization as new teams and workloads are brought onboard. It remediates compliance drift and provides guardrails in the root organization account.</p> </li> </ul> <p>The following diagram illustrates the AWS ASEA reference architecture:</p> <p></p> <p>The following diagram illustrates an alternative AWS ASEA reference architecture which uses spoke VPCs (instead of centralized VPCs):</p> <p></p>"},{"location":"architectures/sensitive/#123-relationship-to-other-aws-reference-architectures","title":"1.2.3. Relationship to other AWS reference architectures","text":"<p>The AWS Secure Environment Accelerator Architecture builds upon AWS standardized design patterns and best practices. The architecture aligns with AWS multi-account guidance, the foundation provided by AWS Control Tower, the AWS Secure Environment Accelerator sample architecture, and the AWS Security reference architecture.</p>"},{"location":"architectures/sensitive/#124-document-conventions","title":"1.2.4. Document Conventions","text":"<p>The following conventions are used throughout this document.</p>"},{"location":"architectures/sensitive/#1241-aws-account-numbers","title":"1.2.4.1. AWS Account Numbers","text":"<p>AWS account numbers are decimal-digit pseudorandom identifiers with 12 digits (e.g. <code>651278770121</code>). This document will use the convention that an AWS Organization Management (root) account has the account ID <code>123456789012</code>, and child accounts are represented by <code>111111111111</code>, <code>222222222222</code>, etc.</p> <p>For example the following ARN would refer to a VPC subnet in the <code>ca-central-1</code> region in the Organization Management (root) account:</p> <pre><code>arn:aws:ec2:ca-central-1:123456789012:subnet/subnet-024759b61fc305ea3\n</code></pre>"},{"location":"architectures/sensitive/#1242-json-annotation","title":"1.2.4.2. JSON Annotation","text":"<p>Throughout the document, JSON snippets may be annotated with comments (starting with <code>//</code>). The JSON language itself does not define comments as part of the specification; these must be removed prior to use in most situations, including the AWS Console and APIs.</p> <p>For example:</p> <pre><code>{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:root\" // Trust the Organization Management (root) account.\n    },\n    \"Action\": \"sts:AssumeRole\"\n}\n</code></pre> <p>The above is not valid JSON without first removing the comment on the fourth line.</p>"},{"location":"architectures/sensitive/#1243-ip-addresses","title":"1.2.4.3. IP Addresses","text":"<p>The design makes use of RFC1918 addresses (e.g. <code>10.1.0.0/16</code>) and RFC6598 (e.g. <code>100.96.250.0/23</code>) for various networks; these will be labeled accordingly. Any specific range or IP shown is purely for illustration purposes only.</p>"},{"location":"architectures/sensitive/#125-customer-naming","title":"1.2.5. Customer Naming","text":"<p>This document will make no reference to specific AWS customers. Where naming is required (e.g. in domain names), this document will use a placeholder name as needed; e.g. <code>example.ca</code>.</p>"},{"location":"architectures/sensitive/accounts/","title":"1. Account Structure","text":""},{"location":"architectures/sensitive/accounts/#11-overview","title":"1.1. Overview","text":"<p>AWS accounts are a strong isolation boundary; by default there is no control plane or data plane access from one AWS account to another. AWS accounts provide different AWS customers an isolated private cloud tenancy inside the AWS commercial cloud. It is worth noting that users and roles reside within AWS accounts, and are the constructs used to grant permissions within an AWS account to people, services and applications. AWS Organizations is a service that provides centralized billing across a fleet of accounts, and optionally, some integration-points for cross-account guardrails and cross-account resource sharing. The AWS Secure Environment Accelerator Architecture uses these features of AWS Organizations to realize its outcomes.</p>"},{"location":"architectures/sensitive/accounts/#12-organization-structure","title":"1.2. Organization structure","text":"<p>The AWS Secure Environment Accelerator Architecture includes the following default AWS organization and account structure.</p> <p>Note that the AWS account structure is strictly a control plane concept - nothing about this structure implies anything about the network architecture or network flows.</p> <p></p>"},{"location":"architectures/sensitive/accounts/#121-organization-management-root-aws-account","title":"1.2.1. Organization Management (root) AWS Account","text":"<p>The AWS Organization resides in the Organization Management (root) AWS account and is traditionally an organization's first AWS account. This account is not used for workloads - it functions primarily as a billing aggregator, and a gateway to the entire cloud footprint for high-trust principals. Access to the Management account must be strictly controlled to a small set of highly trusted individuals from the organization. Additionally, the Organization Management account is where the automation engine or tooling is installed to automate the deployment of the ASEA architecture and its security guardrails. There exists a trust relationship which is used by the automation engine between child AWS accounts in the organization and the Organization Management (root) account; from the Management account, users and roles can assume a role of the following form in child accounts:</p> <pre><code>{\n\"Role\": {\n\"Path\": \"/\",\n\"RoleName\": \"OrganizationAccountAccessRole\",\n\"Arn\": \"arn:aws:iam::111111111111:role/OrganizationAccountAccessRole\", // Child account\n\"AssumeRolePolicyDocument\": {\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"arn:aws:iam::123456789012:root\" // Organization Management (root) account may assume this role\n},\n\"Action\": \"sts:AssumeRole\"\n}\n]\n}\n}\n}\n</code></pre> <p>Note: this is the default role installed by AWS Organizations (<code>OrganizationAccountAccessRole</code>) when new AWS accounts are created using AWS organizations. This role changes to <code>AWSControlTowerExecution</code> when Control Tower is being leveraged.</p>"},{"location":"architectures/sensitive/accounts/#122-aws-sso","title":"1.2.2. AWS SSO","text":"<p>AWS Single Sign-On (SSO) resides in the Organization Management account. Once deployed from the Organization Management account it is recommended that AWS SSO administration is delegated to the Operations account (sometimes referred to as the Shared Services account). AWS SSO is where you create, or connect, your workforce identities in AWS, once, and manage access centrally across your AWS organization. You can create user identities directly in AWS SSO, or you can bring them from your Microsoft Active Directory or a standards-based identity provider, such as Okta Universal Directory or Azure AD. AWS SSO provides a unified administration experience to define, customize, and assign fine-grained access. Your workforce users get a user portal to access all of their assigned AWS accounts. The AWS SSO service deploys IAM roles into accounts in the organization and associates them with the designated workforce identities . More details on SSO are available in the Authentication and Authorization section of this document.</p>"},{"location":"architectures/sensitive/accounts/#13-organizational-units","title":"1.3. Organizational Units","text":"<p>Underneath the root of the organization, Organizational Units (OUs) provide a mechanism for grouping accounts into logical collections. Aside from the benefit of the grouping itself, these collections serve as the attachment points for SCPs (preventative API-blocking controls), and Resource Access Manager sharing (cross-account resource sharing). Additionally, the ASEA leverages OUs to assign AWS accounts a persona which includes a consistent security personality.</p> <p>The OU an AWS account is placed in determines the account's purpose, its security posture and the applicable guardrails. An account placed in the Sandbox OU would have the least restrictive, most agile, and most cloud native functionality, whereas an account placed in the Prod OU would have the most restrictive set of guardrails applied.</p> <p>OUs are NOT designed to reflect an organization's structure, and should instead reflect major shifts in permissions. OUs should not be created for every stage in the SDLC cycle, but those that represent a major permissions shift. For example, organizations that have multiple test stages would often locate the Test and QA Test instances of a workload within the same AWS test account. Customers with a Pre-Prod requirement would often either place their Pre-Prod workloads into their Prod account (alongside the Prod workloads), or in cases requiring more extensive isolation, in a second AWS account located in the Prod OU.</p> <p></p> <p>Example use cases are as follows:</p> <ul> <li>An SCP is attached to the Infrastructure OU to prevent the deletion of Transit Gateway resources in the associated accounts.</li> <li>The Shared Network account uses RAM sharing to share the development line-of-business VPC with accounts in the development OU. This makes the VPC available to a functional account in that OU used by developers, despite residing logically in the shared network account.</li> </ul> <p>OUs may be nested (to a total depth of five), with SCPs and RAM sharing being controlled at the top level by the automation tooling. A typical AWS Secure Environment Accelerator Architecture environment will have the following OUs:</p>"},{"location":"architectures/sensitive/accounts/#131-security-ou","title":"1.3.1. Security OU","text":"<p>The accounts in this OU are considered administrative in nature with access often restricted to IT security personnel.</p> <p>The Security OU is used to hold AWS accounts containing AWS security resources shared or utilized by the rest of the organization. The accounts in the security OU (Log-Archive and Security Tooling) often represent the core or minimum viable set of accounts for organizations wishing to slim the architecture down. No application accounts or application workloads are intended to exist within this OU.</p>"},{"location":"architectures/sensitive/accounts/#132-infrastructure-ou","title":"1.3.2. Infrastructure OU","text":"<p>The accounts in this OU are also considered administrative in nature with access often restricted to IT operations personnel.</p> <p>The Infrastructure OU is used to hold AWS accounts containing AWS infrastructure resources shared or utilized by the rest of the organization. The accounts in the Infrastructure OU are also considered core accounts, including accounts like the centralized Shared Network account, the Perimeter Security centralized ingress/egress account, and the Operations account. No application accounts or application workloads are intended to exist within this OU.</p> <p>These accounts can be optionally removed depending on the outcomes a customer desires. For example, some small customers merge the Operations account into the Security Tooling account, while others may deploy local account-based networking instead of using the Shared Network account and may at the same time drop the central ingress/egress requirements supported by the Perimeter Security account. Eliminating these three accounts should only be considered in the smallest of organizations.</p>"},{"location":"architectures/sensitive/accounts/#133-functional-ous","title":"1.3.3. Functional OUs","text":"<p>It is envisioned that most major applications, groups of workloads or teams will work across a set of three or four dedicated AWS accounts provisioned across different functional OUs (Dev, Test, Prod and optionally Central). As new teams, major applications or groups of workloads are onboarded to AWS, they will be provided with this group of new AWS accounts. New teams, groups of applications and major workloads do not share AWS accounts, they each get their own group of unique AWS accounts providing strong segregation and isolation both between stages in the SDLC cycle and between other workloads or teams.</p>"},{"location":"architectures/sensitive/accounts/#134-functional-ou-sandbox","title":"1.3.4. Functional OU: Sandbox","text":"<p>Accounts in this OU are used by development and IT teams for proof of concept / prototyping work. The Sandbox OU offers the most cloud native, agile experience and is used for experimentation. It is not to be used to hold production workloads or sensitive data as it offers the fewest security controls.</p> <p>These accounts are isolated at a network level and are not connected to the VPCs hosting development, test, production, or shared workloads, nor do they have on-premises network connectivity. These accounts have direct internet access via an internet gateway (IGW). They do not route through the Perimeter Security services account or VPC for internet access.</p>"},{"location":"architectures/sensitive/accounts/#135-functional-ou-dev","title":"1.3.5. Functional OU: Dev","text":"<p>The Dev OU is used to hold accounts at the Development or similarly permissioned stage of the SDLC cycle, often containing sensitive data and workloads.</p> <p>This is the primary account type that an organization's developers would typically work within and hosts development tools and line of business application solutions that are in active development. These accounts are provided internet access for IaaS based workloads via the Perimeter Security account.</p>"},{"location":"architectures/sensitive/accounts/#136-functional-ou-test","title":"1.3.6. Functional OU: Test","text":"<p>The Test OU is used to hold accounts at the test or similarly permissioned (i.e. QA) stage of the SDLC cycle and typically contain sensitive data and workloads. Accounts in this OU host testing tools and line of business application solutions that are being tested prior to promotion to production. These accounts are provided internet access for IaaS based workloads via the Perimeter Security account. As test workloads can easily be destroyed and recreated between test cycles, and temporarily scaled on-demand during performance testing, accounts in the Test OU are generally small in comparison to their Dev and Prod counterparts.</p>"},{"location":"architectures/sensitive/accounts/#137-functional-ou-prod","title":"1.3.7. Functional OU: Prod","text":"<p>The Prod OU is used to hold accounts at the Production or similarly permissioned (i.e. Pre-Prod) stage of the SDLC cycle containing sensitive unclassified data or workloads.</p> <p>Accounts in this OU host production tools and production line of business application solutions. These accounts are provided internet access for IaaS based workloads via the Perimeter Security account. Accounts in this OU are ideally locked down with only specific Operations and Security personnel having access.</p>"},{"location":"architectures/sensitive/accounts/#138-central-ou","title":"1.3.8. Central OU","text":"<p>The Central OU is used to optionally hold AWS accounts which contain group or team resources used and shared across functional OUs or SDLC boundaries like CI/CD or code promotion tools and software development tooling (source control, testing infrastructure, asset repositories). The architecture supports creating a single central CI/CD or DevOps account, a DevOps account per set of team or application accounts, or combinations in-between. The account structure can be customized to meet each organization's own code promotion and shared tooling requirements. These accounts are provided internet access for IaaS based workloads via the Perimeter Security account.</p>"},{"location":"architectures/sensitive/accounts/#139-functional-ou-unclass-optional","title":"1.3.9. Functional OU: UnClass (Optional)","text":"<p>Non-sensitive workloads should generally be placed with sensitive workloads (Dev/Test/Prod/Central OUs), gaining the extra security benefits of these environments. This OU is used when an organization needs to provide AWS console access to users (internal or external) without appropriate security clearance, to enable deploying and testing AWS services not approved for use with sensitive data, or when services are not available in the local AWS regions which support data locality and sovereignty requirements. These accounts are provided internet access for IaaS based workloads via the Perimeter Security account. Unless this specific use case applies, we generally discourage the use of this OU.</p>"},{"location":"architectures/sensitive/accounts/#1310-suspended-ou","title":"1.3.10. Suspended OU","text":"<p>A suspended OU is created to act as a container for end-of-life accounts which have been closed or suspended, as suspended accounts continue to appear in AWS organizations even after they have been closed and suspended. The <code>DenyAll</code> SCP is applied, which prevents all control-plane API operations from taking place by any account principal. Should a suspended account be unintentionally re-activated, no API operations can be performed without intervention of the cloud team.</p>"},{"location":"architectures/sensitive/accounts/#14-mandatory-accounts","title":"1.4. Mandatory Accounts","text":"<p>The AWS Secure Environment Accelerator Architecture is an opinionated architecture, which partly manifests in the accounts that are deemed mandatory within the organization. The following accounts are assumed to exist, and each has an important function with respect to the goals of the overall Architecture.</p> <p></p>"},{"location":"architectures/sensitive/accounts/#141-organization-management-root","title":"1.4.1. Organization Management (root)","text":"<p>This is the Organization Management or root AWS account. Access to this account must be highly restricted, and should not contain customer resources.</p> <p>As discussed above, the Organization Management (root) account functions as the root of the AWS Organization, the billing aggregator, and attachment point for SCPs. Workloads are not intended to run in this account.</p> <p>Note: Customers deploying the AWS Secure Environment Accelerator Architecture via the [ASEA automation engine][accel_tool] will deploy into this account.</p>"},{"location":"architectures/sensitive/accounts/#142-perimeter-security","title":"1.4.2. Perimeter Security","text":"<p>This account is used for internet facing IaaS based ingress/egress security services. The perimeter account, and in particular the perimeter VPC therein, functions as the single point of IaaS based ingress/egress from the sensitive cloud environment to the public internet and, in some cases, on-premises networks. This provides a central point of network control through which all workload-generated IaaS traffic, both ingress and egress, must transit. The perimeter VPC can host AWS and/or 3rd party next-generation firewalls that provide security services such as virus scanning, malware protection, intrusion protection, TLS inspection, and web application firewall functionality. More details can be found in the Networking section of this document.</p>"},{"location":"architectures/sensitive/accounts/#143-shared-network","title":"1.4.3. Shared Network","text":"<p>This account is used for centralized or shared networking resources. The shared network account hosts the vast majority of the AWS-side of the networking resources throughout the AWS Secure Environment Accelerator Architecture. Workload-scoped VPCs (<code>Dev</code>, <code>Test</code>, <code>Prod</code>, etc.) are deployed in the Shared Network account, and shared via RAM sharing to the respective workload accounts based on their associated OUs. A Transit Gateway provides connectivity from the workloads to the internet or on-premises, without permitting cross-environment (AKA \"East/West traffic\") traffic (e.g. the Transit Gateway provides VRF like separation between the <code>Dev</code> VPC, the <code>Test</code> VPC, and the <code>Prod</code> VPC). More details can be found in the Networking section of this document.</p>"},{"location":"architectures/sensitive/accounts/#144-operations-or-alternatively-called-shared-services","title":"1.4.4. Operations (or alternatively called Shared Services)","text":"<p>This account is used for centralized IT Operational resources (Active Directory, traditional syslog tooling, ITSM, etc.). The operations account provides a central location for the cloud team to provide cloud operation services to other AWS accounts across the organization and is where an organizations cloud operations team \"hangs out\" or delivers tooling applicable across all accounts in the organization. The Operations account has View-Only access to CloudWatch logs and metrics across the organization. It is where centralized Systems Manager Session Manager Automation (remediation) documents are located. It is where organizations centralize backup automation (if automated), SSM inventory and patch jobs are automated, and where AWS Managed Active Directory would typically be deployed and accessible to all workloads in the organization. In some AWS documentation this is referred to as the Shared Services account.</p>"},{"location":"architectures/sensitive/accounts/#145-log-archive","title":"1.4.5. Log Archive","text":"<p>The Log archive account is used to centralize and store immutable logs for the organization. The Log Archive account provides a central aggregation and secure storage point for all audit logs created within the AWS Organization. This account contains a centralized storage location for copies of every account\u2019s audit, configuration compliance, and operational logs. It also provides a storage location for any other audit/compliance logs, as well as application/OS logs.</p> <p>As this account is used to provide long-term retention and immutable storage of logs, it is generally recommended nobody have access to this account. Logs should generally be made directly available for local use by teams working in any account on a shorter-term retention basis. Logs will be auto-ingested into a SIEM-like solution as they are centralized into the Log Archive account for analysis and correlation by auditors and security teams. Access to the Log Archive account would be restricted to deep forensic analysis of logs by auditors and security teams during a forensic investigation.</p>"},{"location":"architectures/sensitive/accounts/#146-security-tooling","title":"1.4.6. Security Tooling","text":"<p>This account is used to centralize access to AWS security tooling and consoles, as well as provide View-Only access for investigative purposes into all accounts in the organization. The security account is restricted to authorized security and compliance personnel, and related security or audit tools. This is an aggregation point for security services, including AWS Security Hub, GuardDuty, Macie, Config, Firewall Manager, Detective, Inspector, and IAM Access Analyzer.</p>"},{"location":"architectures/sensitive/accounts/#147-devops-account-andor-shared-teamapplication-accounts","title":"1.4.7. DevOps account and/or Shared Team/Application accounts","text":"<p>These accounts are used to deliver CI/CD capabilities or shared tooling - two patterns are depicted in the architecture diagram in section 1.2.2 of the Overview - The first has a single organization wide central CI/CD or shared tooling account (named the DevOps account), the other has a CI/CD and shared tooling account per major application team/grouping of teams/applications (named Shared-Team-N). Which pattern is used will depend entirely on the organization's size, maturity model, delegation model of the organization and their team structures. We still generally recommend CI/CD tooling in each developer account (i.e. using Code Commit). When designated branch names are leveraged, the branch/PR will automatically be pulled into the centralized CI/CD tooling account. After approval(s), the code will again be automatically pushed through the SDLC cycle to the Test and/or Prod accounts, as appropriate. Refer to this blog for more details on automating this pattern.</p>"},{"location":"architectures/sensitive/accounts/#148-end-user-or-desktop-account","title":"1.4.8. End User or Desktop account","text":"<p>When an organization configures hybrid on-premises to cloud networking, end-users can directly access the cloud environment using their on-premises organization provided desktops or laptops. When hybrid access is not available, or to enable access from locations and devices whose posture may be not be fully assessed, users can access the organization\u2019s cloud environment via virtualized desktops and/or virtual applications provisioned through the End User or Desktop account. The End User or Desktop account is used to provide your organization\u2019s end-user community access to the applications running in the environment.</p> <p>A dedicated Workstation VPC would be created within the End User account to hold the virtual desktops. Virtual desktops would be created within a security group that prevents and blocks all lateral movement or communications between virtual desktops.</p> <p>Different pools of desktops would exist, with most users only being provisioned desktops which could access the front-end of production applications (or the web tier). Additional pools would be created that provide developers full access to their development environments.</p>"},{"location":"architectures/sensitive/accounts/#15-functional-accounts","title":"1.5. Functional Accounts","text":"<p>Functional accounts are created on demand, and placed into an appropriate OU in the organization structure. The purpose of functional accounts is to provide a secure and managed environment where project teams can use AWS resources. They provide an isolated control plane so that the actions of one team in one account cannot inadvertently affect the work of other teams in other accounts.</p> <p>Functional accounts will gain access to the RAM shared resources based on their respective parent OU. Accounts created for <code>systemA</code> and <code>systemB</code> in the <code>Dev</code> OU would have control plane isolation from each other; however these would both have access to the <code>Dev</code> VPC (shared from the <code>Shared Network</code> account).</p> <p>Data plane isolation within the same VPC is achieved by default, by using appropriate security groups whenever ingress is warranted. For example, the app tier of <code>systemA</code> should only permit ingress from the <code>systemA-web</code> security group, not an overly broad range such as <code>0.0.0.0/0</code>, or even the entire VPCs address range.</p>"},{"location":"architectures/sensitive/accounts/#16-account-level-security-settings","title":"1.6. Account Level Security Settings","text":"<p>The AWS Secure Environment Accelerator Architecture enables certain account-wide features on account creation. Namely, these include:</p> <ol> <li>S3 Public Access Block</li> <li>Default encryption of EBS volumes using a customer managed local account KMS key</li> </ol>"},{"location":"architectures/sensitive/accounts/#17-private-marketplace","title":"1.7. Private Marketplace","text":"<p>The AWS Secure Environment Accelerator Architecture enables the AWS Private Marketplace for the organization. Private Marketplace helps administrators govern which products they want their users to run on AWS by making it possible to see only products that have been allow-listed by the organization based on compliance with an organization's security and procurement policies. When Private Marketplace is enabled, it will replace the standard AWS Marketplace for all users, with the new custom branded and curated Private Marketplace.</p> <p></p>"},{"location":"architectures/sensitive/auth/","title":"1. Authorization and Authentication","text":""},{"location":"architectures/sensitive/auth/#11-overview","title":"1.1. Overview","text":"<p>The AWS Secure Environment Accelerator Architecture makes extensive use of AWS authorization and authentication primitives from the Identity and Access Management (IAM) service as a means to enforce the guardrail objectives of the AWS Secure Environment Accelerator Architecture, and govern access to the set of accounts that makes up the organization.</p>"},{"location":"architectures/sensitive/auth/#12-relationship-to-the-organization-management-root-aws-account","title":"1.2. Relationship to the Organization Management (root) AWS Account","text":"<p>AWS accounts, as a default position, are entirely self-contained with respect to IAM principals - their Users, Roles, Groups are independent and scoped only to themselves. Accounts created by AWS Organizations deploy a default role with a trust policy back to the Organization Management account. While it can be customized, by default this role is named the <code>OrganizationAccountAccessRole</code> (or <code>AWSControlTowerExecution</code> when Control Tower is deployed).</p> <p>As discussed, the AWS Organization resides in the Organization Management (root) account. This account is not used for workloads and is primarily a gateway to the entire cloud footprint for a high-trust principal. It is therefore crucial that all Organization Management account credentials be handled with extreme diligence, and with a U2F hardware key enabled as a second-factor (and stored in a secure location such as a safe) for all users created within this account, including the root user, regardless of privilege level assigned directly within the Management account.</p>"},{"location":"architectures/sensitive/auth/#13-break-glass-accounts","title":"1.3. Break Glass Accounts","text":"<p>The Organization Management account is used to provide break glass access to AWS accounts within the organization. Break glass (which draws its name from breaking the glass to pull a fire alarm) refers to a quick means for a person who does not have access privileges to certain AWS accounts to gain access in exceptional circumstances, using an approved process. Access to AWS accounts within the organization is provided through AWS SSO. The use and creation of IAM users is highly discouraged, with one exception, break glass users. It is generally recommended that organizations create between 2 to 4 IAM break glass users within the Organization Management account. These users would have hardware based MFA enabled and would be leveraged in exceptional circumstances to gain access to the Organization Management account or sub-accounts within the organization by assuming a role. Use cases for break glass access include failure of the organizations IdP, an incident involving the organizations IdP, a failure of AWS SSO, or a disaster involving the loss of an organization\u2019s entire cloud or IdP teams.</p> <p>To re-iterate, access to the Organization Management account grants \u2018super admin\u2019 status, given the organizational-wide trust relationship to the management account. Therefore access to the 2 to 4 break glass IAM users must be tightly controlled, yet accessible via a predefined and strict process. This process often involves one trusted individual having access to a safe containing the password and a different trusted individual having access to a safe with the hardware MFA key \u2013 requiring 2 people to access the break glass credentials.</p> <p>It is worth noting that AWS SCPs are not applicable to the Organization Management account. It is also worth noting that from within the Organization Management account, roles can be assumed in any account within the organization which include broad exclusions from the SCPs (discussed below). These roles are needed to allow the automation tooling to apply and update the guardrails as required, to troubleshoot and resolve issues with the automation tooling, and to bypass the guardrails under approved exception scenarios.</p> <p>Two primary roles are available for access across the organization from the Management account: the {AcceleratorPrefix}PipelineRole which is excluded from the majority of the SCPs to enable the automation tooling to deploy, manage and update the guardrails and provide access to troubleshoot and resolve issues with the automation tooling; and the standard OrganizationAccountAccessRole which has been only been excluded from SCPs which strictly deliver preventative security controls. The OrganizationAccountAccessRole is within the bounds of the SCPs which protect automation tooling deployed guardrails and functionality. Access to either of these roles is available to any IAM users or roles in the Organization Management account.</p>"},{"location":"architectures/sensitive/auth/#14-multi-factor-authentication","title":"1.4. Multi-Factor Authentication","text":"<p>The following are commonly used MFA mechanisms, supported by AWS:</p> <ul> <li>RSA tokens are a strong form of hardware based MFA authentication but can only be assigned on a 1:1 basis. A unique token is required for every user in every account. You cannot utilize the same token for multiple users or across AWS accounts.</li> <li>Yubikeys are U2F compliant devices and also a strong form of hardware based MFA authentication. Yubikeys have the advantage of allowing many:1 assignment, with multiple users and accounts able to use a single Yubikey.</li> <li>Virtual MFA like Google Authenticator on a mobile device is generally considered a good hardware based MFA mechanism, but is not considered as strong as tokens or Yubikeys. Virtual MFA also adds considerations around device charge and is not suitable for break glass type scenarios.</li> <li>SMS text messages and email based one time tokens are generally considered a weak form of MFA based authentication, but still highly desirable over no MFA.</li> </ul> <p>MFA should be used by all users regardless of privilege level with some general guidelines:</p> <ul> <li>Yubikeys provide the strongest form of MFA protection and are strongly encouraged for all account root users and all IAM users in the Organization Management (root) account;</li> <li>the Organization Management (root) account requires a dedicated Yubikey, such that when access is required to a sub-account root user, you do not expose the Organization Management account\u2019s Yubikey;</li> <li>every ~50 sub-accounts requires a dedicated Yubikey to protect the root user, minimizing the required number of Yubikeys and the scope of impact should a Yubikey be lost or compromised;</li> <li>each IAM break glass user requires a dedicated Yubikey, as do any additional IAM users in the Organization Management (root) account. While some CSPs do not recommend MFA on the break glass users, it is strongly encouraged in AWS;</li> <li>the MFA devices for all account root users including the management account and the IAM break glass users should be securely stored, with well defined access policies and procedures;</li> <li>all other AWS users (AWS SSO, IAM in sub-accounts, etc.) regardless of privilege level should leverage virtual MFA devices (like Google Authenticator on a mobile device).</li> </ul>"},{"location":"architectures/sensitive/auth/#15-control-plane-access-via-aws-sso","title":"1.5. Control Plane Access via AWS SSO","text":"<p>The vast majority of end-users of the AWS cloud within the organization will never use or interact with the Organization Management account, or the root users of any child account in the organization. The AWS Secure Environment Accelerator Architecture recommends instead that AWS SSO be provisioned in the Organization Management account (a rare case where Organization Management account deployment is mandated). SSO administration is then delegated to the Operations account, to further minimize access to the highly restricted management account. Once delegation is in place, the location of the AWS SSO identity source is also delegated, enabling AWS SSO to directly connect to a Managed Active Directory (AD) or other IdP in the Operations account (this previously required an AWS Directory Connector deployed in the Organization Management account).</p> <p>Users will login to AWS via the web-based endpoint for the AWS SSO service:</p> <p></p> <p>AWS SSO then authenticates the user based on the connected Managed Microsoft AD installation (in the Operations account). Based on group membership, the user will be presented with a set of roles to assume into assigned accounts. For example, a developer may be placed into groups that permit Administrative access in a specific developer account and Read-Only access in a test account; meanwhile an IT Cloud Administrator may have high-privilege access to most, or all, accounts. In effect, AWS SSO adds SAML IdP capabilities to the AWS Managed Microsoft AD, with the AWS Console acting as a service-provider (SP) in SAML parlance. Other SAML-aware SPs may also be used with AWS SSO.</p>"},{"location":"architectures/sensitive/auth/#151-sso-user-roles","title":"1.5.1. SSO User Roles","text":"<p>AWS SSO automatically creates an identity provider (IdP) and associated roles in each account in the organization. The roles used by end users have a trust policy to this IdP. When a user authenticates to AWS SSO (via the underlying Managed AD) and selects a role to assume based on their group membership, the SSO service provides the user with temporary security credentials unique to the role session. In such a scenario, the user has no long-term credentials (e.g. password, or access keys) and instead uses their temporary security credentials.</p> <p>Users, via their AD group membership, are ultimately assigned to SSO user roles via the use of AWS SSO permission sets. A permission set is an assignment of a particular permission policy to an AWS account. For example:</p> <p>An organization might decide to use AWS Managed Policies for Job Functions that are located within the SSO service as the baseline for role-based-access-control (RBAC) separation within an AWS account. This enables job function policies such as:</p> <ul> <li>Administrator - This policy provides full access to all AWS services and resources in the account;</li> <li>Power User - Provides full access to AWS services and resources, but does not allow management of users, groups and policies;</li> <li>Database Administrator - Grants full access permissions to AWS services and actions required to set up and configure AWS database services;</li> <li>View-Only User - This policy grants permissions to view resources and basic metadata across all AWS services. It does not provide access to get or read workload data.</li> </ul>"},{"location":"architectures/sensitive/auth/#152-principal-authorization","title":"1.5.2. Principal Authorization","text":"<p>Having assumed a role, a user\u2019s permission level within an AWS account with respect to any API operation is governed by the IAM policy evaluation logic flow (detailed here):</p> <p></p> <p>Having an <code>allow</code> to a particular API operation on the role (i.e. session policy) does not necessarily imply that API operation will succeed. As depicted above, a deny at any level in the evaluation logic will block access to the API call; for example a restrictive permission boundary or an explicit <code>deny</code> at the resource or SCP level will block the call. SCPs are used extensively as a guardrailing mechanism in the AWS Secure Environment Accelerator Architecture, and are discussed in a later section.</p>"},{"location":"architectures/sensitive/auth/#16-root-authorization","title":"1.6. Root Authorization","text":"<p>Every AWS account has a set of root credentials. These root credentials are generated on account creation with a random 64-character password. It is important that the root credentials for each account be recovered and MFA enabled via the AWS root credential password reset process using the account\u2019s unique email address. To further protect these credentials, the AWS Secure Environment Accelerator Architecture specifically denies the use of the root user via SCP. Root credentials authorize all actions for all AWS services and for all resources in the account (except anything denied by SCPs). There are some actions which only root has the capability to perform which are found within the AWS documentation. These are typically rare operations (e.g. creation of X.509 keys), and should not be required in the normal course of business. Root credentials should be handled with extreme diligence, with MFA enabled per the guidance in the previous section.</p>"},{"location":"architectures/sensitive/auth/#17-service-roles","title":"1.7. Service Roles","text":"<p>A service role is an IAM role that a service assumes to perform actions in an account on the user\u2019s behalf. When a user sets up an AWS service, the user must define an IAM role for the service to assume. This service role must include all the permissions that are required for the service to access the AWS resources that it needs. Service roles provide access only within a single account and cannot be used to grant access to services in other accounts. Users can create, modify, and delete a service role from within the IAM service. For example, a user can create a role that allows Amazon Redshift to access an Amazon S3 bucket on the user\u2019s behalf and then load data from that bucket into an Amazon Redshift cluster. In the case of SSO, during the process in which AWS SSO is enabled, the AWS Organizations service grants AWS SSO the necessary permissions to create subsequent IAM roles.</p>"},{"location":"architectures/sensitive/auth/#18-service-control-policies","title":"1.8. Service Control Policies","text":"<p>Service Control Policies are a key preventative control used by the AWS Secure Environment Accelerator Architecture. It is crucial to note that SCPs, by themselves, never grant permissions. They are most often used to <code>Deny</code> certain actions at an OU, or account level within an AWS Organization. Since <code>deny</code> always overrides <code>allow</code> in the IAM policy evaluation logic, SCPs can have a powerful effect on all principals in any account, and can wholesale deny entire categories of actions irrespective of the permission policy attached to the principal itself - even the root user of the account.</p> <p>SCPs follow an inheritance pattern from all levels of the hierarchy down to the account of the organization:</p> <p></p> <p>In order for any principal to be able to perform an action A, it is necessary (but not sufficient) that there is an <code>Allow</code> on action A from all levels of the hierarchy down to the account, and no explicit <code>Deny</code> anywhere. This is discussed in further detail in How SCPs Work.</p> <p>The AWS Secure Environment Accelerator Architecture leverages the following SCPs in the organization:</p>"},{"location":"architectures/sensitive/auth/#181-sensitive-policy","title":"1.8.1. Sensitive policy","text":"<p>This is a comprehensive policy whose main goal is to provide a compliant cloud environment for medium sensitivity workloads, namely prohibiting any non-centralized networking, data-at-rest encryption and mandating data residency in the home region. It should be attached to all top-level OUs with the exception of Unclassified and Sandbox.</p> Policy Statement ID (SID) Description <code>PMP</code> Prevents the modification or creation of AWS Private Marketplace <code>ROOT</code> Prevents the use of the root user <code>EBS1</code> Prevents starting EC2 instances without volume level encryption <code>EBS2</code> Prevents the creation of an unencrypted EBS volume <code>EFS1</code> Prevents the creation of an unencrypted EFS volume <code>RDS</code> Prevents the creation of an unencrypted RDS instance <code>AUR</code> Prevents the creation of an unencrypted RDS Aurora cluster <code>OTHS</code> Blocks miscellaneous operations and services including Leave Organization, Modify Payment Methods, Object Sharing, Disabling SSO, and Lightsail, Sumerian, Gamelift, AppFlow, and IQ. <code>NET2</code> Prevents the creation of any networking infrastructure such as VPCs, gateways, peering, VPN, etc. Additionally blocks making objects public (RDS, EMR, EC2, etc.) and the creation of IAM users, groups and access keys <code>GBL2</code> Within services that are exempted from GBL1, scope the use of those services to the us-east-1 region (ACM, KMS, SNS) <code>GBL1</code> Prevents the use of any service in any non-approved AWS region with the exception of services that are considered global; e.g. CloudFront, IAM, STS, etc."},{"location":"architectures/sensitive/auth/#1811-encryption-at-rest","title":"1.8.1.1. Encryption at Rest","text":"<p>Note that the Encryption SCP statements above, taken together, mandate encryption at rest for block storage volumes used in EC2 and RDS instances.</p>"},{"location":"architectures/sensitive/auth/#182-unclassified-policy","title":"1.8.2. Unclassified policy","text":"<p>This policy is only used on the Unclassified OU. This policy is broadly similar to Sensitive; however it relaxes the requirement for non-approved region usage to include additional approved regions. The <code>GLBL2</code> services are moved into the <code>GLBL1</code> policy.</p>"},{"location":"architectures/sensitive/auth/#183-sandbox-policy","title":"1.8.3. Sandbox policy","text":"<p>This policy is only used on the Sandbox OU and is only appropriate for accounts in which AWS service experimentation is taking place. This policy is broadly similar to Unclassified; however it does not prohibit network infrastructure creation (e.g. VPCs, IGWs), dropping the <code>NET2</code> section.</p>"},{"location":"architectures/sensitive/auth/#184-guardrail-protection-parts-0-and-1","title":"1.8.4. Guardrail Protection (Parts 0 and 1)","text":"<p>These guardrails apply across the organization and protect the guardrails and infrastructure deployed by the automation tooling. Note that this policy is split into two parts due to a current limitation of SCP sizing, but logically it should be considered a single policy.</p> Policy Statement ID (SID) Description <code>TAG1</code> Prevents creation, deletion and modification of a protected security group <code>TAG2</code> Prevents creation, deletion and modification and use of any protected IAM resource <code>S3</code> Prevents deletion and modification of any S3 bucket used by the automation tooling incl. centralized logs <code>CFN</code> Prevents creation, deletion or modification of any CloudFormation stacks deployed by the automation tooling <code>ALM</code> Prevents deletion and modification of protected cloudwatch alarms which alert on significant control plane events <code>ROL</code> Prevents any IAM operation on protected IAM resources <code>SSM</code> Prevents creation, deletion or modification of any SSM resource deployed by the automation tooling <code>LOG</code> Prevents the deletion and modification of any CloudWatch Log groups and VPC flow logs <code>LOG2</code> Additional CloudWatch Log group protections <code>LAM</code> Prevents the creation, deletion and modification of any Lambda functions deployed by the automation tooling <code>NET1</code> Prevents deletion of any protected networking (VPC) constructs like subnets and route tables <code>NFW</code> Prevents destructive operations on protected AWS Network Firewalls <code>CT</code> Prevents deletion and modification of protected Cloud Trails <code>CON</code> Protects AWS Config configuration from modification or deletion <code>CWE</code> Prevents deletion and modification of protected CloudWatch events <code>RUL</code> Protects AWS Config rules from modification or deletion <code>Deny</code> Protects deletion and modification of protected KMS keys <code>IAM</code> Protects creation, deletion, and modification of protected IAM policies <code>ACM</code> Prevents deletion of a protected certificates and Load Balancers <code>SEC</code> Prevents the deletion and modification to AWS security services like GuardDuty, Security Hub, Macie, Firewall Manager, Access Analyzer, password policies, and resource shares <code>SNS</code> Prevents creation, deletion and modification of a protected SNS topics <code>RDGW</code> Prevents the modification of the role used for Remote Desktop Gateway <code>KIN</code> Prevents creation, deletion and modification of a protected Kinesis streams <p>Note: Two versions of the Part-0 policy exist (<code>CoreOUs</code> and <code>WkldOUs</code>), with the Wkld OUs version of the policy removing <code>SNS</code>, <code>RDGW</code>, and <code>KIN</code> sections as they are not relevant outside the Security and Infrastructure OUs.</p>"},{"location":"architectures/sensitive/auth/#185-quarantine-new-object","title":"1.8.5. Quarantine New Object","text":"<p>This policy is attached to an account to \u2018quarantine\u2019 it - to prevent any AWS operation from taking place. This is useful in the case of an account with credentials which are believed to have been compromised. This policy is also applied to new accounts upon creation. After the installation of guardrails, it is removed. In the meantime, it prevents all AWS control plane operations except by principals required to deploy guardrails.</p> Policy Statement ID (SID) Description <code>DenyAllAWSServicesExceptBreakglassRoles</code> Blanket denial on all AWS control plane operations for all non-break-glass roles"},{"location":"architectures/sensitive/diagrams/","title":"1. Prescriptive Sensitive Sample Architecture Diagrams","text":""},{"location":"architectures/sensitive/diagrams/#11-shared-vpc-architecture","title":"1.1. Shared VPC Architecture","text":""},{"location":"architectures/sensitive/diagrams/#12-spoke-vpc-architecture","title":"1.2. Spoke VPC Architecture","text":""},{"location":"architectures/sensitive/diagrams/#13-vpc-and-security-group-patterns","title":"1.3. VPC and Security Group Patterns","text":""},{"location":"architectures/sensitive/diagrams/#14-additional-perimeter-patterns","title":"1.4. Additional Perimeter Patterns","text":""},{"location":"architectures/sensitive/logging/","title":"1. Logging and Monitoring","text":""},{"location":"architectures/sensitive/logging/#11-overview","title":"1.1. Overview","text":"<p>The AWS Secure Environment Accelerator Architecture requires the following security services be deployed across the organization. These services, taken together, provide a comprehensive picture of the security posture of the organization.</p>"},{"location":"architectures/sensitive/logging/#12-cloudtrail","title":"1.2. CloudTrail","text":"<p>The AWS CloudTrail service provides a comprehensive log of control plane and data plane operations (audit history) of all actions taken against most AWS services, including users logging into accounts. A CloudTrail Organizational trail should be deployed into the organization. For each account, this captures management events and optionally S3 data plane events taking place by every principal in every account in the organization. These records are sent to both a CloudWatch log group in the Organization Management account and an S3 bucket in the Log Archive account. The trail itself cannot be modified or deleted by any principal in any child account. This provides an audit trail for detective purposes in the event of the need for forensic analysis into account usage. The logs themselves provide an integrity guarantee: every hour, CloudTrail produces a digest of that hour\u2019s log files, with a hash, and signs it with its own private key. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection. This process is detailed here. The Log Archive bucket is protected with SCPs and has versioning enabled ensuring deleted or overwritten files are retained.</p>"},{"location":"architectures/sensitive/logging/#13-vpc-flow-logs","title":"1.3. VPC Flow Logs","text":"<p>VPC Flow Logs capture information about the IP traffic going to and from network interfaces in an AWS Account VPC such as source and destination IPs, protocol, ports, and success/failure of the flow. The AWS Secure Environment Accelerator Architecture enables ALL (i.e. both accepted and rejected traffic) logs for all VPCs in all accounts in both a local CloudWatch log group and an S3 bucket in the log-archive account. It is important to use custom flow log formats to ensure all fields are captured as important fields are not part of the basic format. More details about VPC Flow Logs are available here.</p> <p>It should be noted that certain categories of network flows are not captured, including traffic to and from the instance metadata service (<code>169.254.169.254</code>), and DNS traffic with an Amazon VPC resolver (available in DNS resolver logs).</p>"},{"location":"architectures/sensitive/logging/#14-guardduty","title":"1.4. GuardDuty","text":"<p>Amazon GuardDuty is a cloud native threat detection and Intrusion Detection Service (IDS) that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats. GuardDuty uses a number of data sources including VPC Flow Logs, DNS logs, CloudTrail logs and several threat feeds.</p> <p>The AWS Secure Environment Accelerator Architecture requires GuardDuty be enabled at the Organization level, and delegating the Security account as the GuardDuty Administrative account. The GuardDuty Administrative account should be auto-enabled to add new accounts as they come online. Note that this should be done in every region as a defense in depth measure, with the understanding that the SCPs will prevent service usage in all other regions.</p>"},{"location":"architectures/sensitive/logging/#15-config","title":"1.5. Config","text":"<p>AWS Config provides a detailed view of the resources associated with each account in the AWS Organization, including how they are configured, how they are related to one another, and how the configurations have changed on a recurring basis. Resources can be evaluated on the basis of their compliance with Config Rules - for example, a Config Rule might continually examine EBS volumes and check that they are encrypted.</p> <p>Config may be enabled at the Organization level - this provides an overall view of the compliance status of all resources across the organization. The AWS Config multi-account multi-region data aggregation capability has been located in both the Organization Management account and the Security account.</p>"},{"location":"architectures/sensitive/logging/#16-cloudwatch-logs","title":"1.6. CloudWatch Logs","text":"<p>CloudWatch Logs is AWS\u2019 log aggregator service, used to monitor, store, and access log files from EC2 instances, AWS CloudTrail, Route 53, and other sources. The AWS Secure Environment Accelerator Architecture requires that log subscriptions are created for all log groups in all workload accounts, and streamed into S3 in the log-archive account (via Kinesis) for analysis and long-term audit purposes. The CloudWatch log group retention period on all log groups should be set to a medium retention period (such as 2 years) to enable easy online access.</p>"},{"location":"architectures/sensitive/logging/#17-securityhub","title":"1.7. SecurityHub","text":"<p>The primary dashboard for Operators to assess the security posture of the AWS footprint is the centralized AWS Security Hub service. Security Hub needs to be configured to aggregate findings from Amazon GuardDuty, Amazon Macie, AWS Config, Systems Manager, Firewall Manager, Amazon Detective, Amazon Inspector and IAM Access Analyzers. Events from security integrations are correlated and displayed on the Security Hub dashboard as \u2018findings\u2019 with a severity level (informational, low, medium, high, critical).</p> <p>The AWS Secure Environment Accelerator Architecture recommends that the current 3 Security Hub frameworks be enabled, specifically:</p> <ul> <li>AWS Foundational Security Best Practices v1.0.0</li> <li>PCI DSS v3.2.1</li> <li>CIS AWS Foundations Benchmark v1.2.0</li> </ul> <p>These frameworks will perform checks against the accounts via Config Rules that are evaluated against the AWS Config resources in scope. See the above links for a definition of the associated controls.</p>"},{"location":"architectures/sensitive/logging/#18-systems-manager-session-manager","title":"1.8. Systems Manager Session Manager","text":"<p>Session Manager is a fully managed AWS Systems Manager capability that lets you manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, on-premises instances, and virtual machines (VMs) through an interactive one-click browser-based shell or through the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also makes it easy to comply with corporate policies that require controlled access to instances, strict security practices, and fully auditable logs with instance access details, while still providing end users with simple one-click cross-platform access to your managed instances.1</p> <p>The AWS Secure Environment Accelerator Architecture stores encrypted session log data in both CloudWatch logs and in the centralized S3 bucket for auditing purposes.</p>"},{"location":"architectures/sensitive/logging/#19-systems-manager-inventory","title":"1.9. Systems Manager Inventory","text":"<p>AWS Systems Manager Inventory provides visibility into your AWS computing environment. AWS ASEA architecture uses SSM Inventory to collect metadata from your managed nodes and stores this metadata in the central Log Archive S3 bucket. These logs enable customers to quickly determine which nodes are running the software and configurations required by your software policy, and which nodes need to be updated.</p>"},{"location":"architectures/sensitive/logging/#110-other-services","title":"1.10. Other Services","text":"<p>The following additional services are configured with their organization-wide administrative and visibility capabilities centralized to the Security account: Macie, Firewall Manager, Access Analyzer. The following additional logging and reporting services are configured: CloudWatch Alarms, Cost and Usage Reports, rsyslog, MAD, R53 logs, OS/App, ELB, OpenSearch SIEM.</p>"},{"location":"architectures/sensitive/network/","title":"1. Networking","text":""},{"location":"architectures/sensitive/network/#11-overview","title":"1.1. Overview","text":"<p>The AWS Secure Environment Accelerator Architecture networking is built on a principle of centralized on-premises and internet ingress/egress, while enforcing data plane isolation between workloads in different environments. Connectivity to on-premises environments, internet egress, shared resources and AWS APIs are mediated at a central point of ingress/egress via the use of a Transit Gateway. Consider the following overall network diagram:</p> <p></p> <p>All functional accounts use RAM-shared networking infrastructure as depicted above. The workload VPCs (Dev, Test, Prod, etc) are hosted in the Shared Network account and made available to the appropriate accounts based on their OU in the organization.</p>"},{"location":"architectures/sensitive/network/#12-perimeter","title":"1.2. Perimeter","text":"<p>The perimeter VPC hosts the organization's perimeter security services. The Perimeter VPC is used to control the flow of traffic between AWS Accounts and external networks for IaaS workloads: both public (internet) and in some cases private (access to on-premises datacenters). This VPC hosts AWS Network Firewall and/or 3rd party Next Generation Firewalls (NGFW) that provide perimeter security services including virus scanning / malware protection, Intrusion Protection services, TLS Inspection and Web Application Firewall protection. If applicable, this VPC also hosts reverse proxy servers.</p> <p>Note that this VPC is in its own isolated account, separate from Shared Network, in order to facilitate networking and security 'separation of duties'. Internal networking teams may administer the cloud networks in Shared Network without being granted permission to administer the security perimeter itself.</p>"},{"location":"architectures/sensitive/network/#121-ip-ranges","title":"1.2.1. IP Ranges","text":"<ul> <li>Primary Range: The AWS Secure Environment Accelerator Architecture recommends that the perimeter VPC have a primary range in the RFC1918 block (e.g. <code>10.7.4.0/22</code>), used only for subnets dedicated to 'detonation' purposes. This primary range, in an otherwise-unused RFC1918 range, is not intended to be routable outside of the VPC, and is reserved for future use with malware detonation capabilities of NGFW devices.</li> <li>Secondary Range: This VPC should also have a secondary range in the RFC6598 block (e.g. <code>100.96.250.0/23</code>) used for the overlay network (NGFW devices inside VPN tunnel) for all other subnets. This secondary range is assigned by an external entity, and should be carefully selected in order to co-exist with AWS Secure Environment Accelerator Architecture deployments that exist at peer organizations; for instance other government departments that maintain a relationship with the same shared entity in a carrier-grade NAT topology. Although this is a 'secondary' range in VPC parlance, this VPC CIDR should be interpreted as the more 'significant' of the two with respect to Transit Gateway routing; the Transit Gateway will only ever interact with this 'secondary' range.</li> </ul> <p>This VPC has four subnets per AZ, each of which hosts a port used by the NGFW devices, which are deployed in an HA pair. The purpose of these subnets is as follows.</p> <ul> <li>Detonation: This is an unused subnet reserved for future use with malware detonation capabilities of the NGFW devices (e.g. <code>10.7.4.0/24</code> - not routable except local);</li> <li>Proxy: This subnet hosts reverse proxy services for web and other protocols. It also contains the three interface endpoints necessary for AWS Systems Manager Session Manager, which enables SSH-less CLI access to authorized and authenticated principals in the perimeter account (e.g. <code>100.96.251.64/26</code>);</li> <li>On-Premises: This subnet hosts the private interfaces of the firewalls, corresponding to connections from the on-premises network (e.g. <code>100.96.250.192/26</code>);</li> <li>FW-Management: This subnet is used to host management tools and the management of the Firewalls itself (e.g. <code>100.96.251.160/27</code> - a smaller subnet is permissible due to modest IP requirements for management instances);</li> <li>Public: This subnet is the public-access zone for the perimeter VPC. It hosts the public interface of the firewalls, as well as application load balancers that are used to balance traffic across the firewall pair. There is one Elastic IPv4 address per public subnet that corresponds to the IPSec Customer Gateway (CGW) for the VPN connections into the Transit Gateway in Shared Networking (e.g. <code>100.96.250.0/26</code>).</li> </ul> <p>Outbound internet connections (for software updates, etc.) can be initiated from within the workload VPCs, and use the transparent proxy feature of the next-gen Firewalls.</p> <p>Note on VPN Tunnel Redundancy: Each NGFW device manifests as a unique CGW on the AWS side (shared network account) of the IPSec VPNs. Moreover, there are two Site-to-Site VPNs in this architecture, each with two active tunnels; this configuration is highly redundant as only one tunnel is required per firewall to provide complete redundancy.</p>"},{"location":"architectures/sensitive/network/#13-shared-network","title":"1.3. Shared Network","text":"<p>The Shared Network account, and the AWS networking resources therein, form the core of the cloud networking infrastructure across the account structure. Rather than the individual accounts defining their own networks, these are instead centralized here and shared out to the relevant OUs. Principals in a Dev OU will have access to a Dev VPC, Test OU will have access to a Test VPC and so on - all of which are owned by this account.</p> <p>You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with AWS Resource Access Manager (RAM). The RAM service eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account.</p>"},{"location":"architectures/sensitive/network/#131-transit-gateway","title":"1.3.1. Transit Gateway","text":"<p>The Transit Gateway is a central hub that performs several core functions within the Shared Network account.</p> <ol> <li>Routing of permitted flows; for example a Workload to On-premises via the Perimeter VPC.<ul> <li>All routing tables in Shared Network VPCs send <code>0.0.0.0/0</code> traffic to the TGW, where its handling will be determined by the TGW Route Table (TGW-RT) that its attachment is associated with. For example:<ul> <li>an HTTP request to <code>registry.hub.docker.com</code> from the Test VPC will go to the TGW</li> <li>The Segregated TGW RT will direct that traffic to the Perimeter VPC via the IPsec VPNs</li> <li>The request will be proxied to the internet, via GC-CAP if appropriate</li> <li>The return traffic will again transit the IPsec VPNs</li> <li>The <code>10.3.0.0/16</code> bound response traffic will arrive at the Core TGW RT, where a propagation in that TGW RT will direct the response back to the Test VPC.</li> </ul> </li> </ul> </li> <li>Defining separate routing domains that prohibit undesired east-west flows at the network level; for example, by prohibiting Dev to Prod traffic. For example:<ul> <li>All routing tables in Shared Network VPCs send <code>0.0.0.0/0</code> traffic to the TGW, which defines where the next permissible hop is. For example, <code>10.2.0.0/16</code> Dev traffic destined for the <code>10.0.4.0/16</code> Prod VPC will be blocked by the blackhole route in the Segregated TGW RT.</li> </ul> </li> <li>Enabling centralization of shared resources; namely a shared Microsoft AD installation in the Central VPC, and access to shared VPC Endpoints in the Endpoint VPC.<ul> <li>The Central VPC, and the Endpoint VPC are routable from Workload VPCs. This provides an economical way to share organization-wide resources that are nonetheless isolated into their own VPCs. For example:<ul> <li>a <code>git</code> request in the <code>Dev</code> VPC to <code>git.private-domain.ca</code> resolves to a <code>10.1.0.0/16</code> address in the <code>Central</code> VPC.</li> <li>The request from the <code>Dev</code> VPC will go to the TGW due to the VPC routing table associated with that subnet</li> <li>The TGW will send the request to the <code>Central</code> VPC via an entry in the Segregated TGW RT</li> <li>The <code>git</code> response will go to the TGW due to the VPC routing table associated with that subnet</li> <li>The Shared TGW RT will direct the response back to the <code>Dev</code> VPC</li> </ul> </li> </ul> </li> </ol> <p>The four TGW RTs exist to serve the following main functions:</p> <ul> <li>Segregated TGW RT: Used as the association table for the workload VPCs; prevents east-west traffic, except to shared resources.</li> <li>Core TGW RT: Used for internet/on-premises response traffic, and Endpoint VPC egress.</li> <li>Shared TGW RT: Used to provide <code>Central</code> VPC access east-west for the purposes of response traffic to shared workloads</li> <li>Standalone TGW RT: Reserved for future use. Prevents TGW routing except to the Endpoint VPC.</li> </ul> <p>Note that a unique BGP ASN will need to be used for the TGW.</p>"},{"location":"architectures/sensitive/network/#132-endpoint-vpc","title":"1.3.2. Endpoint VPC","text":"<p>DNS functionality for the network architecture is centralized in the Endpoint VPC. It is recommended that the Endpoint VPC use a RFC1918 range - e.g. <code>10.7.0.0/22</code> with sufficient capacity to support 60+ AWS services and future endpoint expansion, and inbound and outbound resolvers (all figures per AZ).</p> <p></p>"},{"location":"architectures/sensitive/network/#133-endpoint-vpc-interface-endpoints","title":"1.3.3. Endpoint VPC: Interface Endpoints","text":"<p>The endpoint VPC hosts VPC Interface Endpoints (VPCEs) and associated Route 53 private hosted zones for all applicable services in the designated home region. This permits traffic destined for an eligible AWS service; for example SQS, to remain entirely within the Shared Network account rather than transiting via the IPv4 public endpoint for the service:</p> <p></p> <p>From within an associated workload VPC such as <code>Dev</code>, the service endpoint (e.g. <code>sqs.ca-central-1.amazonaws.com</code>) will resolve to an IP in the <code>Endpoint</code> VPC:</p> <pre><code>sh-4.2$ nslookup sqs.ca-central-1.amazonaws.com\nServer:         10.2.0.2                  # Dev VPC's .2 resolver.\nAddress:        10.2.0.2#53\n\nNon-authoritative answer:\nName:   sqs.ca-central-1.amazonaws.com\nAddress: 10.7.1.190                       # IP in Endpoint VPC - AZ-a.\nName:   sqs.ca-central-1.amazonaws.com\nAddress: 10.7.0.135                       # IP in Endpoint VPC - AZ-b.\n</code></pre> <p>This cross-VPC resolution of the service-specific private hosted zone functions via the association of each VPC to each private hosted zone, as depicted above.</p>"},{"location":"architectures/sensitive/network/#134-endpoint-vpc-hybrid-dns","title":"1.3.4. Endpoint VPC: Hybrid DNS","text":"<p>The Endpoint VPC also hosts the common DNS infrastructure used to resolve DNS queries:</p> <ul> <li>within the cloud</li> <li>from the cloud to on-premises</li> <li>from on-premises to the cloud</li> </ul>"},{"location":"architectures/sensitive/network/#1341-within-the-cloud","title":"1.3.4.1. Within The Cloud","text":"<p>In-cloud DNS resolution applies beyond the DNS infrastructure that is put in place to support the Interface Endpoints for the AWS services in-region. Other DNS zones, associated with the Endpoint VPC, are resolvable the same way via an association to workload VPCs.</p>"},{"location":"architectures/sensitive/network/#1342-from-cloud-to-on-premises","title":"1.3.4.2. From Cloud to On-Premises","text":"<p>DNS Resolution from the cloud to on-premises is handled via the use of a Route 53 Outbound Endpoint, deployed in the Endpoint VPC, with an associated Resolver rule that forwards DNS traffic to the outbound endpoint. Each VPC is associated to this rule.</p> <p></p>"},{"location":"architectures/sensitive/network/#1343-from-on-premises-to-cloud","title":"1.3.4.3. From On-Premises to Cloud","text":"<p>Conditional forwarding from on-premises networks is made possible via the use of a Route 53 Inbound Endpoint. On-premises networks send resolution requests for relevant domains to the endpoints deployed in the Endpoint VPC:</p> <p></p>"},{"location":"architectures/sensitive/network/#135-workload-vpcs","title":"1.3.5. Workload VPCs","text":"<p>The workload VPCs are where line of business applications ultimately reside, segmented by environment (<code>Dev</code>, <code>Test</code>, <code>Prod</code>, etc). It is recommended that the Workload VPC use a RFC1918 range (e.g. <code>10.2.0.0/16</code> for <code>Dev</code>, <code>10.3.0.0/16</code> for <code>Test</code>, etc).</p> <p></p> <p>Note that security groups are recommended as the primary data-plane isolation mechanism between applications that may coexist in the same VPC. It is anticipated that unrelated applications would coexist in their respective tiers without ever permitting east-west traffic flows.</p> <p>The following subnets are defined by the AWS Secure Environment Accelerator Architecture:</p> <ul> <li>TGW subnet: This subnet hosts the elastic-network interfaces for the TGW attachment. A <code>/27</code> subnet is sufficient.</li> <li>Web subnet: This subnet hosts front-end or otherwise 'client' facing infrastructure. A <code>/20</code> or larger subnet is recommended to facilitate auto-scaling.</li> <li>App subnet: This subnet hosts app-tier code (EC2, containers, etc). A <code>/19</code> or larger subnet is recommended to facilitate auto-scaling.</li> <li>Data subnet: This subnet hosts data-tier code (RDS instances, ElastiCache instances). A <code>/21</code> or larger subnet is recommended.</li> <li>Mgmt subnet: This subnet hosts bastion or other management instances. A <code>/21</code> or larger subnet is recommended.</li> </ul> <p>Each subnet is associated with a Common VPC Route Table, as depicted above. Gateway Endpoints for relevant services (Amazon S3, Amazon DynamoDB) are installed in the Common route tables of all Workload VPCs. Aside from local traffic or gateway-bound traffic, <code>0.0.0.0/0</code> is always destined for the TGW.</p>"},{"location":"architectures/sensitive/network/#1351-security-groups","title":"1.3.5.1. Security Groups","text":"<p>Security Groups are instance level stateful firewalls, and represent a foundational unit of network segmentation across AWS networking. Security groups are stateful, and support ingress/egress rules based on protocols and source/destinations. While CIDR ranges are supported by the latter, it is preferable to instead use other security groups as sources/destinations. This permits a higher level of expressiveness that is not coupled to particular CIDR choices and works well with autoscaling; e.g.</p> <p>\"permit port 3306 traffic from the <code>App</code> tier to the <code>Data</code> tier\"</p> <p>versus</p> <p>\"permit port 3306 traffic from <code>10.0.1.0/24</code> to <code>10.0.2.0/24</code>.</p> <p>Security group egress rules are often used in 'allow all' mode (<code>0.0.0.0/0</code>), with the focus primarily being on consistently allow listing required ingress traffic. This ensures day to day activities like patching, access to Windows DNS, and to directory services can function without friction. The provided sample security groups in the workload accounts offers a good balance that considers both security, ease of operations, and frictionless development. They allow developers to focus on developing, enabling them to simply use the pre-created security constructs for their workloads, and avoid the creation of wide-open security groups. Developers can equally choose to create more appropriate least-privilege security groups more suitable for their application, if they are skilled in this area. It is expected as an application is promoted through the SDLC cycle from Dev through Test to Prod, these security groups will be further refined by the extended customer teams to further reduce privilege, as appropriate. It is expected that each customer will review and tailor their Security Groups based on their own security requirements.</p>"},{"location":"architectures/sensitive/network/#1352-nacls","title":"1.3.5.2. NACLs","text":"<p>Network Access-Control Lists (NACLs) are stateless constructs used sparingly as a defense-in-depth measure in this architecture. AWS generally discourages the use of NACLs given the added complexity and management burden, given the availability and ease of use provided by security groups. Each network flow often requires four NACL entries (egress from ephemeral, ingress to destination, egress from destination, ingress to ephemeral). The architecture recommends NACLs as a segmentation mechanism for <code>Data</code> subnets; i.e. <code>deny</code> all inbound traffic to such a subnet except that which originates in the <code>App</code> subnet for the same VPC. As with security groups, we encourage customers to review and tailor their NACLs based on their own security requirements.</p>"},{"location":"architectures/sensitive/network/#136-central-vpc","title":"1.3.6. Central VPC","text":"<p>The Central VPC is a network for localizing operational infrastructure that may be needed across the organization, such as code repositories, artifact repositories, and notably, the managed Directory Service (Microsoft AD). Instances that are domain joined will connect to this AD domain - a network flow that is made possible from anywhere in the network structure due to the inclusion of the Central VPC in all relevant association TGW RTs.</p> <p>It is recommended that the Central VPC use a RFC1918 range (e.g. <code>10.1.0.0/16</code>) for the purposes of routing from the workload VPCs, and a secondary range from the RFC6598 block (e.g. <code>100.96.252.0/23</code>) to support the Microsoft AD workload.</p> <p></p>"},{"location":"architectures/sensitive/network/#1361-domain-joining","title":"1.3.6.1. Domain Joining","text":"<p>An EC2 instance deployed in the Workload VPCs can join the domain corresponding to the Microsoft AD in <code>Central</code> provided the following conditions are all true:</p> <ol> <li>The instance needs a network path to the Central VPC (given by the Segregated TGW RT), and appropriate security group assignment;</li> <li>The Microsoft AD should be 'shared' with the account the EC2 instance resides in (The AWS Secure Environment Accelerator Architecture recommends these directories are shared to workload accounts);</li> <li>The instance has the AWS managed policies <code>AmazonSSMManagedInstanceCore</code> and <code>AmazonSSMDirectoryServiceAccess</code> attached to its IAM role, or runs under a role with at least the permission policies given by the combination of these two managed policies; and</li> <li>The EC2's VPC has an associated resolver rule that directs DNS queries for the AD domain to the Central VPC.</li> </ol>"},{"location":"architectures/sensitive/network/#137-sandbox-vpc","title":"1.3.7. Sandbox VPC","text":"<p>A sandbox VPC, not depicted, may be included in the AWS Secure Environment Accelerator Architecture. This is not connected to the Transit Gateway, Perimeter VPC, on-premises network, or other common infrastructure. It contains its own Internet Gateway, and is an entirely separate VPC with respect to the rest of the AWS Secure Environment Accelerator Architecture.</p> <p>The sandbox VPC should be used exclusively for time-limited experimentation, particularly with out-of-region services, and never used for any line of business workload or data.</p>"},{"location":"developer/","title":"Accelerator Developer Guide","text":"<p>It is important to read the Operations Guide before reading this document.</p> <ul> <li>Development Guide</li> <li>Tech Stack</li> <li>Best Practices</li> <li>How to Contribute</li> <li>Release Process</li> </ul>"},{"location":"developer/best-practices/","title":"1. Best Practices","text":""},{"location":"developer/best-practices/#11-typescript-and-nodejs","title":"1.1. TypeScript and NodeJS","text":""},{"location":"developer/best-practices/#111-handle-unhandled-promises","title":"1.1.1. Handle Unhandled Promises","text":"<p>Entry point TypeScript files -- files that start execution instead of just defining methods and classes -- should have the following code snippet at the start of the file.</p> <pre><code>process.on('unhandledRejection', (reason, _) =&gt; {\nconsole.error(reason);\nprocess.exit(1);\n});\n</code></pre> <p>This prevents unhandled promise rejection errors by NodeJS. Please read https://medium.com/dailyjs/how-to-prevent-your-node-js-process-from-crashing-5d40247b8ab2 for more information.</p>"},{"location":"developer/best-practices/#12-cloudformation","title":"1.2. CloudFormation","text":""},{"location":"developer/best-practices/#121-cross-accountregion-references","title":"1.2.1. Cross-Account/Region References","text":"<p>When managing multiple AWS accounts, the Accelerator may need permissions to modify resources in the managed accounts. For example, a transit gateway could be created in a shared network account and it need to be shared to the perimeter account to create a VPN connection.</p> <p>In a single-account environment we would could just:</p> <ol> <li>create a single stack and use <code>!Ref</code> to refer to the transit gateway;</li> <li>or deploy two stacks<ul> <li>one stack that contains the transit gateway and creates a CloudFormation exported output that contains the transit gateway ID;</li> <li>another stack that imports the exported output value from the previous stack and uses it to create a VPN connection.</li> </ul> </li> </ol> <p>In a multi-account environment this is not possible and we had to find a way to share outputs across accounts and regions.</p> <p>See Passing Outputs Between Phases.</p>"},{"location":"developer/best-practices/#122-resource-names-and-logical-ids","title":"1.2.2. Resource Names and Logical IDs","text":"<p>Some resources, like <code>AWS::S3::Bucket</code>, can have an explicit name. Setting an explicit name can introduce some possible issues.</p> <p>The first issue that could occur goes as follows:</p> <ul> <li>the named resource has a retention policy to retain the resource after deleting;</li> <li>then the named resource is created through a CloudFormation stack;</li> <li>next, an error happens while creating or updating the stack and the stack rolls back;</li> <li>and finally the named resource is deleted from the stack but has a retention policy to retain, so the resource not be deleted;</li> </ul> <p>Suppose then that the stack creation issue is resolved and we retry to create the named resource through the CloudFormation stack:</p> <ul> <li>the named resource is created through a CloudFormation stack;</li> <li>the named resource will fail to create because a resource with the given name already exists.</li> </ul> <p>The best way to prevent this issue from happening is to not explicitly set a name for the resource and let CloudFormation generate the name.</p> <p>Another issue could occur when changing the logical ID of the named resource. This is documented in the following section.</p>"},{"location":"developer/best-practices/#123-changing-logical-ids","title":"1.2.3. Changing Logical IDs","text":"<p>When changing the logical ID of a resource CloudFormation assumes the resource is a new resource since it has a logical ID it does not know yet. When updating a stack, CloudFormation will always prioritize resource creation before deletion.</p> <p>The following issue could occur when the resource has an explicit name. CloudFormation will try to create the resource anew and will fail since a resource with the given name already exists. Example of resources where this could happen are <code>AWS::S3::Bucket</code>, <code>AWS::SecretManager::Secret</code>.</p>"},{"location":"developer/best-practices/#124-changing-immutable-properties","title":"1.2.4. Changing (Immutable) Properties","text":"<p>Not only changing logical IDs could cause CloudFormation to replace resources. Changing immutable properties also cause replacement of resources. See Update behaviors of stack resources.</p> <p>Be especially careful when:</p> <ul> <li>changing immutable properties for a named resource. Example of a resource is <code>AWS::Budgets::Budget</code>, <code>AWS::ElasticLoadBalancingV2::LoadBalancer</code>.</li> <li>updating network interfaces for an <code>AWS::EC2::Instance</code>. Not only will this cause the instance to re-create, it will also fail to attach the network interfaces to the new EC2 instance. CloudFormation creates the new EC2 instance first before deleting the old one. It will try to attach the network interfaces to the new instance, but the network interfaces are still attached to the old instance and CloudFormation will fail.</li> </ul> <p>For some named resources, like <code>AWS::AutoScaling::LaunchConfiguration</code> and <code>AWS::Budgets::Budget</code>, we append a hash to the name of the resource that is based on its properties. This way when an immutable property is changed, the name will also change, and the resource will be replaced successfully. See for example <code>src/lib/cdk-constructs/src/autoscaling/launch-configuration.ts</code> and <code>src/lib/cdk-constructs/src//billing/budget.ts</code>.</p> <pre><code>export type LaunchConfigurationProps = autoscaling.CfnLaunchConfigurationProps;\n\n/**\n * Wrapper around CfnLaunchConfiguration. The construct adds a hash to the launch configuration name that is based on\n * the launch configuration properties. The hash makes sure the launch configuration gets replaced correctly by\n * CloudFormation.\n */\nexport class LaunchConfiguration extends autoscaling.CfnLaunchConfiguration {\nconstructor(scope: cdk.Construct, id: string, props: LaunchConfigurationProps) {\nsuper(scope, id, props);\n\nif (props.launchConfigurationName) {\nconst hash = hashSum({ ...props, path: this.node.path });\nthis.launchConfigurationName = `${props.launchConfigurationName}-${hash}`;\n}\n}\n}\n</code></pre>"},{"location":"developer/best-practices/#13-cdk","title":"1.3. CDK","text":"<p>CDK makes heavy use of CloudFormation so all best practices that apply to CloudFormation also apply to CDK.</p>"},{"location":"developer/best-practices/#131-logical-ids","title":"1.3.1. Logical IDs","text":"<p>The logical ID of a CDK component is calculated based on its path in the construct tree. Be careful moving around constructs in the construct tree -- e.g. changing the parent of a construct or nesting a construct in another construct -- as this will change the logical ID of the construct. Then you could end up with the issues described in section Changing Logical IDs and section Changing (Immutable) Properties.</p> <p>See Logical ID Stability for more information.</p>"},{"location":"developer/best-practices/#132-moving-resources-between-nested-stacks","title":"1.3.2. Moving Resources between Nested Stacks","text":"<p>In some cases we use nested stacks to overcome the limit of 200 CloudFormation resources per stack.</p> <p>In the code snippet below you can see how we generate a dynamic amount of nested stack based on the amount of interface endpoints we construct. The <code>InterfaceEndpoint</code> construct contains CloudFormation resources so we have to be careful to not exceed the limit of 200 CloudFormation resources per nested stack. That is why we limit the amount of interface endpoints to 30 per nested stack.</p> <pre><code>let endpointCount = 0;\nlet endpointStackIndex = 0;\nlet endpointStack;\nfor (const endpoint of endpointConfig.endpoints) {\nif (!endpointStack || endpointCount &gt;= 30) {\nendpointStack = new NestedStack(accountStack, `Endpoint${endpointStackIndex++}`);\nendpointCount = 0;\n}\nnew InterfaceEndpoint(endpointStack, pascalCase(endpoint), {\nserviceName: endpoint,\n});\nendpointCount++;\n}\n</code></pre> <p>We have to be careful here though. Suppose the configuration file contains 40 interface endpoints. The first 30 interface endpoints will be created in the first nested stack; the next 10 interface endpoints will be created in the second nested stack. Suppose now that we remove the first nested endpoint from the configuration file. This will cause the 31st interface endpoint to become the 30th interface endpoint in the list and it will cause the interface endpoint to be moved from the second nested stack to the first nested stack. This will cause the stack updates to fail since CloudFormation will first try to create the interface endpoint in the first nested stack before removing it from the second nested stack. We do currently not support changes to the interface endpoint configuration because of this behavior.</p>"},{"location":"developer/best-practices/#133-l1-vs-l2-constructs","title":"1.3.3. L1 vs. L2 Constructs","text":"<p>See AWS Construct library for an explanation on L1 and L2 constructs.</p> <p>The L2 constructs for EC2 and VPC do not map well onto the Accelerator-managed resources. For this reason we mostly use L1 CDK constructs -- such as <code>ec2.CfnVPC</code>, <code>ec2.CfnSubnet</code> -- instead of using L2 CDK constructs -- such as <code>ec2.Vpc</code> and <code>ec2.Subnet</code>.</p>"},{"location":"developer/best-practices/#134-cdk-code-dependency-on-lambda-function-code","title":"1.3.4. CDK Code Dependency on Lambda Function Code","text":"<p>You can read about the distinction between CDK code and runtime code in the introduction of the Development section.</p> <p>CDK code can depend on runtime code. For example when we want to create a Lambda function using CDK, we need the runtime code to define the Lambda function. We use <code>npm scripts</code>, <code>npm</code> dependencies and the <code>NodeJS</code> <code>modules</code> API to define this dependency between CDK code and runtime code.</p> <p>First of all, we create a separate folder that contains the workspace and runtime code for our Lambda function. Throughout the project we've called these workspaces <code>...-lambda</code> but it could also be named <code>...-runtime</code>. See <code>src/lib/custom-resources/cdk-acm-import-certificate/runtime/package.json</code>.</p> <p>This workspace's <code>package.json</code> file needs a <code>prepare</code> script that compiles the runtime code. See <code>npm-scripts</code>.</p> <p>The <code>package.json</code> file also needs a <code>name</code> and a <code>main</code> entry that points to the compiled code.</p> <p><code>runtime/package.json</code></p> <pre><code>{\n\"name\": \"lambda-fn-runtime\",\n\"main\": \"dist/index.js\",\n\"scripts\": {\n\"prepare\": \"webpack-cli --config webpack.config.ts\"\n}\n}\n</code></pre> <p>Now when another workspace depends on our Lambda function runtime code workspace, the <code>prepare</code> script will run and it will compile the Lambda function runtime code.</p> <p>Next, we add the dependency to the new workspace to the workspace that contains the CDK code using <code>pnpm</code> or by adding it to <code>package.json</code>.</p> <p><code>cdk/package.json</code></p> <pre><code>{\n\"devDependencies\": {\n\"lambda-fn-runtime\": \"workspace:^0.0.1\"\n}\n}\n</code></pre> <p>In the CDK code we can now resolve the path to the compiled code using the <code>NodeJS</code> <code>modules</code> API. See NodeJS <code>modules</code> API.</p> <p><code>cdk/src/index.ts</code></p> <pre><code>class LambdaFun extends cdk.Construct {\nconstructor(scope: cdk.Construct, id: string) {\nsuper(scope, id);\n\n// Find the runtime package folder and resolves the `main` entry of `package.json`.\n// In our case this is `node_modules/lambda-fn-runtime/dist/index.js`.\nconst runtimeMain = resolve.require('lambda-fn-runtime');\n\n// Find the directory containing our `index.js` file.\n// In our case this is `node_modules/lambda-fn-runtime/dist`.\nconst runtimeDir = path.dirname(lambdaPath);\n\nnew lambda.Function(this, 'Resource', {\nruntime: lambda.Runtime.NODEJS_14_X,\ncode: lambda.Code.fromAsset(runtimeDir),\nhandler: 'index.handler', // The `handler` function in `index.js`\n});\n}\n}\n</code></pre> <p>You now have a CDK Lambda function that uses the compiled Lambda function runtime code.</p> <p>Note: The runtime code needs to recompile every time it changes since the <code>prepare</code> script only runs when the runtime workspace is installed.</p>"},{"location":"developer/best-practices/#135-custom-resource","title":"1.3.5. Custom Resource","text":"<p>We create custom resources for functionality that is not supported natively by CloudFormation. We have two types of custom resources in this project:</p> <ol> <li>Custom resource that calls an SDK method;</li> <li>Custom resource that needs additional functionality and is backed by a custom Lambda function.</li> </ol> <p>CDK has a helper construct for the first type of custom resources. See CDK <code>AwsCustomResource</code> documentation. This helper construct is for example used in the custom resource <code>ds-log-subscription</code>.</p> <p>The second type of custom resources requires a custom Lambda function runtime as described in the previous section. For example <code>acm-import-certificate</code> is backed by a custom Lambda function.</p> <p>Only a single Lambda function is created per custom resource, account and region. This is achieved by creating only a single Lambda function in the construct tree.</p> <p><code>src/lib/custom-resources/custom-resource/cdk/index.ts</code></p> <pre><code>class CustomResource extends cdk.Construct {\nconstructor(scope: cdk.Construct, id: string, props: CustomResourceProps) {\nsuper(scope, id);\n\nnew cdk.CustomResource(this, 'Resource', {\nresourceType: 'Custom::CustomResource',\nserviceToken: this.lambdaFunction.functionArn,\n});\n}\n\nprivate get lambdaFunction() {\nconst constructName = `CustomResourceLambda`;\n\nconst stack = cdk.Stack.of(this);\nconst existing = stack.node.tryFindChild(constructName);\nif (existing) {\nreturn existing as lambda.Function;\n}\n\n// The package '@aws-accelerator/custom-resources/cdk-custom-resource-runtime' contains the runtime code for the custom resource\nconst lambdaPath = require.resolve('@aws-accelerator/custom-resources/cdk-custom-resource-runtime');\nconst lambdaDir = path.dirname(lambdaPath);\n\nreturn new lambda.Function(stack, constructName, {\ncode: lambda.Code.fromAsset(lambdaDir),\n});\n}\n}\n</code></pre>"},{"location":"developer/best-practices/#136-escape-hatches","title":"1.3.6. Escape Hatches","text":"<p>Sometimes CDK does not support a property on a resource that CloudFormation does support. You can then override the property using the <code>addOverride</code> or <code>addPropertyOverride</code> methods on CDK CloudFormation resources. See CDK escape hatches.</p>"},{"location":"developer/best-practices/#1361-autoscaling-group-metadata","title":"1.3.6.1. AutoScaling Group Metadata","text":"<p>An example where we override metadata is when we create a launch configuration.S</p> <pre><code>const launchConfig = new autoscaling.CfnLaunchConfiguration(this, 'LaunchConfig', { ... });\n\nlaunchConfig.addOverride('Metadata.AWS::CloudFormation::Authentication', {\nS3AccessCreds: {\ntype: 'S3',\nroleName,\nbuckets: [bucketName],\n},\n});\n\nlaunchConfig.addOverride('Metadata.AWS::CloudFormation::Init', {\nconfigSets: {\nconfig: ['setup'],\n},\nsetup: {\nfiles: {\n// Add files here\n},\nservices: {\n// Add services here\n},\ncommands: {\n// Add commands here\n},\n},\n});\n</code></pre>"},{"location":"developer/best-practices/#1362-secret-secretvalue","title":"1.3.6.2. Secret <code>SecretValue</code>","text":"<p>Another example is when we want to use <code>secretsmanager.Secret</code> and set the secret value.</p> <pre><code>function setSecretValue(secret: secrets.Secret, value: string) {\nconst cfnSecret = secret.node.defaultChild as secrets.CfnSecret; // Get the L1 resource that backs this L2 resource\ncfnSecret.addPropertyOverride('SecretString', value); // Override the property `SecretString` on the L1 resource\ncfnSecret.addPropertyDeletionOverride('GenerateSecretString'); // Delete the property `GenerateSecretString` from the L1 resource\n}\n</code></pre>"},{"location":"developer/contributing-guidelines/","title":"1. How to Contribute","text":""},{"location":"developer/contributing-guidelines/#11-general","title":"1.1. General","text":"<p>Please first refer to and comply with the Contributing and Governance document found here</p>"},{"location":"developer/contributing-guidelines/#12-adding-new-functionality","title":"1.2. Adding New Functionality?","text":"<p>Before making a change or adding new functionality you have to verify what kind of functionality is being added.</p> <ul> <li>Is it an Accelerator-management change?<ul> <li>Is the change related to the <code>Installer</code> stack?<ul> <li>Is the change CDK related?<ul> <li>Make the change in <code>src/installer/cdk</code>.</li> </ul> </li> <li>Is the change runtime related?<ul> <li>Make the change in <code>src/installer/cdk/assets</code>.</li> </ul> </li> </ul> </li> <li>Is the change related to the <code>Initial Setup</code> stack?<ul> <li>Is the change CDK related?<ul> <li>Make the change in <code>src/core/cdk</code></li> </ul> </li> <li>Is the change runtime related?<ul> <li>Make the change in <code>src/core/runtime</code></li> </ul> </li> </ul> </li> </ul> </li> <li>Is it an Accelerator-managed change?<ul> <li>Is the change related to the <code>Phase</code> stacks?<ul> <li>Is the change CDK related?<ul> <li>Make the change in <code>src/deployments/cdk</code></li> </ul> </li> <li>Is the change runtime related?<ul> <li>Make the change in <code>src/deployments/runtime</code></li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"developer/contributing-guidelines/#13-create-a-cdk-lambda-function-with-lambda-runtime-code","title":"1.3. Create a CDK Lambda Function with Lambda Runtime Code","text":"<p>See CDK Code Dependency on Lambda Function Code for a short introduction.</p>"},{"location":"developer/contributing-guidelines/#14-create-a-custom-resource","title":"1.4. Create a Custom Resource","text":"<p>See Custom Resource and Custom Resources for a short introduction.</p> <ol> <li>Create a separate folder that contains the CDK and Lambda function runtime code, e.g. <code>src/lib/custom-resources/my-custom-resource</code>;</li> <li>Create a folder <code>my-custom-resource</code> that contains the CDK code;<ol> <li>Create a <code>package.json</code> file with a dependency to the <code>my-custom-resource/runtime</code> package;</li> <li>Create a <code>cdk</code> folder that contains the source of the CDK code;</li> </ol> </li> <li>Create a folder <code>my-custom-resource/runtime</code> that contains the runtime code;<ol> <li>Create a <code>runtime/package.json</code> file with a <code>\"name\"</code>, <code>\"prepare\"</code> script and a <code>\"main\"</code>;</li> <li>Create a <code>runtime/webpack.config.ts</code> file that compiles TypeScript code to a single JavaScript file;</li> <li>Create a <code>runtime/src</code> folder that contains the source of the Lambda function runtime code;</li> </ol> </li> </ol> <p>You can look at the <code>src/lib/custom-resources/cdk-acm-import-certificate</code> custom resource as an example.</p> <p>It is best practice to add tags to any resources that the custom resource creates using the <code>cfn-tags</code> library.</p>"},{"location":"developer/contributing-guidelines/#15-run-all-unit-tests","title":"1.5. Run All Unit Tests","text":"<p>Run in the root of the project.</p> <pre><code>pnpm recursive run test --no-bail --stream -- --silent\n</code></pre>"},{"location":"developer/contributing-guidelines/#16-accept-unit-test-snapshot-changes","title":"1.6. Accept Unit Test Snapshot Changes","text":"<p>Run in <code>src/deployments/cdk</code>.</p> <pre><code>pnpm run test -- -u\n</code></pre>"},{"location":"developer/contributing-guidelines/#17-validate-code-with-prettier","title":"1.7. Validate Code with Prettier","text":"<p>Run in the root of the project.</p> <pre><code>pnpx prettier --check **/*.ts\n</code></pre>"},{"location":"developer/contributing-guidelines/#18-format-code-with-prettier","title":"1.8. Format Code with Prettier","text":"<p>Run in the root of the project.</p> <pre><code>pnpx prettier --write **/*.ts\n</code></pre>"},{"location":"developer/contributing-guidelines/#19-validate-code-with-tslint","title":"1.9. Validate Code with <code>tslint</code>","text":"<p>Run in the root of the project.</p> <pre><code>pnpm recursive run lint --stream --no-bail\n</code></pre>"},{"location":"developer/development/","title":"1. Development Guide","text":"<p>This document is a reference document. Instead of reading through it in linear order, you can use it to look up specific issues as needed.</p> <p>It is important to read the Operations Guide before reading this document. If you're interested in actively contributing to the project, you should also review the Governance and Contributing Guide.</p>"},{"location":"developer/development/#11-overview","title":"1.1. Overview","text":"<p>There are different types of projects in this monorepo.</p> <ol> <li>Projects containing CDK code that compiles to CloudFormation templates and deploy to AWS using the CDK toolkit;</li> <li>Projects containing runtime code that is used by the CDK code to deploy Lambda functions;</li> <li>Projects containing reusable code; both for use by the CDK code and/or runtime code.</li> </ol> <p>The CDK code either deploys Accelerator-management resources or Accelerator-managed resources. See the Operations Guide for the distinction between Accelerator-management and Accelerator-managed resources.</p> <p>The only language used in the project is TypeScript and exceptionally JavaScript. We do not write CloudFormation templates, only CDK code.</p> <p>When we want to enable functionality in a managed account we try to</p> <ol> <li>use native CloudFormation/CDK resource to enable the functionality;</li> <li>create a custom resource to enable the functionality; or</li> <li>lastly create a new step in the <code>Initial Setup</code> state machine to enable the functionality.</li> </ol>"},{"location":"developer/development/#12-project-structure","title":"1.2. Project Structure","text":"<p>The folder structure of the project is as follows:</p> <ul> <li><code>src/installer/cdk</code>: See Installer Stack;</li> <li><code>src/core/cdk</code>: See Initial Setup Stack;</li> <li><code>src/core/runtime</code> See Initial Setup Stack and Phase Steps and Phase Stacks;</li> <li><code>src/deployments/runtime</code> See Phase Steps and Phase Stacks;</li> <li><code>src/deployments/cdk</code>: See Phase Steps and Phase Stacks;</li> <li><code>src/lib/accelerator-cdk</code>: See Libraries &amp; Tools;</li> <li><code>src/lib/cdk-constructs</code>: See Libraries &amp; Tools;</li> <li><code>src/lib/cdk-plugin-assume-role</code>: See CDK Assume Role Plugin.</li> <li><code>src/lib/common-config</code>: See Libraries &amp; Tools;</li> <li><code>src/lib/common-outputs</code>: See Libraries &amp; Tools;</li> <li><code>src/lib/common-types</code>: See Libraries &amp; Tools;</li> <li><code>src/lib/common</code>: See Libraries &amp; Tools;</li> <li><code>src/lib/custom-resources/**/cdk</code>: See Custom Resources;</li> <li><code>src/lib/custom-resources/**/runtime</code>: See Custom Resources;</li> </ul>"},{"location":"developer/development/#13-installer-stack","title":"1.3. Installer Stack","text":"<p>Read the Operations Guide first before reading this section. This section is a technical addition to the section in the Operations Guide.</p> <p>As stated in the Operations Guide, the <code>Installer</code> stack is responsible for installing the <code>Initial Setup</code> stack. It is an Accelerator-management resource. The main resource in the <code>Installer</code> stack is the <code>ASEA-Installer</code> CodePipeline. The CodePipeline uses this GitHub repository as source action and runs CDK in a CodeBuild step to deploy the <code>Initial Setup</code> stack.</p> <pre><code>new codebuild.PipelineProject(stack, 'InstallerProject', {\nbuildSpec: codebuild.BuildSpec.fromObject({\nversion: '0.2',\nphases: {\ninstall: {\n'runtime-versions': {\nnodejs: 14,\n},\n// The flag '--unsafe-perm' is necessary to run pnpm scripts in Docker\ncommands: ['npm install --global pnpm@6.2.3', 'pnpm install --unsafe-perm --frozen-lockfile'],\n},\npre_build: {\n// The flag '--unsafe-perm' is necessary to run pnpm scripts in Docker\ncommands: ['pnpm recursive run build --unsafe-perm'],\n},\nbuild: {\ncommands: [\n'cd src/core/cdk',\n// Bootstrap the environment for use by CDK\n'pnpx cdk bootstrap --require-approval never',\n// Deploy the Initial Setup stack\n'pnpx cdk deploy --require-approval never',\n],\n},\n},\n}),\n});\n</code></pre> <p>When the CodePipeline finishes deploying the <code>Initial Setup</code> stack, it starts a Lambda function that starts the execution of the <code>Initial Setup</code> stack's main state machine.</p> <p>The <code>Initial Setup</code> stack deployment receives environment variables from the CodePipeline's CodeBuild step. The most notable environment variables are:</p> <ul> <li><code>ACCELERATOR_STATE_MACHINE_NAME</code>: The <code>Initial Setup</code> will use this name for the main state machine. So it is the <code>Installer</code> stack that decides the name of the main state machine. This way we can confidently start the main state machine of the <code>Initial Setup</code> stack from the CodePipeline;</li> <li><code>ENABLE_PREBUILT_PROJECT</code>: See Prebuilt Docker Image.</li> </ul>"},{"location":"developer/development/#14-initial-setup-stack","title":"1.4. Initial Setup Stack","text":"<p>Read Operations Guide first before reading this section. This section is a technical addition to the section in the Operations Guide.</p> <p>As stated in the Operations Guide, the <code>Initial Setup</code> stack consists of a state machine, named <code>ASEA-MainStateMachine_sm</code>, which executes steps to create the Accelerator-managed stacks and resources in the managed accounts. It is an Accelerator-management resource.</p> <p>The <code>Initial Setup</code> stack is defined in the <code>src/core/cdk</code> folder.</p> <p>The <code>Initial Setup</code> stack is similar to the <code>Installer</code> stack, as in that it runs a CodeBuild project to deploy others stacks using CDK. In case of the <code>Initial Setup</code> stack</p> <ul> <li>we use a AWS Step Functions State Machine to run steps instead of using a CodePipeline;</li> <li>we deploy multiple stacks, called <code>Phase</code> stacks, in Accelerator-managed accounts. These <code>Phase</code> stacks contain Accelerator-managed resources.</li> </ul> <p>In order to install these <code>Phase</code> stacks in Accelerator-managed accounts, we need access to those accounts. We create a stack set in the Organization Management (root) account that has instances in all Accelerator-managed accounts. This stack set contains what we call the <code>PipelineRole</code>.</p> <p>The code for the steps in the state machine is in <code>src/core/runtime</code>. All the steps are in different files but are compiled into a single file. We used to compile all the steps separately but we would hit a limit in the amount of parameters in the generated CloudFormation template. Each step would have its own CDK asset that would introduce three new parameters. We quickly reached the limit of 60 parameters in a CloudFormation template and decided to compile the steps into a single file and use it across all different Lambda functions.</p>"},{"location":"developer/development/#141-codebuild-and-prebuilt-docker-image","title":"1.4.1. CodeBuild and Prebuilt Docker Image","text":"<p>The CodeBuild project that deploys the different <code>Phase</code> stacks is constructed using the <code>CdkDeployProject</code> or <code>PrebuiltCdkDeployProject</code> based on the value of the environment variable <code>ENABLE_PREBUILT_PROJECT</code>.</p> <p>The first, <code>CdkDeployProject</code> constructs a CodeBuild project that copies this whole Github repository as a ZIP file to S3 using CDK S3 assets. This ZIP file is then used as source for the CodeBuild project. When the CodeBuild project executes, it runs <code>pnpm recursive install</code> which in turn will run all <code>prepare</code> scripts in all <code>package.json</code> files in the project -- as described in section CDK Code Dependency on Lambda Function Code.</p> <p>After installing the dependencies, the CodeBuild project deploys the <code>Phase</code> stacks.</p> <pre><code>cd src/deployments/cdk\nsh codebuild-deploy.sh\n</code></pre> <p>We have more than 50 workspace projects in the monorepo with a <code>prepare</code> script, so the <code>pnpm recursive install</code> step can take some time. Also, the CodeBuild project will run for each deployed <code>Phase</code> stack in each Accelerator-managed account.</p> <p>This is where the <code>PrebuiltCdkDeployProject</code> CodeBuild project comes in. The <code>PrebuiltCdkDeployProject</code> contains a Docker image that contains the whole project in the <code>/app</code> directory and has all the dependencies already installed.</p> <pre><code>FROM node:12-alpine3.11\n# Install the package manager\nRUN npm install --global pnpm\nRUN mkdir /app\nWORKDIR /app\n# Copy over the project root to the /app directory\nADD . /app/\n# Install the dependencies\nRUN pnpm install --unsafe-perm --frozen-lockfile\n# Build all Lambda function runtime code\nRUN pnpm recursive run build --unsafe-perm\n</code></pre> <p>When this CodeBuild project executes, it uses the Docker image as base -- the dependencies are already installed -- and runs the same commands as the <code>CdkDeployProject</code> to deploy the <code>Phase</code> stacks.</p>"},{"location":"developer/development/#142-passing-data-to-phase-steps-and-phase-stacks","title":"1.4.2. Passing Data to Phase Steps and Phase Stacks","text":"<p>Some steps in the state machine write data to Amazon DynamoDB. This data is necessary to deploy the <code>Phase</code> stacks later on. At one time this data was written to Secrets Manager and/or S3, these mechanisms were deemed ineffective due to object size limitations or consistency challenges and were all eventually migrated to DynamoDB.</p> <ul> <li><code>Load Accounts</code> step: This step finds the Accelerator-managed accounts in AWS Organizations and stores the account key -- the key of the account in <code>mandatory-account-configs</code> or <code>workload-account-configs</code> object in the Accelerator config -- and account ID and other useful information in the <code>ASEA-Parameters</code> table, <code>accounts/#</code> key and <code>accounts-items-count</code> key;</li> <li><code>Load Organizations</code> step: More or less the same as the <code>Load Accounts</code> step but for organizational units in AWS Organizations and stores the values in the <code>ASEA-Parameters</code> table, <code>organizations</code> key;</li> <li><code>Load Limits</code> step: This step requests limit increases for Accelerator-managed accounts and stores the current limits in the the <code>ASEA-Parameters</code> table, <code>limits</code> key.</li> <li><code>Store Phase X Output</code>: This step loads stack outputs from all existing <code>Phase</code> stacks and stores the outputs in the DynamoDB table <code>ASEA-Outputs</code>.</li> </ul> <p>Other data is passed through environment variables:</p> <ul> <li><code>ACCELERATOR_NAME</code>: The name of the Accelerator;</li> <li><code>ACCELERATOR_PREFIX</code>: The prefix for all named Accelerator-managed resources;</li> <li><code>ACCELERATOR_EXECUTION_ROLE_NAME</code>: The name of the execution role in the Accelerator-managed accounts. This is the <code>PipelineRole</code> we created using stack sets.</li> </ul>"},{"location":"developer/development/#15-phase-steps-and-phase-stacks","title":"1.5. Phase Steps and Phase Stacks","text":"<p>Read Operations Guide first before reading this section. This section is a technical addition to the Deploy Phase X sections in the Operations Guide.</p> <p>The <code>Phase</code> stacks contain the Accelerator-managed resources. The reason the deployment of Accelerator-managed resources is split into different phases is because there cannot be cross account/region references between CloudFormation stacks. See Cross-Account/Region References.</p> <p>The <code>Phase</code> stacks are deployed by a CodeBuild project in the <code>Initial Setup</code> stack as stated in the previous paragraphs. The CodeBuild project executes the <code>codebuild-deploy.sh</code> script. See <code>initial-setup.ts</code>.</p> <p>The <code>codebuild-deploy.sh</code> script executes the <code>cdk.ts</code> file.</p> <p>The <code>cdk.ts</code> file is meant as a replacement for the <code>cdk</code> CLI command. To deploy a phase stack you would not run <code>pnpx cdk deploy</code> but <code>cdk.sh --phase 1</code>. See CDK API for more information why we use the CDK API instead of using the CDK CLI.</p> <p>The <code>cdk.ts</code> command parses command line arguments and creates all the <code>cdk.App</code> for all accounts and regions for the given <code>--phase</code>. When you pass the <code>--region</code> or <code>--account-key</code> command, all the <code>cdk.App</code> for all accounts and regions will still be created, except that only the <code>cdk.App</code>s matching the parameters will be deployed. This behavior could be optimized in the future. See Stacks with Same Name in Different Regions for more information why we're creating multiple <code>cdk.App</code>s.</p>"},{"location":"developer/development/#16-store-outputs-to-ssm-parameter-store","title":"1.6. Store outputs to SSM Parameter Store","text":"<p>Customers need the CloudFormation outputs of resources that are created by the accelerator in order to deploy their own resources in AWS. eg. vpcId in shared-network account to create an ec2 instance, etc.</p> <p>This step loads the stack outputs from our DynamoDB Table <code>ASEA-Outputs</code> and stores as key value pairs in SSM Parameter Store in each account.</p> <p>Example values are</p> <ul> <li>/ASEA/network/vpc/1/name =&gt; Endpoint</li> <li>/ASEA/network/vpc/1/id =&gt; vpc-XXXXXXXXXX</li> </ul> <p><code>ASEA-Outputs-Utils</code> DynamoDB Table is used extensively to maintain same index irrespective of configuration changes.</p> <p>This allows customers to reliably build Infrastructure as Code (IaC) which depends on accelerator deployed objects like VPC's, security groups, subnets, ELB's, KMS keys, IAM users and policies. Rather than making the parameters dependent on object names, we used an indexing scheme, which we maintain and don't update as a customers configuration changes. We have attempted to keep the index values consistent across accounts (based on the config file), such that when code is propoted through the SDLC cycle from Dev to Test to Prod, the input parameters to the IaC scripts do not need to be updated, the App subnet, for example, will have the same index value in all accounts.</p>"},{"location":"developer/development/#161-phases-and-deployments","title":"1.6.1. Phases and Deployments","text":"<p>The <code>cdk.ts</code> file calls the <code>deploy</code> method in the <code>apps/app.ts</code>. This <code>deploy</code> method loads the Accelerator configuration, accounts, organizations from DynamoDB; loads the stack outputs from Amazon DynamoDB; and loads required environment variables.</p> <pre><code>/**\n * Input to the `deploy` method of a phase.\n */\nexport interface PhaseInput {\n// The config.json file\nacceleratorConfig: AcceleratorConfig;\n// Auxiliary class to construct stacks\naccountStacks: AccountStacks;\n// The list of accounts, their key in the configuration file and their ID\naccounts: Account[];\n// The parsed environment variables\ncontext: Context;\n// The list of stack outputs from previous phases\noutputs: StackOutput[];\n// Auxiliary class to manage limits\nlimiter: Limiter;\n}\n</code></pre> <p>It is important to note that no configuration is hard-coded. The CloudFormation templates are generated by CDK and the CDK constructs are created according to the configuration file. Changes to the configuration will change the CDK construct tree and that will result in a different CloudFormation template that is deployed.</p> <p>The different phases are defined in <code>apps/phase-x.ts</code>. Historically we created all CDK constructs in the <code>phase-x.ts</code> files. After a while the <code>phase-x.ts</code> files started to get too big and we moved to separating the logic into separate deployments. Every logical component has a separate folder in the <code>deployments</code> folder. Every <code>deployment</code> consists of so-called steps. Separate steps are put in loaded in phases.</p> <p>For example, take the <code>deployments/defaults</code> deployment. The deployment consists of two steps, i.e. <code>step-1.ts</code> and <code>step-2.ts</code>. <code>deployments/defaults/step-1.ts</code> is created in <code>apps/phase-0.ts</code> and <code>deployments/defaults/step-2.ts</code> is created in <code>apps/phase-1.ts</code>. You can find more details about what happens in each phase in the Operations Guide.</p> <p><code>apps/phase-0.ts</code></p> <pre><code>export async function deploy({ acceleratorConfig, accountStacks, accounts, context }: PhaseInput) {\n// Create defaults, e.g. S3 buckets, EBS encryption keys\nconst defaultsResult = await defaults.step1({\nacceleratorPrefix: context.acceleratorPrefix,\naccountStacks,\naccounts,\nconfig: acceleratorConfig,\n});\n</code></pre> <p><code>apps/phase-1.ts</code></p> <pre><code>export async function deploy({ acceleratorConfig, accountStacks, accounts, outputs }: PhaseInput) {\n// Find the central bucket in the outputs\nconst centralBucket = CentralBucketOutput.getBucket({\naccountStacks,\nconfig: acceleratorConfig,\noutputs,\n});\n\n// Find the log bucket in the outputs\nconst logBucket = LogBucketOutput.getBucket({\naccountStacks,\nconfig: acceleratorConfig,\noutputs,\n});\n\n// Find the account buckets in the outputs\nconst accountBuckets = await defaults.step2({\naccounts,\naccountStacks,\ncentralLogBucket: logBucket,\nconfig: acceleratorConfig,\n});\n}\n</code></pre>"},{"location":"developer/development/#162-passing-outputs-between-phases","title":"1.6.2. Passing Outputs between Phases","text":"<p>The CodeBuild step that is responsible for deploying a <code>Phase</code> stack runs in the Organization Management (root) account. We wrote a CDK plugin that allows the CDK deploy step to assume a role in the Accelerator-managed account and create the CloudFormation <code>Phase</code> stack in the managed account. See CDK Assume Role Plugin.</p> <p>After a <code>Phase-X</code> is deployed in all Accelerator-managed accounts, a step in the <code>Initial Setup</code> state machine collects all the <code>Phase-X</code> stack outputs in all Accelerator-managed accounts and regions and stores theses outputs in DynamoDB.</p> <p>Then the next <code>Phase-X+1</code> deploys using the outputs from the previous <code>Phase-X</code> stacks.</p> <p>See Creating Stack Outputs for helper constructs to create outputs.</p>"},{"location":"developer/development/#163-decoupling-configuration-from-constructs","title":"1.6.3. Decoupling Configuration from Constructs","text":"<p>At the start of the project we created constructs that had tight coupling to the Accelerator config structure. The properties to instantiate a construct would sometimes have a reference to an Accelerator-specific interface. An example of this is the <code>Vpc</code> construct in <code>src/deployments/cdk/common/vpc.ts</code>.</p> <p>Later on in the project we started decoupling the Accelerator config from the construct properties. Good examples are in <code>src/lib/cdk-constructs/</code>.</p> <p>Decoupling the configuration from the constructs improves reusability and robustness of the codebase.</p>"},{"location":"developer/development/#17-libraries-tools","title":"1.7. Libraries &amp; Tools","text":""},{"location":"developer/development/#171-cdk-assume-role-plugin","title":"1.7.1. CDK Assume Role Plugin","text":"<p>At the time of writing, CDK does not support cross-account deployments of stacks. It is possible however to write a CDK plugin and implement your own credential loader for cross-account deployment.</p> <p>We wrote a CDK plugin that can assume a role into another account. In our case, the Organization Management (root) account will assume the <code>PipelineRole</code> in an Accelerator-managed account to deploy stacks.</p>"},{"location":"developer/development/#172-cdk-api","title":"1.7.2. CDK API","text":"<p>We use the internal CDK API to deploy the <code>Phase</code> stacks instead of the CDK CLI for the following reasons:</p> <ul> <li>It allows us to deploy multiple stacks in parallel;</li> <li>Disable stack termination before destroying a stack;</li> <li>Delete a stack after it initially failed to create;</li> <li>Deploy multiple apps at the same time -- see Stacks with Same Name in Different Regions.</li> </ul> <p>The helper class <code>CdkToolkit</code> in <code>toolkit.ts</code> wraps around the CDK API.</p> <p>The risk of using the CDK API directly is that the CDK API can change at any time. There is no stable API yet. When upgrading the CDK version, the <code>CdkToolkit</code> wrapper might need to be adapted.</p>"},{"location":"developer/development/#173-aws-sdk-wrappers","title":"1.7.3. AWS SDK Wrappers","text":"<p>You can find <code>aws-sdk</code> wrappers in the <code>src/lib/common/src/aws</code> folder. Most of the classes and functions just wrap around <code>aws-sdk</code> classes and implement promises and exponential backoff to retryable errors. Other classes, like <code>Organizations</code> have additional functionality such as listing all the organizational units in an organization in the function <code>listOrganizationalUnits</code>.</p> <p>Please use the <code>aws-sdk</code> wrappers throughout the project or write an additional wrapper when necessary.</p>"},{"location":"developer/development/#174-configuration-file-parsing","title":"1.7.4. Configuration File Parsing","text":"<p>The configuration file is defined and validated using the <code>io-ts</code> library. See <code>src/lib/common-config/src/index.ts</code>. In case any changes need to be made to the configuration file parsing, this is the place to be.</p> <p>We wrap a class around the <code>AcceleratorConfig</code> type that contains additional helper functions. You can add your own additional helper functions.</p>"},{"location":"developer/development/#1741-acceleratornametagger","title":"1.7.4.1. <code>AcceleratorNameTagger</code>","text":"<p><code>AcceleratorNameTagger</code> is a CDK aspect that sets the name tag on specific resources based on the construct ID of the resource.</p> <p>The following example illustrates its purpose.</p> <pre><code>const stack = new cdk.Stack();\nnew ec2.CfnVpc(stack, 'SharedNetwork', {});\nAspects.of(stack).add(new AcceleratorNameTagger());\n</code></pre> <p>The example above synthesizes to the following CloudFormation template.</p> <pre><code>Resources:\nSharedNetworkAB7JKF7:\nProperties:\nTags:\n- Key: Name\nValue: SharedNetwork_vpc\n</code></pre>"},{"location":"developer/development/#1742-acceleratorstack","title":"1.7.4.2. <code>AcceleratorStack</code>","text":"<p><code>AcceleratorStack</code> is a class that extends <code>cdk.Stack</code> and adds the <code>Accelerator</code> tag to all resources in the stack. It also applies the aspect <code>AcceleratorNameTagger</code>.</p> <p>It is also used by the <code>accelerator-name-generator.ts</code> functions to find the name of the <code>Accelerator</code>.</p>"},{"location":"developer/development/#1743-name-generator","title":"1.7.4.3. Name Generator","text":"<p>The <code>accelerator-name-generator.ts</code> file contains methods that create names for resources that are optionally prefixed with the Accelerator name, and optionally suffixed with a hash based on the path of the resource, the account ID and region of the stack.</p> <p>The functions should be used to create pseudo-random names for IAM roles, KMS keys, key pairs and log groups.</p>"},{"location":"developer/development/#1744-accountstacks","title":"1.7.4.4. <code>AccountStacks</code>","text":"<p><code>AccountStacks</code> is a class that manages the creation of an <code>AcceleratorStack</code> based on a given account key and region. If an account with the given account key cannot be found in the accounts object -- which is loaded by <code>apps/app.ts</code> then no stack will be created. This class is used extensively throughout the phases and deployment steps.</p> <pre><code>export async function step1(props: CertificatesStep1Props) {\nconst { accountStacks, centralBucket: centralBucket, config } = props;\n\nfor (const { accountKey, certificates } of config.getCertificateConfigs()) {\nif (certificates.length === 0) {\ncontinue;\n}\n\nconst accountStack = accountStacks.tryGetOrCreateAccountStack(accountKey);\nif (!accountStack) {\nconsole.warn(`Cannot find account stack ${accountKey}`);\ncontinue;\n}\n\nfor (const certificate of certificates) {\ncreateCertificate({\ncentralBucket,\ncertificate,\nscope: accountStack,\n});\n}\n}\n}\n</code></pre>"},{"location":"developer/development/#1745-vpc-and-importedvpc","title":"1.7.4.5. <code>Vpc</code> and <code>ImportedVpc</code>","text":"<p><code>Vpc</code> is an interface in the <code>src/lib/cdk-constructs/src/vpc/vpc.ts</code> file that attempts to define an interface for a VPC. The goal of the interface is to be implemented by a <code>cdk.Construct</code> that implements the interface. This CDK issue provides more background [https://github.com/aws/aws-cdk/issues/5927].</p> <p>Another goal of the interface is to provide an interface on top of imported VPC outputs. This is what the <code>ImportedVpc</code> class implements. The class loads outputs from VPC in a previous phase and implements the <code>Vpc</code> interface on top of those outputs.</p>"},{"location":"developer/development/#1746-limiter","title":"1.7.4.6. <code>Limiter</code>","text":"<p>So far we haven't talked about limits yet. There is a step in the <code>Initial Setup</code> state machine that requests limit increases according to the desired limits in the configuration file. The step saves the current limits to the <code>limits</code> key in the DynamoDB table <code>ASEA-Parameters</code>. The <code>apps/app.ts</code> file loads the limits and passes them as an input to the phase deployment.</p> <p>The <code>Limiter</code> class helps keeps track of resource we create and prevents exceeding these limits.</p> <pre><code>for (const { ouKey, accountKey, vpcConfig, deployments } of acceleratorConfig.getVpcConfigs()) {\nif (!limiter.create(accountKey, Limit.VpcPerRegion, region)) {\nconsole.log(`Skipping VPC \"${vpcConfig.name}\" deployment.`);\nconsole.log(`Reached maximum VPCs per region for account \"${accountKey}\" and region \"${region}\"`);\ncontinue;\n}\n\ncreateVpc({ ouKey, accountKey, vpcConfig });\n}\n</code></pre> <p>Action Item: This functionality could be redesigned to scan all the constructs in a <code>cdk.App</code> and remove resource that are exceeding any limits.</p>"},{"location":"developer/development/#175-creating-stack-outputs","title":"1.7.5. Creating Stack Outputs","text":"<p>Initially we would create stack outputs like this:</p> <pre><code>new cdk.CfnOutput(stack, 'BucketOutput', {\nvalue: bucket.bucketArn,\n});\n</code></pre> <p>But then we'd get a lot of outputs in a stack. We started some outputs together using JSON. This allowed us to store structured data inside the stack outputs.</p> <pre><code>new JsonOutputValue(stack, 'Output', {\ntype: 'FirewallInstanceOutput',\nvalue: {\ninstanceId: instance.instanceId,\nname: firewallConfig.name,\naz,\n},\n});\n</code></pre> <p>Using the solution above, we'd not have type checking when reading or writing outputs. That's what the class <code>StructuredOutputValue</code> has a solution for. It uses the <code>io-ts</code> library to serialize and deserialize structured types.</p> <pre><code>export const FirewallInstanceOutput = t.interface(\n{\nid: t.string,\nname: t.string,\naz: t.string,\n},\n'FirewallInstanceOutput',\n);\n\nexport type FirewallInstanceOutput = t.TypeOf&lt;typeof FirewallInstanceOutput&gt;;\n\nnew StructuredOutputValue&lt;FirewallInstanceOutput&gt;(stack, 'Output', {\ntype: FirewallInstanceOutput,\nvalue: {\ninstanceId: instance.instanceId,\nname: firewallConfig.name,\naz,\n},\n});\n</code></pre> <p>And we can even improve on this a bit more.</p> <pre><code>export const CfnFirewallInstanceOutput = createCfnStructuredOutput(FirewallInstanceOutput);\n\nnew CfnFirewallInstanceOutput(stack, 'Output', {\nvpcId: vpc.ref,\nvpcName: vpcConfig.name,\n});\n</code></pre> <pre><code>export const FirewallInstanceOutputFinder = createStructuredOutputFinder(FirewallInstanceOutput, () =&gt; ({}));\n\n// Create an OutputFinder\nconst firewallInstances = FirewallInstanceOutputFinder.findAll({\noutputs,\naccountKey,\n});\n\n// Example usage of the OutputFinder\nconst firewallInstance = firewallInstances.find(i =&gt; i.name === target.name &amp;&amp; i.az === target.az);\n</code></pre> <p>Generally you would place the output type definition inside <code>src/lib/common-outputs</code> along with the output finder. Then in the deployment folder in <code>src/deployments/cdk/deployments</code> you would create an <code>output.ts</code> file where you would define the CDK output type with <code>createCfnStructuredOutput</code>. You would not define the CDK output type in <code>src/lib/common-outputs</code> since that project is also used by runtime code that does not need to know about CDK and CloudFormation.</p>"},{"location":"developer/development/#1751-adding-tags-to-shared-resources-in-destination-account","title":"1.7.5.1. Adding Tags to Shared Resources in Destination Account","text":"<p>There is another special type of output, <code>AddTagsToResourcesOutput</code>. It can be used to attach tags to resources that are shared into another account.</p> <pre><code>new AddTagsToResourcesOutput(this, 'OutputSharedResourcesSubnets', {\ndependencies: sharedSubnets.map(o =&gt; o.subnet),\nproduceResources: () =&gt;\nsharedSubnets.map(o =&gt; ({\nresourceId: o.subnet.ref,\nresourceType: 'subnet',\nsourceAccountId: o.sourceAccountId,\ntargetAccountIds: o.targetAccountIds,\ntags: o.subnet.tags.renderTags(),\n})),\n});\n</code></pre> <p>This will add the outputs to the stack in the account that is initiating the resource share.</p> <p>Next, the state machine step <code>Add Tags to Shared Resources</code> looks for all those outputs. The step will assume the <code>PipelineRole</code> in the <code>targetAccountIds</code> and attach the given tags to the shared resource.</p>"},{"location":"developer/development/#176-custom-resources","title":"1.7.6. Custom Resources","text":"<p>There are different ways to create a custom resource using CDK. See the Custom Resource section for more information.</p> <p>All custom resources have a <code>README.md</code> that demonstrates their usage.</p>"},{"location":"developer/development/#1761-externalizing-aws-sdk","title":"1.7.6.1. Externalizing <code>aws-sdk</code>","text":"<p>Some custom resources set the <code>aws-sdk</code> as external dependency and some do not.</p> <p>Example of setting <code>aws-sdk</code> as external dependency.</p> <p><code>src/lib/custom-resources/cdk-kms-grant/runtime/package.json</code></p> <pre><code>{\n\"externals\": [\"aws-lambda\", \"aws-sdk\"],\n\"dependencies\": {\n\"aws-lambda\": \"1.0.6\",\n\"aws-sdk\": \"2.631.0\"\n}\n}\n</code></pre> <p>Example of setting <code>aws-sdk</code> as embedded dependency.</p> <p><code>src/lib/custom-resources/cdk-guardduty-enable-admin/runtime/package.json</code></p> <pre><code>{\n\"externals\": [\"aws-lambda\"],\n\"dependencies\": {\n\"aws-lambda\": \"1.0.6\",\n\"aws-sdk\": \"2.711.0\"\n}\n}\n</code></pre> <p>Setting the <code>aws-sdk</code> library as external is sometimes necessary when a newer <code>aws-sdk</code> version is necessary for the Lambda runtime code. At the time of writing the NodeJS 12 runtime uses <code>aws-sdk</code> version <code>2.631.0</code></p> <p>For example the method <code>AWS.GuardDuty.enableOrganizationAdminAccount</code> was only introduced in <code>aws-sdk</code> version <code>2.660</code>. That means that Webpack has to embed the <code>aws-sdk</code> version specified in <code>package.json</code> into the compiled JavaScript file. This can be achieved by removing <code>aws-sdk</code> from the <code>external</code> array.</p> <p><code>src/lib/custom-resources/cdk-kms-grant/runtime/package.json</code></p>"},{"location":"developer/development/#1762-cfn-response","title":"1.7.6.2. cfn-response","text":"<p>This library helps you send a custom resource response to CloudFormation.</p> <p><code>src/lib/custom-resources/cdk-kms-grant/runtime/src/index.ts</code></p> <pre><code>export const handler = errorHandler(onEvent);\n\nasync function onEvent(event: CloudFormationCustomResourceEvent) {\nconsole.log(`Creating KMS grant...`);\nconsole.log(JSON.stringify(event, null, 2));\n\n// eslint-disable-next-line default-case\nswitch (event.RequestType) {\ncase 'Create':\nreturn onCreate(event);\ncase 'Update':\nreturn onUpdate(event);\ncase 'Delete':\nreturn onDelete(event);\n}\n}\n</code></pre>"},{"location":"developer/development/#1763-cfn-tags","title":"1.7.6.3. cfn-tags","text":"<p>This library helps you send attaching tags to resource created in a custom resource.</p>"},{"location":"developer/development/#1764-webpack-base","title":"1.7.6.4. webpack-base","text":"<p>This library defines the base Webpack template to compile custom resource runtime code.</p> <p><code>src/lib/custom-resources/cdk-kms-grant/runtime/package.json</code></p> <pre><code>{\n\"name\": \"@aws-accelerator/custom-resource-kms-grant-runtime\",\n\"version\": \"0.0.1\",\n\"private\": true,\n\"scripts\": {\n\"prepare\": \"webpack-cli --config webpack.config.ts\"\n},\n\"source\": \"src/index.ts\",\n\"main\": \"dist/index.js\",\n\"types\": \"dist/index.d.ts\",\n\"externals\": [\"aws-lambda\", \"aws-sdk\"],\n\"devDependencies\": {\n\"@aws-accelerator/custom-resource-runtime-webpack-base\": \"workspace:^0.0.1\",\n\"@types/aws-lambda\": \"8.10.46\",\n\"@types/node\": \"14.14.31\",\n\"ts-loader\": \"7.0.5\",\n\"typescript\": \"3.8.3\",\n\"webpack\": \"4.42.1\",\n\"webpack-cli\": \"3.3.11\"\n},\n\"dependencies\": {\n\"@aws-accelerator/custom-resource-runtime-cfn-response\": \"workspace:^0.0.1\",\n\"aws-lambda\": \"1.0.6\",\n\"aws-sdk\": \"2.668.0\"\n}\n}\n</code></pre> <p><code>src/lib/custom-resources/cdk-ec2-image-finder/runtime/webpack.config.ts</code></p> <pre><code>import { webpackConfigurationForPackage } from '@aws-accelerator/custom-resource-runtime-webpack-base';\nimport pkg from './package.json';\n\nexport default webpackConfigurationForPackage(pkg);\n</code></pre>"},{"location":"developer/development/#18-workarounds","title":"1.8. Workarounds","text":""},{"location":"developer/development/#181-stacks-with-same-name-in-different-regions","title":"1.8.1. Stacks with Same Name in Different Regions","text":"<p>The reason we're creating a <code>cdk.App</code> per account and per region and per phase is because stack names across environments might overlap, and at the time of writing, the CDK CLI does not handle stacks with the same name well. For example, when there is a stack <code>Phase1</code> in <code>us-east-1</code> and another stack <code>Phase1</code> in <code>ca-central-1</code>, the stacks will both be synthesized by CDK to the <code>cdk.out/Phase1.template.json</code> file and one stack will overwrite another's output. Using multiple <code>cdk.App</code>s overcomes this issues as a different <code>outdir</code> can be set on each <code>cdk.App</code>. These <code>cdk.App</code>s are managed by the <code>AccountStacks</code> abstraction.</p>"},{"location":"developer/development/#19-local-development","title":"1.9. Local Development","text":""},{"location":"developer/development/#191-local-installer-stack","title":"1.9.1. Local Installer Stack","text":"<p>Use CDK to synthesize the CloudFormation template.</p> <pre><code>cd src/installer/cdk\npnpx cdk synth\n</code></pre> <p>The installer template file is now in <code>cdk.out/AcceleratorInstaller.template.json</code>. This file can be used to install the installer stack.</p> <p>You can also deploy the installer stack directly from the command line but then you'd have to pass some stack parameters. See CDK documentation: Deploying with parameters.</p> <pre><code>cd accelerator/installer\npnpx cdk deploy --parameters GithubBranch=main --parameters ConfigS3Bucket=ASEA-myconfigbucket\n</code></pre>"},{"location":"developer/development/#192-local-initial-setup-stack","title":"1.9.2. Local Initial Setup Stack","text":"<p>There is a script called <code>cdk.sh</code> in <code>src/core/cdk</code> that allows you to deploy the Initial Setup stack.</p> <p>The script sets the required environment variables and makes sure all workspace projects are built before deploying the CDK stack.</p>"},{"location":"developer/development/#193-phase-stacks","title":"1.9.3. Phase Stacks","text":"<p>There is a script called <code>cdk.sh</code> in <code>src/deployments/cdk</code> that allows you to deploy a phase stack straight from the command-line without having to deploy the Initial Setup stack first.</p> <p>The script enables development mode which means that accounts, organizations, configuration, limits and outputs will be loaded from the local environment instead of loading the values from DynamoDB. The local files that need to be available in the <code>src/deployments/cdk</code> folder are the following.</p> <ol> <li><code>accounts.json</code> based on <code>accelerator/accounts</code> (-Parameters table)</li> </ol> <pre><code>[\n{\n\"key\": \"shared-network\",\n\"id\": \"000000000001\",\n\"arn\": \"arn:aws:organizations::000000000000:account/o-0123456789/000000000001\",\n\"name\": \"myacct-ASEA-shared-network\",\n\"email\": \"myacct+ASEA-mandatory-shared-network@example.com\",\n\"ou\": \"core\"\n},\n{\n\"key\": \"operations\",\n\"id\": \"000000000002\",\n\"arn\": \"arn:aws:organizations::000000000000:account/o-0123456789/000000000002\",\n\"name\": \"myacct-ASEA-operations\",\n\"email\": \"myacct+ASEA-mandatory-operations@example.com\",\n\"ou\": \"core\"\n}\n]\n</code></pre> <ol> <li><code>organizations.json</code> based on <code>accelerator/organizations</code> (-Parameters table)</li> </ol> <pre><code>[\n{\n\"ouId\": \"ou-0000-00000000\",\n\"ouArn\": \"arn:aws:organizations::000000000000:ou/o-0123456789/ou-0000-00000000\",\n\"ouName\": \"core\",\n\"ouPath\": \"core\"\n},\n{\n\"ouId\": \"ou-0000-00000001\",\n\"ouArn\": \"arn:aws:organizations::000000000000:ou/o-0123456789/ou-0000-00000001\",\n\"ouName\": \"prod\",\n\"ouPath\": \"prod\"\n}\n]\n</code></pre> <ol> <li><code>limits.json</code> based on <code>accelerator/limits</code> (-Parameters table)</li> </ol> <pre><code>[\n{\n\"accountKey\": \"shared-network\",\n\"limitKey\": \"Amazon VPC/VPCs per Region\",\n\"serviceCode\": \"vpc\",\n\"quotaCode\": \"L-F678F1CE\",\n\"value\": 15,\n\"region\": \"ca-central-1\"\n},\n{\n\"accountKey\": \"shared-network\",\n\"limitKey\": \"Amazon VPC/Interface VPC endpoints per VPC\",\n\"serviceCode\": \"vpc\",\n\"quotaCode\": \"L-29B6F2EB\",\n\"value\": 50,\n\"region\": \"ca-central-1\"\n}\n]\n</code></pre> <ol> <li><code>outputs.json</code> based on the -Outputs table</li> </ol> <pre><code>[\n{\n\"accountKey\": \"shared-network\",\n\"outputKey\": \"DefaultBucketOutputC7CE5936\",\n\"outputValue\": \"{\\\"type\\\":\\\"AccountBucket\\\",\\\"value\\\":{\\\"bucketArn\\\":\\\"arn:aws:s3:::ASEA-sharednetwork-phase1-cacentral1-18vq0emthri3h\\\",\\\"bucketName\\\":\\\"ASEA-sharednetwork-phase1-cacentral1-18vq0emthri3h\\\",\\\"encryptionKeyArn\\\":\\\"arn:aws:kms:ca-central-1:0000000000001:key/d54a8acb-694c-4fc5-9afe-ca2b263cd0b3\\\",\\\"region\\\":\\\"ca-central-1\\\"}}\"\n}\n]\n</code></pre> <ol> <li><code>context.json</code> that contains the default values for values that are otherwise passed as environment variables.</li> </ol> <pre><code>{\n\"acceleratorName\": \"ASEA\",\n\"acceleratorPrefix\": \"ASEA-\",\n\"acceleratorExecutionRoleName\": \"ASEA-PipelineRole\",\n\"defaultRegion\": \"ca-central-1\"\n}\n</code></pre> <ol> <li><code>config.json</code> that contains the Accelerator configuration.</li> </ol> <p>The script also sets the default execution role to allow CDK to assume a role in subaccounts to deploy the phase stacks.</p> <p>Now that you have all the required local files you can deploy the phase stacks using <code>cdk.sh</code>.</p> <pre><code>cd src/deployments/cdk\n./cdk.sh deploy --phase 1                             # deploy all phase 1 stacks\n./cdk.sh deploy --phase 1 --parallel                  # deploy all phase 1 stacks in parallel\n./cdk.sh deploy --phase 1 --account shared-network    # deploy phase 1 stacks for account shared-network in all regions\n./cdk.sh deploy --phase 1 --region ca-central-1       # deploy phase 1 stacks for region ca-central-1 for all accounts\n./cdk.sh deploy --phase 1 --account shared-network --region ca-central-1 # deploy phase 1 stacks for account shared-network and region ca-central\n</code></pre> <p>Other CDK commands are also available.</p> <pre><code>cd src/deployments/cdk\n./cdk.sh bootstrap --phase 1\n./cdk.sh synth --phase 1\n</code></pre>"},{"location":"developer/development/#110-testing","title":"1.10. Testing","text":"<p>We use <code>jest</code> for unit testing. There are no integration tests but this could be set-up by configuring the <code>Installer</code> CodePipeline to have a webhook on the repository and deploying changes automatically.</p> <p>To run unit tests locally you can run the following command in the monorepo.</p> <pre><code>pnpx recursive run test -- --pass-with-no-tests --silent\n</code></pre> <p>See CDK's documentation on Testing constructs for more information on how to tests CDK constructs.</p>"},{"location":"developer/development/#1101-validating-immutable-property-changes-and-logical-id-changes","title":"1.10.1. Validating Immutable Property Changes and Logical ID Changes","text":"<p>The most important unit test in this project is one that validates that logical IDs and immutable properties do not change unexpectedly. To avoid the issues described in section Resource Names and Logical IDs, Changing Logical IDs and Changing (Immutable) Properties.</p> <p>This test can be found in the <code>src/deployments/cdk/test/apps/unsupported-changes.spec.ts</code> file. It synthesizes the <code>Phase</code> stacks using mocked outputs and uses <code>jest</code> snapshots to compare against future changes.</p> <p>The test will fail when changing immutable properties or changing logical IDs of existing resources. In case the changes are expected then the snapshots will need to be updated. You can update the snapshots by running the following command.</p> <pre><code>pnpx run test -- -u\n</code></pre> <p>See Accept Unit Test Snapshot Changes.</p>"},{"location":"developer/development/#1102-upgrade-cdk","title":"1.10.2. Upgrade CDK","text":"<p>There's a test in the file <code>src/deployments/cdk/test/apps/unsupported-changes.spec.ts</code> that is currently commented out. The test takes a snapshot of the whole <code>Phase</code> stack and compares the snapshot to changes in the code.</p> <pre><code>test('templates should stay exactly the same', () =&gt; {\nfor (const [stackName, resources] of Object.entries(stackResources)) {\n// Compare the relevant properties to the snapshot\nexpect(resources).toMatchSnapshot(stackName);\n}\n});\n</code></pre> <p>Before upgrading CDK we uncomment this test. We run the test to update all the snapshots. Then we update all CDK versions and run the test again to compare the snapshots with the code using the new CDK version. If the test passes, then the upgrade should be stable.</p> <p>Action Item: Automate this process.</p>"},{"location":"developer/release-process/","title":"1. AWS Internal - Accelerator Release Process","text":""},{"location":"developer/release-process/#11-creating-a-new-accelerator-code-release","title":"1.1. Creating a new Accelerator Code Release","text":"<ol> <li>Ensure <code>main</code> branch is in a suitable state</li> <li>Disable branch protection for both the <code>main</code> branch and for the <code>release/</code> branches</li> <li> <p>Create a version branch with SemVer semantics and a <code>release/</code> prefix: e.g. <code>release/v1.0.5</code> or <code>release/v1.0.5-b</code> using github UI or using the commands below</p> <ul> <li>On latest <code>main</code>, run: <code>git checkout -b release/vX.Y.Z</code></li> <li>Important: Certain git operations are ambiguous if tags and branches have the same name. Using the <code>release/</code> prefix reserves the actual version name for the tag itself; i.e. every <code>release/vX.Y.Z</code> branch will have a corresponding <code>vX.Y.Z</code> tag.</li> <li>Push that branch to GitHub (if created locally)<ul> <li><code>git push origin release/vX.Y.Z</code></li> </ul> </li> </ul> </li> <li> <p>The release workflow will run, and create a DRAFT release if successful with all commits since the last tagged release.</p> </li> <li>Prune the commits that have been added to the release notes (e.g. remove any low-information commits)</li> <li>Publish the release - this creates the git tag in the repo and marks the release as latest. It also bumps the <code>version</code> key in several project <code>package.json</code> files.</li> <li> <p>Re-enable branch protection for both the <code>main</code> branch and for the <code>release/</code> branches</p> <ul> <li>Note: The <code>Publish</code> operation will run the following GitHub Action, which merges the <code>release/vX.Y.Z</code> branch to <code>main</code>. Branch Protection in GitHub will cause this to fail, and why we are momentarily disabling branch protection.</li> </ul> </li> </ol>"},{"location":"developer/tech-stack/","title":"1. Technology Stack","text":""},{"location":"developer/tech-stack/#11-overview","title":"1.1. Overview","text":"<p>We use TypeScript, NodeJS, CDK and CloudFormation. You can find some more information in the sections below.</p>"},{"location":"developer/tech-stack/#12-typescript-and-nodejs","title":"1.2. TypeScript and NodeJS","text":"<p>In the following sections we describe the tools and libraries used along with TypeScript.</p>"},{"location":"developer/tech-stack/#121-pnpm","title":"1.2.1. pnpm","text":"<p>We use the <code>pnpm</code> package manager along with <code>pnpm workspaces</code> to manage all the packages in this monorepo.</p> <p>https://pnpm.js.org</p> <p>https://pnpm.js.org/en/workspaces</p> <p>The binary <code>pnpx</code> runs binaries that belong to <code>pnpm</code> packages in the workspace.</p> <p>https://pnpm.js.org/en/pnpx-cli</p>"},{"location":"developer/tech-stack/#122-prettier","title":"1.2.2. prettier","text":"<p>We use <code>prettier</code> to format code in this repository. A GitHub action makes sure that all the code in a pull requests adheres to the configured <code>prettier</code> rules. See Github Actions.</p>"},{"location":"developer/tech-stack/#123-eslint","title":"1.2.3. eslint","text":"<p>We use <code>eslint</code> as a static analysis tool that checks our TypeScript code. A GitHub action makes sure that all the code in a pull requests adheres to the configured <code>eslint</code> rules. See Github Actions.</p>"},{"location":"developer/tech-stack/#13-cloudformation","title":"1.3. CloudFormation","text":"<p>CloudFormation deploys both the Accelerator stacks and resources and the deployed stacks and resources. See Operations Guide: System Overview for the distinction between Accelerator resources and deployed resources.</p>"},{"location":"developer/tech-stack/#14-cdk","title":"1.4. CDK","text":"<p>AWS CDK defines the cloud resources in a familiar programming language. While AWS CDK supports TypeScript, JavaScript, Python, Java, and C#/.Net, the contributions should be made in Typescript, as outlined in the Accelerator Development First Principles.</p> <p>Developers can use programming languages to define reusable cloud components known as Constructs. You compose these together into Stacks and Apps. Learn more at https://docs.aws.amazon.com/cdk/latest/guide/home.html</p>"},{"location":"faq/","title":"1. Accelerator Basic Operation and Frequently asked Questions","text":""},{"location":"faq/#11-operational-activities","title":"1.1. Operational Activities","text":"1.1.1. How do I add new AWS accounts to my AWS Organization? 1.1.2. I tried to enroll a new account via Control Tower but it failed? 1.1.3. Can I use AWS Organizations for all tasks I currently use AWS Organizations for? 1.1.4. How do I make changes to items I defined in the Accelerator configuration file during installation? 1.1.5. Can I update the config file while the State Machine is running? When will those changes be applied? 1.1.6. What if I really mess up the configuration file? 1.1.7. What if my State Machine fails? Why? Previous solutions had complex recovery processes, what's involved? 1.1.8. How do I update some of the supplied sample configuration items found in reference-artifact, like SCPs and IAM policies? 1.1.9. I deployed AWS Managed Active Directory (MAD) as part of my deployment, how do I manage Active Directory domain users, groups, and domain policies after deployment? 1.1.10. How do I suspend an AWS account? 1.1.11. I need a new VPC, where shall I define it? 1.1.12. How do I modify and extend the Accelerator or execute my own code after the Accelerator provisions a new AWS account or the state machine executes? 1.1.13. How can I easily access my virtual machines or EC2 instances? 1.1.14. I ran the state machine but it failed when it tried to delete the default VPC?"},{"location":"faq/#how-do-i-add-new-aws-accounts-to-my-aws-organization","title":"How do I add new AWS accounts to my AWS Organization?","text":"<p>We offer three options and all can be used in the same Accelerator deployment. All options work with AWS Control Tower, ensuring the account is both ingested into Control Tower and all Accelerator guardrails are automatically applied.</p> <p>No matter the mechanism you choose, new accounts will automatically be blocked from use until fully guardrailed, the Accelerator will automatically execute, and accounts will automatically be ingested into AWS Control Tower (if deployed).</p> <p>Option 1</p> <p>Users can simply add the following five lines to the configuration file <code>workload-account-configs</code> section and rerun the state machine. The majority of the account configuration will be picked up from the OU the AWS account has been assigned. You can also add additional account specific configuration, or override items like the default OU budget with an account specific budget. This mechanism is often used by customers that wish to programmatically create AWS accounts using the Accelerator and allows for adding many new accounts at one time.</p> <pre><code>\"fun-acct\": {\n\"account-name\": \"TheFunAccount\",\n\"email\": \"myemail+aseaT-funacct@example.com\",\n\"src-filename\": \"config.json\",\n\"ou\": \"Sandbox\"\n}\n</code></pre> <p>Option 2</p> <p>We've heard consistent feedback that our customers wish to use native AWS services and do not want to do things differently once security controls, guardrails, or accelerators are applied to their environment. In this regard, simply create your new AWS account in AWS Organizations as you did before**, either by a) using the AWS Console or b) by using standard AWS account creation API's, CLI or 3rd party tools like Terraform.</p> <ul> <li>** IMPORTANT: When creating the new AWS account using AWS Organizations, you need to specify the role name provided in the Accelerator configuration file <code>global-options\\organization-admin-role</code>, otherwise we cannot bootstrap the account. In Control Tower  installations, this MUST be set to <code>AWSControlTowerExecution</code>, for customers who installed prior to v1.2.5 this value is <code>AWSCloudFormationStackSetExecutionRole</code> and after v1.2.5 we were recommending using the role <code>OrganizationAccountAccessRole</code> as this role is used by default by AWS Organizations if no role name is specified when creating AWS accounts through the AWS console or cli.</li> <li>On account creation we will apply a quarantine SCP which prevents the account from being used by anyone until the Accelerator has applied the appropriate guardrails</li> <li>Moving the account into the appropriate OU triggers the state machine and the application of the guardrails to the account, once complete, we will remove the quarantine SCP.<ul> <li>NOTE: Accounts CANNOT be moved between OU's to maintain compliance, so select the proper top-level OU with care</li> <li>In AWS Organizations, select ALL the newly created AWS accounts and move them all (preferably at once) to the correct destination OU (assuming the same OU for all accounts)</li> <li>In case you need to move accounts to multiple OU's we have added a 2 minute delay before triggering the State Machine</li> <li>Any accounts moved after the 2 minute window will NOT be properly ingested, and will need to be ingested on a subsequent State Machine Execution.</li> </ul> </li> </ul> <p>Option 3</p> <p>Create your account using Account Factory in the AWS Control Tower console.</p>"},{"location":"faq/#i-tried-to-enroll-a-new-account-via-control-tower-but-it-failed","title":"I tried to enroll a new account via Control Tower but it failed?","text":"<p>or \"The state machine failed during the 'Load Organization Configuration' step with the error 'The Control Tower account: ACCOUNT_NAME is in a failed state ERROR'\"</p> <p>If account enrollment fails within Control Tower, you will need to follow the troubleshooting steps here. A common reason for this is not having the <code>ControlTowerExectution</code> role created in the account you are trying to enroll. Even after you successfully enroll the account, it is possible the state machine will fail at <code>Load Organization Configuration</code>. If you look at the CloudWatch logs you will see the error message:</p> <pre><code>There were errors while loading the configuration: The Control Tower account: ACCOUNT_NAME is in a failed state ERROR.\n</code></pre> <p>This is because the Accelerator checks that there are no errors with Control Tower before continuing. In some cases Control Tower can leave an orphaned Service Catalog product in an Error state. You need to cleanup Control Towers Service Catalogs Provisioned Products so there are no products remaining in an error or tainted state before you can successfully re-run the state machine.</p>"},{"location":"faq/#can-i-use-aws-organizations-for-all-tasks-i-currently-use-aws-organizations-for","title":"Can I use AWS Organizations for all tasks I currently use AWS Organizations for?","text":"<p>In AWS Organizations you can continue to:</p> <ul> <li>create and rename AWS accounts</li> <li>move AWS accounts between OU's</li> <li>create, delete and rename OU's, including support for nested OU's</li> <li>create, rename, modify, apply and remove SCP's</li> </ul> <p>What can't I do:</p> <ul> <li>modify Accelerator or Control Tower controlled SCP's</li> <li>add/remove SCP's on top-level OU's (these are Accelerator and/or Control Tower controlled)<ul> <li>users can change SCP's on non-top-level OU's and non-Accelerator controlled accounts as they please</li> </ul> </li> <li>add/remove SCP's on specific accounts that have Accelerator controlled SCPs</li> <li>move an AWS account between top-level OU's (i.e. <code>Sandbox</code> to <code>Prod</code> is a security violation)<ul> <li>moving between <code>Prod/sub-ou-1</code> to <code>Prod/sub-ou2</code> or <code>Prod/sub-ou2/sub-ou2a/sub-ou2ab</code> is fully supported</li> </ul> </li> <li>create a top-level OU (need to validate, as they require config file entries)</li> <li>remove quarantine SCP from newly created accounts</li> <li>we do not support forward slashes (<code>/</code>) in OU names, even though the AWS platform does</li> </ul> <p>More details:</p> <ul> <li>If an AWS account is renamed, an account email is changed, or an OU is renamed, on the next state machine execution, the config file will automatically be updated.</li> <li>If you edit an Accelerator controlled SCP through Organizations, we will reset it per what is defined in the Accelerator configuration files.</li> <li>If you add/remove an SCP from a top-level OU or Accelerator controlled account, we will put them back as defined in the Accelerator configuration file.</li> <li>If you move an account between top-level OU's, we will put it back to its original designated top-level OU.</li> <li>The Accelerator fully supports nested OU's, customers can create any depth OU structure in AWS Organizations and add/remove/change SCP's below the top-level as they desire or move accounts between these OU's without restriction. Users can create OU's to the full AWS OU structure/depth</li> <li>Except for the Quarantine SCP applied to specific accounts, we do not 'control' SCP's below the top level, customers can add/create/customize SCP's<ul> <li>as of v1.3.3 customers can optionally control account level SCP's through the configuration file</li> </ul> </li> </ul>"},{"location":"faq/#how-do-i-make-changes-to-items-i-defined-in-the-accelerator-configuration-file-during-installation","title":"How do I make changes to items I defined in the Accelerator configuration file during installation?","text":"<p>Simply update your configuration file in CodeCommit and rerun the state machine! In most cases, it is that simple.</p> <p>If you ask the Accelerator to do something that is not supported by the AWS platform, the state machine will fail, so it needs to be a supported capability. For example, the platform does not allow you to change the CIDR block on a VPC, but you can accomplish this as you would today by using the Accelerator to deploy a new second VPC, manually migrating workloads, and then removing the deprecated VPC from the Accelerator configuration.</p> <p>Below we have also documented additional considerations when creating or updating the configuration file.</p> <p>It should be noted that we have added code to the Accelerator to block customers from making many 'breaking' or impactful changes to their configuration files. If someone is positive they want to make these changes, we also provide override switches to allow these changes to be attempted forcefully.</p>"},{"location":"faq/#can-i-update-the-config-file-while-the-state-machine-is-running-when-will-those-changes-be-applied","title":"Can I update the config file while the State Machine is running? When will those changes be applied?","text":"<p>Yes. The state machine captures a consistent input state of the requested configuration when it starts. The running Accelerator instance does not see or consider any configuration changes that occur after it has started. All configuration changes occurring after the state machine is running will only be leveraged on the next state machine execution.</p>"},{"location":"faq/#what-if-i-really-mess-up-the-configuration-file","title":"What if I really mess up the configuration file?","text":"<p>The Accelerator is designed with checks to compare your current configuration file with the version of the config file from the previous successful execution of the state machine. If we believe you are making major or breaking changes to the config file, we will purposefully fail the state machine. See Config file and Deployment Protections for more details.</p> <p>With the release of v1.3.0 we introduced state machine scoping capabilities to further protect customers, detailed here.</p>"},{"location":"faq/#what-if-my-state-machine-fails-why-previous-solutions-had-complex-recovery-processes-whats-involved","title":"What if my State Machine fails? Why? Previous solutions had complex recovery processes, what's involved?","text":"<p>If your main state machine fails, review the error(s), resolve the problem and simply re-run the state machine. We've put a huge focus on ensuring the solution is idempotent and to ensure recovery is a smooth and easy process.</p> <p>Ensuring the integrity of deployed guardrails is critical in operating and maintaining an environment hosting protected data. Based on customer feedback and security best practices, we purposely fail the state machine if we cannot successfully deploy guardrails.</p> <p>Additionally, with millions of active customers each supporting different and diverse use cases and with the rapid rate of evolution of the AWS platform, sometimes we will encounter unexpected circumstances and the state machine might fail.</p> <p>We've spent a lot of time over the course of the Accelerator development process ensuring the solution can roll forward, roll backward, be stopped, restarted, and rerun without issues. A huge focus was placed on dealing with and writing custom code to manage and deal with non-idempotent resources (like S3 buckets, log groups, KMS keys, etc.). We've spent a lot of time ensuring that any failed artifacts are automatically cleaned up and don't cause subsequent executions to fail. We've put a strong focus on ensuring you do not need to go into your various AWS sub-accounts and manually remove or cleanup resources or deployment failures. We've also tried to provide usable error messages that are easy to understand and troubleshoot. As new scenarios are brought to our attention, we continue to adjust the codebase to better handle these situations.</p> <p>Will your state machine fail at some point in time, likely. Will you be able to easily recover and move forward without extensive time and effort, YES!</p>"},{"location":"faq/#how-do-i-update-some-of-the-supplied-sample-configuration-items-found-in-reference-artifact-like-scps-and-iam-policies","title":"How do I update some of the supplied sample configuration items found in reference-artifact, like SCPs and IAM policies?","text":"<p>To override items like SCP's or IAM policies, customers simply need to provide the identically named file in their input bucket. As long as the file exists in the correct folder in the customers input bucket, the Accelerator will use the customers supplied version of the configuration item, rather than the Accelerator version. Customer SCP's need to be placed into a folder named <code>scp</code> and IAM policies in a folder named <code>iam-policy</code> (case sensitive).</p> <p>The Accelerator was designed to allow customers complete customization capabilities without any requirement to update code or fork the GitHub repo. Additionally, rather than forcing customers to provide a multitude of config files for a standard or prescriptive installation, we provide and auto-deploy with Accelerator versions of most required configuration items from the reference-artifacts folder of the repo. If a customer provides the required configuration file in their Accelerator S3 input bucket, we will use the customer supplied version of the configuration file rather than the Accelerator version. At any time, either before initial installation, or in future, a customer can place new or updated SCPs, policies, or other supported file types into their input bucket and we will use those instead of or in addition to Accelerator supplied versions. Customer only need to provide the specific files they wish to override, not all files.</p> <p>Customers can also define additional SCPs (or modify existing SCPs) using the name, description and filename of their choosing, and deploy them by referencing them on the appropriate organizational unit in the config file.</p> <p>Prior to v1.2.5, if we updated the default files, we overwrote customers customizations during upgrade. Simply updating the timestamp after upgrade on the customized versions and then rerunning the state machine re-instates customer customizations. In v1.2.5 we always use the customer customized version from the S3 bucket. Its important customers assess newly provided defaults during an upgrade process to ensure they are incorporating all the latest fixes and improvements. If a customer wants to revert to Accelerator provided default files, they will need to manually copy it from the repo into their input bucket.</p> <p>NOTE: Most of the provided SCPs are designed to protect the Accelerator deployed resources from modification and ensure the integrity of the Accelerator. Extreme caution must be exercised if the provided SCPs are modified. In v1.5.0 we restructured the SCPs based on a) customer requests, and b) the addition of Control Tower support for new installs.</p> <ul> <li>we reorganized and optimized our SCP's from 4 SCP files down to 3 SCP files, without removing any protections or guardrails;</li> <li>these optimizations have resulted in minor enhancements to the SCP protections and in some cases better scoping;</li> <li>the first two SCP files (Part-0 and Part-1) contain the controls which protect the integrity of the Accelerator itself;</li> <li>the third file (Sensitive, Unclass, Sandbox) contains customer data protection specific guardrails, which may change based on workload data classification or customer profiles and requirements;</li> <li>this freed the fourth SCP for use by Control Tower. As Control Tower leverages 2 SCP files on the Security OU, we have moved some of our SCP's to the account level.</li> </ul>"},{"location":"faq/#i-deployed-aws-managed-active-directory-mad-as-part-of-my-deployment-how-do-i-manage-active-directory-domain-users-groups-and-domain-policies-after-deployment","title":"I deployed AWS Managed Active Directory (MAD) as part of my deployment, how do I manage Active Directory domain users, groups, and domain policies after deployment?","text":"<p>Customers have clearly indicated they do NOT want to use the Accelerator to manage their Active Directory domain or change the way they manage Active Directory on an ongoing basis. Customer have also indicated, they need help getting up and running quickly. For these reasons, the Accelerator only sets the domain password policy, and creates AD users and groups on the initial installation of MAD. After the initial installation, customers must manage Windows users and groups using their traditional tools. A bastion Windows host is deployed as a mechanism to support these capabilities. Passwords for all newly created MAD users have been stored, encrypted, in AWS Secrets Manager in the Management (root) Organization AWS account.</p> <p>To create new users and groups:</p> <ul> <li>RDP into the ASEA-RDGW bastion host in the Ops account<ul> <li>Run ADUC and create users and groups as you please under the NETBIOSDOMAIN (example) tree</li> </ul> </li> <li>Or run the appropriate PowerShell command</li> <li>Go to AWS SSO and map the Active Directory group to the appropriate AWS account and permission set</li> </ul> <p>The Accelerator will not create/update/delete new AD users or groups, nor will it update the domain password policy after the initial installation of Managed Active Directory. It is your responsibility to rotate these passwords on a regular basis per your organizations password policy. (NOTE: After updating the admin password it needs to be stored back in secrets manager).</p>"},{"location":"faq/#how-do-i-suspend-an-aws-account","title":"How do I suspend an AWS account?","text":"<p>Suspending accounts is blocked via SCP and purposely difficult, two options exist:</p> <ol> <li>Modify SCP method (not desired)</li> <li> <p>Leverage the UnManaged OU</p> <ul> <li>validate your config file contains the value: <code>\"ignored-ous\": [\"UnManaged\"]</code><ul> <li>the state machine must be executed at least once after this value is added to the config file</li> </ul> </li> <li>In AWS Organizations create an OU named <code>UnManaged</code> in the root of the OU tree, if it does not exist</li> <li>Change to the <code>us-east-1</code> region and open CloudWatch and navigate to Rules<ul> <li>Select the <code>PBMMAccel-MoveAccount_rule</code>, select actions, select <code>Disable</code></li> </ul> </li> <li>In Organizations move the account to be suspended to the <code>UnManaged</code> OU</li> <li>Change to the <code>us-east-1</code> region and open CloudWatch and navigate to Rules<ul> <li>Select the <code>PBMMAccel-MoveAccount_rule</code>, select actions, select <code>Enable</code></li> </ul> </li> <li>login to the account to be suspended as the account root user</li> <li>suspend the account through <code>My Account</code></li> <li>Run the state machine (from the Organization management account), the account will:<ul> <li>have a deleted=true value added to the config file</li> <li>be moved to the suspended OU (OU value and path stays the same in the config file)</li> <li>deleted=true causes OU validation to be skipped on this account on subsequent SM executions</li> </ul> </li> <li>If the AWS account was listed in the mandatory-accounts section of the config file the SM will fail (expected)<ul> <li>after the above tasks have been completed, remove all references to the suspended mandatory account from the config file</li> <li>rerun the state machine, specifying: <code>{ \"overrideComparison\": true }</code></li> </ul> </li> <li>Deleted accounts will continue to appear under the <code>Suspended</code> OU for 90-days</li> </ul> </li> </ol>"},{"location":"faq/#i-need-a-new-vpc-where-shall-i-define-it","title":"I need a new VPC, where shall I define it?","text":"<p>You can define a VPC in one of four major sections of the Accelerator configuration file:</p> <ul> <li>within an organization unit (this is the recommended and preferred method);</li> <li>within an account in mandatory-account-configs;</li> <li>within an account in workload-account-configs;</li> <li>defined within an organization unit, but opted-in within the account config.</li> </ul> <p>We generally recommend most items be defined within organizational units, such that all workload accounts pickup their persona from the OU they are associated and minimize per account configuration. Both a local account based VPC (as deployed in the Sandbox OU accounts), or a central shared VPC (as deployed in the Dev/Test/Prod OU accounts in many of the example configs) can be defined at the OU level.</p> <p>As mandatory accounts often have unique configuration requirements, for example the centralized Endpoint VPC, they must be configured within the account's configuration. Customers can define VPC's or other account specific settings within any account's configuration, but this requires editing the configuration file for each account configuration.</p> <p>Prior to v1.5.0, local VPC's defined at the OU level were each deployed with the same CIDR ranges and therefor could not be connected to a TGW. Local VPC's requiring centralized networking (i.e. TGW connectivity) were required to be defined in each account config, adding manual effort and bloating the configuration file.</p> <p>The addition of <code>dynamic</code> and <code>lookup</code> CIDR sources in v1.5.0 resolves this problem. Local VPCs can be defined in an OU, and each VPC will be dynamically assigned a unique CIDR range from the assigned CIDR pool, or looked up from the DynamoDB database. Customers can now ensure connected, templated VPCs are consistently deployed to every account in an OU, each with unique IP addresses.</p> <p>v1.5.0 also added a new opt-in VPC capability. A VPC is defined in an OU and a new config file variable is added to this VPC <code>opt-in: true</code>. When opt-in is set to true, the state machine does NOT create the VPC for the accounts in the OU, essentially ignoring the VPC definition. Select accounts in the OU can then be opted-in to the VPC(s) definition, by adding the value <code>accountname\\opt-in-vpcs: [\u201copt-in-vpc-name1\u201d, \u201copt-in-vpc-name2\u201d, \u201copt-in-vpc-nameN\u201d]</code> to the specific accounts which need the VPC(s). A VPC definition with the specified name (i.e. <code>opt-in-vpc-name1</code>) and the value <code>opt-in: true</code>, must exist in the OU config for the specified account. When these conditions apply, the VPC will be created in the account per the OU definition. Additional opt-in VPCs can be added to an account, but VPC's cannot be removed from the opt-in-vpcs array. VPC's can be TGW attached, assuming <code>dynamic</code> cidr-src is utilized, or DynamoDB is prepopulated with the required CIDR ranges using <code>lookup</code> mode. <code>cidr-src</code> provided is suitable for disconnected Sandbox type accounts.</p> <p>The Future: While Opt-In VPCs are powerful, we want to take this further. Why not deploy an AWS Service Catalog template which contains the names of all the available opt-in VPCs for the accounts OU, inside each account. An account end user could then request a new VPC for their account from the list of available opt-in patterns. A user's selection would be sent to a centralized queue for approval (w/auto-approval options), which would result in the opt-in-vpc entry in that account being updated with the end users requested VPC pattern and the personalized VPC being created in the account and attached to the centralized TGW (if part of the pattern). This would ensure all VPC's conformed to a set of desirable design patterns, but also allow the end-user community choices based on their desired development and app patterns. If you like this idea, please +1 this feature request.</p>"},{"location":"faq/#how-do-i-modify-and-extend-the-accelerator-or-execute-my-own-code-after-the-accelerator-provisions-a-new-aws-account-or-the-state-machine-executes","title":"How do I modify and extend the Accelerator or execute my own code after the Accelerator provisions a new AWS account or the state machine executes?","text":"<p>Flexibility:</p> <ul> <li>The AWS Secure Environment Accelerator was developed to enable extreme flexibility without requiring a single line of code to be changed. One of our primary goals throughout the development process was to avoid making any decisions that would result in users needing to fork or branch the Accelerator codebase. This would help ensure we had a sustainable and upgradable solution for a broad customer base over time.</li> <li>Functionality provided by the Accelerator can generally be controlled by modifying the main Accelerator configuration file.</li> <li>Items like SCP's, rsyslog config, PowerShell scripts, and iam-policies have config files provided and auto-deployed as part of the Accelerator to deliver on the prescriptive architecture (these are located in the \\reference-artifacts folder of the GitHub repo for reference). If you want to alter the functionality delivered by any of these additional config files, you can simply provide your own by placing it in your specified Accelerator bucket in the appropriate sub-folder. The Accelerator will use your provided version instead of the supplied repo reference version.</li> <li>As SCP's and IAM policies are defined in the main config file, you can simply define new policies, pointing to new policy files, and provide these new files in your bucket, and they will be used.</li> <li>While a sample firewall config file is provided in the \\reference-artifacts folder, it must be manually placed in your S3 bucket/folder on new Accelerator deployments</li> <li>Any/all of these files can be updated at any time and will be used on the next execution of the state machine</li> <li>Over time, we predict we will provide several sample or reference architectures and not just the current single PBMM architecture (all located in the \\reference-artifacts\\SAMPLE_CONFIGS folder).</li> </ul> <p>Extensibility:</p> <ul> <li>Every execution of the state machine sends a state machine status event to a state machine SNS topic</li> <li>These status events include the Success/Failure status of the state machine, and on success, a list of all successfully processed AWS accounts</li> <li>While this SNS topic is automatically subscribed to a user provided email address for user notification, users can also create additional SNS subscriptions to enable triggering their own subsequent workflows, state machines, or custom code using any supported SNS subscription type (Lambda, SQS, Email, HTTPS, HTTPS)</li> <li>Additionally, objects deployed within an account have been populated in Parameter Store, see answer 1.3.2 for details</li> </ul> <p>Example:</p> <ul> <li>One of our early adopter customers has developed a custom user interface which allows their clients to request new AWS environments. Clients provide items like cost center, budget, and select their environment requirements (i.e. Sandbox, Unclass or full sensitive SDLC account set). On appropriate approval, this pushes the changes to the Accelerator configuration file and triggers the state machine.</li> <li>Once the state machine completes, the SNS topic triggers their follow-up workflow, validates the requested accounts were provisioned, updates the customer's account database, and then executes a collection of customer specific follow-up workflow actions on any newly provisioned accounts.</li> </ul>"},{"location":"faq/#how-can-i-easily-access-my-virtual-machines-or-ec2-instances","title":"How can I easily access my virtual machines or EC2 instances?","text":"<p>The preferred and recommended method to connect to instances within the Accelerator is by using AWS Systems Manager Session Manager. Session Manager allows access to instances without the need to have any open firewall ports. Session Manager allows for Command line access to instances (both Windows and Linux) directly through the AWS console, with all activity logged to CloudWatch Logs. Session Manager enables customers to connect to Windows instances with a native RDP client and Linux instances with a native SSH client, if desired. Customers can gain quick access to instances through the AWS console, or using their preferred clients.</p> <p>General</p> <ul> <li>Both the RDGW and rsyslog instances deployed in the Ops account are properly configured for Systems Manager Session Manager</li> <li>We have implemented automation such that all instances are also automatically configured for Session Manager (i.e. configured with the appropriate launch role, has a recent session manager agent installed (most amazon AMI's do), has access to an SSM endpoint)</li> </ul> <p>NEW - System Manager Fleet Manager is now available, which allows connecting graphically to Windows desktops directly from the AWS console without the need for any command line access or tools, and without any requirement for an RDSH/RDP client. As the Accelerator configures Systems Manager, this feature \"simply works\" for most instances deployed within the Accelerator.</p> <pre><code>- Navigate to Systems Manager, open Fleet Manager, select an instance, click \"Node Actions\" and select \"Connect with Remote Desktop\". If you are logging in via AWS SSO, you can seamlessly connect. IAM users must supply valid credentials.\n</code></pre> <p>Connecting to an Instance</p> <ul> <li>From the AWS Console<ul> <li>Go to: EC2, Instances, select the instance (i.e. ASEA-RDGW), click \u201cConnect\u201d, select Session Manager, Connect</li> <li>Ideal for Linux or Windows PowerShell users</li> <li>Everything is fully logged for future reference</li> </ul> </li> <li>Directly through local RDP client using Session Managers tunnel capability:<ul> <li>Provides direct access to your instances/host without any open internet ports on the RDGW instance, using a local/fat client tool</li> <li>Install AWS CLI v2 on your PC - available here (uninstall CLIv1 first, if installed)</li> <li>Install the SSM plugin on your PC - available here</li> <li>Get AWS temporary credentials from AWS SSO for the account your workload resides (i.e. Ops account when accessing the ASEA-RDGW instance) by selecting \u201cCommand line or programmatic access\u201d instead of \u201cManagement Console\u201d and paste them into a command prompt<ul> <li>i.e. via logging in here: https://xxxxxxx.awsapps.com/start or</li> <li>This blog describes the process to use SSO to get credentials for the AWS CLI directly without the GUI</li> </ul> </li> <li>Then enter: aws ssm start-session --target \"i-12345678901234567\" --document-name AWS-StartPortForwardingSession --parameters portNumber=\"3389\",localPortNumber=\"56789\"--region ca-central-1<ul> <li>Command syntax is slightly different on Linux/Mac</li> <li>Replace i-1111adddce582b23c with the instance id of your RDGW instance</li> <li>A tunnel will open</li> <li>As these are tunnels to proprietary protocols (i.e. RDP/screen scraping) session content is not logged.</li> </ul> </li> <li>Run mstsc/rdp client and connect to 127.0.0.1:56789<ul> <li>By replacing 3389 with a new port for another applications (i.e. SSH running on a Linux instance), you can connect to a different application type</li> <li>You can change the local port by changing 56789 to any other valid port number (i.e. connecting to multiple instances at the same time)</li> </ul> </li> <li>Login with the windows credentials discussed above in the format NETBIOSDOMAIN\\User1 (i.e. example\\user1)<ul> <li>Your Netbios domain is found here in your config file: \"netbios-domain\": \"example\",</li> </ul> </li> </ul> </li> <li>Connect to your desktop command line to command line interface of remote Windows or Linux servers, instead of through console (i.e. no tunnel):<ul> <li>aws ssm start-session --target \"i-090c25e64c2d9d276\"\"--region ca-central-1</li> <li>Replace i-xxx with your instance ID</li> <li>Everything is fully logged for future reference</li> </ul> </li> <li>If you want to remove the region from your command line, you can:<ul> <li>Type: \u201caws configure\u201d from command prompt, hit {enter} (key), {enter} (secret), enter: ca-central-1, {enter}</li> </ul> </li> </ul>"},{"location":"faq/#i-ran-the-state-machine-but-it-failed-when-it-tried-to-delete-the-default-vpc-the-state-machine-cannot-delete-the-default-vpc-error-vpc-has-dependencies-and-cannot-be-deleted","title":"I ran the state machine but it failed when it tried to delete the default VPC? The state machine cannot delete the default VPC (Error: VPC has dependencies and cannot be deleted)","text":"<p>You need to ensure that resources don\u2019t exist in the default VPC or else the state machine won't be able to delete it. If you encounter this error, you can either delete the resources within the VPC or delete the default VPC manually and run the state machine again.</p>"},{"location":"faq/#12-existing-accounts-organizations","title":"1.2. Existing Accounts / Organizations","text":"1.2.1. How do I import an existing AWS account into my Accelerator managed AWS Organization (or what if I created a new AWS account with a different Organization trust role)? 1.2.2. Is it possible to deploy the Accelerator on top of an AWS Organization that I have already installed the AWS Landing Zone (ALZ) solution into? 1.2.3. What if I want to move an account from an AWS Organization that has the ALZ deployed into an AWS Organization running the Accelerator?"},{"location":"faq/#how-do-i-import-an-existing-aws-account-into-my-accelerator-managed-aws-organization-or-what-if-i-created-a-new-aws-account-with-a-different-organization-trust-role","title":"How do I import an existing AWS account into my Accelerator managed AWS Organization (or what if I created a new AWS account with a different Organization trust role)?","text":"<ul> <li>Ensure you have valid administrative privileges for the account to be invited/added</li> <li>Add the account to your AWS Organization using standard processes (i.e. Invite/Accept)<ul> <li>this process does NOT create an organization trust role</li> <li>imported accounts do NOT have the quarantine SCP applied as we don't want to break existing workloads</li> </ul> </li> <li>Login to the account using the existing administrative credentials</li> <li>Execute the Accelerator provided CloudFormation template to create the required Accelerator bootstrapping role - in the GitHub repo here: <code>reference-artifacts\\Custom-Scripts\\Import-Account-CFN-Role-Template.yml</code><ul> <li>add the account to the Accelerator config file and run the state machine</li> </ul> </li> <li>If you simply created the account with an incorrect role name, you likely need to take extra steps:<ul> <li>Update the Accelerator config file to add the parameter: <code>global-options\\ignored-ous</code> = <code>[\"UnManagedAccounts\"]</code></li> <li>In AWS Organizations, create a new OU named <code>UnManagedAccounts</code> (case sensitive)</li> <li>Move the account to the <code>UnManagedAccounts</code> OU</li> <li>You can now remove the Quarantine SCP from the account</li> <li>Assume an administrative role into the account</li> <li>Execute the Accelerator provided CloudFormation template to create the required Accelerator bootstrapping role</li> </ul> </li> </ul>"},{"location":"faq/#is-it-possible-to-deploy-the-accelerator-on-top-of-an-aws-organization-that-i-have-already-installed-the-aws-landing-zone-alz-solution-into","title":"Is it possible to deploy the Accelerator on top of an AWS Organization that I have already installed the AWS Landing Zone (ALZ) solution into?","text":"<p>Existing ALZ customers are required to uninstall their ALZ deployment before deploying the Accelerator. Please work with your AWS account team to find the best mechanism to uninstall the ALZ solution (procedures and scripts exist). It is often easier to migrate AWS accounts to a new Accelerator Organization, per the process detailed in the next FAQ question.  Additionally, please reference the following section of the Installation Guide for additional considerations.</p>"},{"location":"faq/#what-if-i-want-to-move-an-account-from-an-aws-organization-that-has-the-alz-deployed-into-an-aws-organization-running-the-accelerator","title":"What if I want to move an account from an AWS Organization that has the ALZ deployed into an AWS Organization running the Accelerator?","text":"<p>Before removing the AWS account from the source organization, terminate the AWS Service Catalog product associated with the member account that you're interested in moving. Ensuring the product terminates successfully and that there aren't any remaining CloudFormation stacks in the account that were deployed by the ALZ. You can then remove the account from the existing Organization and invite it into the new organization. Accounts invited into the Organization do NOT get the <code>Deny All</code> SCP applied, as we do not want to break existing running workloads. Moving the newly invited account into its destination OU will trigger the state machine and result in the account being ingested into the Accelerator and having the guardrails applied per the target OU persona.</p> <p>For a detailed procedure, please review this document.</p>"},{"location":"faq/#13-end-user-environment","title":"1.3. End User Environment","text":"1.3.1. Is there anything my end users need to be aware of? Why do some of my end users struggle with CloudWatch Log groups errors? 1.3.2. How can I leverage Accelerator deployed objects in my IaC? Do I need to manually determine the arn's and object id's of Accelerator deployed objects to leverage them in my IaC? 1.3.3. How do I deploy AWS Elastic Beanstalk instances?"},{"location":"faq/#is-there-anything-my-end-users-need-to-be-aware-of-why-do-some-of-my-end-users-struggle-with-cloudwatch-log-groups-errors","title":"Is there anything my end users need to be aware of? Why do some of my end users struggle with CloudWatch Log groups errors?","text":"<p>CloudWatch Log group deletion is prevented for security purposes and bypassing this rule would be a fundamental violation of security best practices. This protection does NOT exist solely to protect ASEA logs, but ALL log groups. Users of the Accelerator environment will need to ensure they set CloudFormation stack Log group retention type to RETAIN, or stack deletes will fail when attempting to delete a stack (as deleting the log group will be blocked) and users will encounter errors. As repeated stack deployments will be prevented from recreating the same log group name (as it already exists), end users will either need to check for the existence of the log group before attempting creation, or include a random hash in the log group name. The Accelerator also sets log group retention for all log groups to value(s) specified by customers in the config file and prevents end users from setting or changing Log group retentions. When creating new log groups, end users must either not configure a retention period, or set it to the default <code>NEVER expire</code> or they will also be blocked from creating the CloudWatch Log group. If applied by bypassing the guardrails, customer specified retention periods on log group creation will be overridden with the Accelerator specified retention period.</p> <p>While a security best practice, some end users continue to request this be changed, but you need to ask: Are end users allowed to go in and clean out logs from Windows Event Viewer (locally or on domain controllers) after testing? Clean out Linux kernel logs? Apache log histories? The fundamental principal is that all and as many logs as possible will be retained for a defined retention period (some longer). In the \"old days\", logs were hidden deep within OS directory structures or access restricted by IT from developers - now that we make them all centralized, visible, and accessible, end users seem to think they suddenly need to clean them up. Customers need to establish a usable and scalable log group naming standard/convention as the first step in moving past this concern, such that they can always find their active logs easily. As stated, to enable repeated install and removal of stacks during test cycles, end user CloudFormation stacks need to set log groups to RETAIN and leverage a random hash in log group naming (or check for existence, before creating).</p> <p>The Accelerator provided SCPs (guardrails/protections) are our recommendations, yet designed to be fully customizable, enabling any customer to carefully override these defaults to meet their individual requirements. If insistent, we'd suggest only bypassing the policy on the Sandbox OU, and only for log groups that start with a very specific prefix (not all log groups). When a customer wants to use the delete capability, they would need to name their log group with the designated prefix - i.e. opt-in to allow CloudWatch log group deletes.</p>"},{"location":"faq/#how-can-i-leverage-accelerator-deployed-objects-in-my-iac-do-i-need-to-manually-determine-the-arns-and-object-ids-of-accelerator-deployed-objects-to-leverage-them-in-my-iac","title":"How can I leverage Accelerator deployed objects in my IaC? Do I need to manually determine the arn's and object id's of Accelerator deployed objects to leverage them in my IaC?","text":"<p>Objects deployed by the Accelerator which customers may need to leverage in their own IaC have been populated in parameters in AWS parameter store for use by the IaC tooling of choice. The Accelerator ensures parameters are deployed consistently across accounts and OUs, such that a customer's code does not need to be updated when it is moved between accounts or promoted from Dev to Test to Prod.</p> <p>Objects of the following types and their associated values are stored in parameter store: VPC, subnet, security group, ELB (ALB/NLB w/DNS address), IAM policy, IAM role, KMS key, ACM cert, SNS topic, and the firewall replacement variables.</p> <p>Additionally, setting \"populate-all-elbs-in-param-store\": true for an account will populates all Accelerator wide ELB information into parameter store within that account. The sample PBMM configuration files set this value on the perimeter account, such that ELB information is available to configure centralized ingress capabilities.</p>"},{"location":"faq/#how-do-i-deploy-aws-elastic-beanstalk-instances","title":"How do I deploy AWS Elastic Beanstalk instances?","text":"<p>If your deployed environment contains an SCP enforcing volume encryption of EC2 instances, your Elastic Beanstalk deployment will fail.</p> <p>The SCP will contain an entry like this:</p> <p><pre><code>{\n\"Sid\": \"EBS1\",\n\"Effect\": \"Deny\",\n\"Action\": \"ec2:RunInstances\",\n\"Resource\": \"arn:aws:ec2:*:*:volume/*\",\n\"Condition\": {\n\"Bool\": {\n\"ec2:Encrypted\": \"false\"\n}\n}\n},\n</code></pre> A solution is to encrypt the root volume of the AMI that Elastic Beanstalk uses for your selected platform, and perform a custom AMI deployment of your Elastic Beanstalk application.</p> <p>You can gather the AMI that Elastic Beanstalk uses via CLI with the following command:</p> <pre><code>aws elasticbeanstalk describe-platform-version --region &lt;YOUR_REGION&gt; --platform-arn &lt;ARN_EB_PLATFORM&gt;\n</code></pre> <p>Once you have gathered the AMI ID successfully, go to the EC2 console and:</p> <ul> <li>Click on the \u2018AMIs\u2019 option in the left navigation pane </li> <li>Search for your AMI after selecting \u2018Public Images\u2019 from the dropdown list. </li> <li>Select the AMI </li> <li>Go to Actions and Copy AMI</li> <li>Click on the checkbox to enable \u2018Encryption\u2019 and then select \"Copy AMI\".</li> </ul> <p>Once the AMI is successfully copied, you can use this AMI to specify a custom AMI in your Elastic Beanstalk environments with root volume encrypted.</p>"},{"location":"faq/#14-upgrades","title":"1.4. Upgrades","text":"1.4.1. Can I upgrade directly to the latest release, or must I perform upgrades sequentially? 1.4.2. Why do I get the error \"There were errors while comparing the configuration changes:\" when I update the config file?"},{"location":"faq/#can-i-upgrade-directly-to-the-latest-release-or-must-i-perform-upgrades-sequentially","title":"Can I upgrade directly to the latest release, or must I perform upgrades sequentially?","text":"<p>Yes, currently customers can upgrade from whatever version they have deployed to the latest Accelerator version. There is no requirement to perform sequential upgrades. In fact, we strongly discourage sequential upgrades.</p> <p>Given the magnitude of the v1.5.0 release, we have added a one-time requirement that all customers upgrade to a minimum of v1.3.8 before attempting to upgrade to v1.5.0.</p>"},{"location":"faq/#why-do-i-get-the-error-there-were-errors-while-comparing-the-configuration-changes-when-i-update-the-config-file","title":"Why do I get the error \"There were errors while comparing the configuration changes:\" when I update the config file?","text":"<p>In v1.3.0 we added protections to allow customers to verify the scope of impact of their intended changes to the configuration file. In v1.3.0 and above, the state machine does not allow changes to the config file (other than new accounts) without providing the <code>scope</code> parameter. Please refer to the State Machine behavior and inputs Guide for more details.</p>"},{"location":"faq/#15-support-concerns","title":"1.5. Support Concerns","text":"1.5.1. The Accelerator is written in CDK and deploys CloudFormation, does this restrict the Infrastructure as Code (IaC) tools that I can use? 1.5.2. What happens if AWS stops enhancing the Accelerator? 1.5.3. What level of Support will the ASEA have from AWS Support? 1.5.4. What does it take to support the Accelerator? 1.5.5. Is the Accelerator only designed and suitable for Government of Canada or PBMM customers?"},{"location":"faq/#the-accelerator-is-written-in-cdk-and-deploys-cloudformation-does-this-restrict-the-infrastructure-as-code-iac-tools-that-i-can-use","title":"The Accelerator is written in CDK and deploys CloudFormation, does this restrict the Infrastructure as Code (IaC) tools that I can use?","text":"<p>No. Customers can choose the IaC framework or tooling of their choice. The tooling used to deploy the Accelerator has no impact on the automation framework customers use to deploy their applications within the Accelerator environment. It should be noted that the functionality deployed by the Accelerator is extremely platform specific and would not benefit from multi-platform IaC frameworks or tooling.</p>"},{"location":"faq/#what-happens-if-aws-stops-enhancing-the-accelerator","title":"What happens if AWS stops enhancing the Accelerator?","text":"<p>The Accelerator is an open source project, should AWS stop enhancing the solution for any reason, the community has access to the full codebase, its roadmap and history. The community can enhance, update, fork and take ownership of the project, as appropriate.</p> <p>The Accelerator is an AWS CDK based project and synthesizes to native AWS CloudFormation. AWS sub-accounts simply contain native CloudFormation stacks and associated custom resources, when required. The Accelerator architecture is such that all CloudFormation stacks are native to each AWS account with no links or ties to code in other AWS accounts or even other stacks within the same AWS account. This was an important initial design decision.</p> <p>The Accelerator codebase can be completely uninstalled from the organization management (root) account, without any impact to the deployed functionality or guardrails. In this situation, guardrail updates and new account provisioning reverts to a manual process. Should a customer decide they no longer wish to utilize the solution, they can remove the Accelerator codebase without any impact to deployed resources and go back to doing things natively in AWS as they did before they deployed the Accelerator. By adopting the Accelerator, customers are not locking themselves in or making a one-way door decision.</p>"},{"location":"faq/#what-level-of-support-will-the-asea-have-from-aws-support","title":"What level of Support will the ASEA have from AWS Support?","text":"<p>The majority of the solution leverages native AWS services which are fully supported by AWS Support. Additionally, the Accelerator is an AWS CDK based project and synthesizes to native AWS CloudFormation. AWS sub-accounts simply contain native CloudFormation stacks and associated custom resources (when required). The Accelerator architecture is such that all CloudFormation stacks are native to each AWS account with no direct links or ties to code in other AWS accounts (no stacksets, no local CDK). This was an important project design decision, keeping deployed functionality in independent local CloudFormation stacks and decoupled from solution code, which allows AWS support to effectively troubleshoot and diagnose issues local to the sub-account.</p> <p>As the Accelerator also includes code, anything specifically related to the Accelerator codebase will be only supported on a \"best effort\" basis by AWS support, as AWS support does not support custom code. The first line of support for the codebase is typically your local AWS team (your SA, TAM, ProServe and/or AWS Partner). As an open source project, customers can file requests using GitHub Issues against the Accelerator repository or open a discussion in GitHub discussions. Most customer issues arise during installation and are related to configuration customization or during the upgrade process.</p>"},{"location":"faq/#what-does-it-take-to-support-the-accelerator","title":"What does it take to support the Accelerator?","text":"<p>We advise customers to allocate a 1/2 day per quarter to upgrade to the latest Accelerator release.</p> <p>Customers have indicated that deploying the Accelerator reduces their ongoing operational burden over operating in native AWS, saving hours of effort every time a new account is provisioned by automating the deployment of the persona associated with new accounts (guardrails, networking and security). The Accelerator does NOT alleviate a customer's requirement to learn to effectively operate in the cloud (like monitoring security tooling/carrying out Security Operation Center (SOC) duties). This effort exists regardless of the existence of the Accelerator.</p>"},{"location":"faq/#is-the-accelerator-only-designed-and-suitable-for-government-of-canada-or-pbmm-customers","title":"Is the Accelerator only designed and suitable for Government of Canada or PBMM customers?","text":"<p>No. The Accelerator is targeted at any AWS customer that is looking to automate the deployment and management of a comprehensive end-to-end multi-account environment in AWS. It is ideally suited for customers interested in achieving a high security posture in AWS.</p> <p>The Accelerator is a sophisticated deployment framework that allows for the deployment and management of virtually any AWS multi-account \"Landing Zone\" architecture without any code modifications. The Accelerator is actually delivering two separate and distinct products which can each be used on their own:</p> <ol> <li>the Accelerator the tool, which can deploy virtually any architecture based on a provided config file (no code changes), and;</li> <li>the Government of Canada (GC) prescriptive PBMM architecture which is delivered as a sample configuration file and documentation.</li> </ol> <p>The tooling was purposely built to be extremely flexible, as we realized that some customers may not like some of the opinionated and prescriptive design decisions we made in the GC architecture. Virtually every feature being deployed can be turned on/off, not be used or can have its configuration adjusted to meet your specific design requirements.</p> <p>We are working on building a library of sample config files to support additional customer needs and better demonstrate product capabilities and different architecture patterns. In no way is it required that the prescriptive GC architecture be used or deployed. Just because we can deploy, for example, an AWS Managed Active Directory, does not mean you need to use that feature of the solution. Disabling or changing these capabilities also requires zero code changes.</p> <p>While the prescriptive sample configuration files were originally developed based on GC requirements, they were also developed following AWS Best Practices. Additionally, many security frameworks around the world have similar and overlapping security requirements (you can only do security so many ways). The provided architecture is applicable to many security compliance regimes around the world and not just the GC.</p>"},{"location":"faq/#16-deployed-functionality","title":"1.6. Deployed Functionality","text":"1.6.1. I wish to be in compliance with the 12 GC TBS Guardrails, what don't you cover with the provided sample architecture? 1.6.2. Does the ALB perform SSL offloading? 1.6.3. What is the recommended approach to manage the ALB certificates deployed by the Accelerator? 1.6.4. Why do we have rsyslog servers? I thought everything was sent to CloudWatch? 1.6.5. Can you deploy the solution without Fortinet Firewall Licenses? 1.6.6. I installed additional software on my Accelerator deployed RDGW / rsyslog host, where did it go? 1.6.7. Some sample configurations provide NACLs and Security Groups. Is that enough? 1.6.8. Can I deploy the solution as the account root user? 1.6.9. Is the Organizational Management root account monitored similarly to the other accounts in the organization? 1.6.10. How are the perimeter firewall configurations and licensing managed after deployment? 1.6.11. Can the Fortinet Firewall deployments use static private IP address assignments? 1.6.12. I've noticed CloudTrail logs and in certain situation VPC flow logs are stored in the centralized log-archive account logging bucket twice? 1.6.13. I need a Route53 Private Hosted Zone in my workload account. How shall I proceed? 1.6.14. How do I create a role which has read access to the log-archive bucket to enabling log forwarding to my favorite SIEM solution? 1.6.15. How do I create a role for use by Azure Sentinel using the new S3 Connector method? 1.6.16. Does the ASEA include a full SIEM solution? 1.6.17. Why are only select interface endpoints provisioned in the sample configuration files? <p>For economic reasons, most of the sample configuration files only include the following minimum set of required interface endpoints:</p> <p>\"ec2\", \"ec2messages\", \"ssm\", \"ssmmessages\", \"secretsmanager\", \"cloudformation\", \"kms\", \"logs\", \"monitoring\"</p> <p>The full sample configuration file included all interface endpoints that existed in the Canada (Central) region at the time the configuration file was originally developed:</p> <p>\"access-analyzer\", \"acm-pca\", \"application-autoscaling\", \"appmesh-envoy-management\", \"athena\", \"autoscaling\", \"autoscaling-plans\", \"awsconnector\", \"cassandra\", \"clouddirectory\", \"cloudformation\", \"cloudtrail\", \"codebuild\", \"codecommit\", \"codepipeline\", \"config\", \"datasync\", \"ebs\", \"ec2\", \"ec2messages\", \"ecr.api\", \"ecr.dkr\", \"ecs\", \"ecs-agent\", \"ecs-telemetry\", \"elasticbeanstalk\", \"elasticbeanstalk-health\", \"elasticfilesystem\", \"elasticloadbalancing\", \"elasticmapreduce\", \"email-smtp\", \"events\", \"execute-api\", \"git-codecommit\", \"glue\", \"kinesis-firehose\", \"kinesis-streams\", \"kms\", \"license-manager\", \"logs\", \"macie2\", \"monitoring\", \"notebook\", \"sagemaker.api\", \"sagemaker.runtime\", \"secretsmanager\", \"servicecatalog\", \"sms\", \"sns\", \"sqs\", \"ssm\", \"ssmmessages\", \"states\", \"storagegateway\", \"sts\", \"synthetics\", \"transfer\", \"transfer.server\", \"workspaces\"</p> <p>Since that time these additional endpoints have been launched in the ca-central-1 region and can be optionally added to customer configuration files to make them accessible from private address space: </p> <p>\"airflow.api\", \"airflow.env\", \"airflow.ops\", \"app-integrations\", \"appstream.api\", \"appstream.streaming\", \"auditmanager\", \"backup\", \"backup-gateway\", \"batch\", \"cloudhsmv2\", \"codedeploy\", \"codedeploy-commands-secure\", \"codestar-connections.api\", \"comprehend\", \"comprehendmedical\", \"databrew\", \"dms\", \"elasticache\", \"emr-containers\", \"finspace\", \"finspace-api\", \"fis\", \"fsx\", \"greengrass\", \"imagebuilder\", \"inspector2\", \"iot.data\", \"iot.fleethub.api\", \"iotsitewise.api\", \"iotsitewise.data\", \"kendra\", \"lakeformation\", \"lambda\", \"memory-db\", \"mgn\", \"models-v2-lex\", \"nimble\", \"panorama\", \"profile\", \"qldb.session\", \"rds\", \"rds-data\", \"redshift\", \"redshift-data\", \"rekognition\", \"runtime-v2-lex\", \"sagemaker.featurestore-runtime\", \"securityhub\", \"servicecatalog-appregistry\", \"ssm-contacts\", \"ssm-incidents\", \"sync-states\", \"textract\", \"transcribe\", \"transcribestreaming\", \"translate\", \"xray\"</p> <p>The aws.sagemaker.ca-central-1.studio interface endpoint was also launched, but cannot be auto-deployed by the Accelerator at this time as it does not utilize standardized naming and requires a code update to enable deployment.</p> <p>Additional endpoints may exist in other AWS regions.  Any endpoint can be added to any Accelerator configuration file, as long as it follows the standardized endpoint naming convention (e.g. com.amazonaws.{region}.{service}).</p>"},{"location":"faq/#i-wish-to-be-in-compliance-with-the-12-gc-tbs-guardrails-what-dont-you-cover-with-the-provided-sample-architecture","title":"I wish to be in compliance with the 12 GC TBS Guardrails, what don't you cover with the provided sample architecture?","text":"<p>The AWS SEA allows for a lot of flexibility in deployed architectures. If used, the provided PBMM sample architecture was designed to help deliver on the technical portion of all 12 of the GC guardrails, when automation was possible.</p> <p>What don't we cover? Assigning MFA to users is a manual process. Specifically, you need to procure Yubikeys for your root/break glass users, and enable a suitable form of MFA for all other users (i.e. virtual, email, other). The guardrails also include some organizational processes (i.e. break glass procedures, or signing an MOU with CCCS) which customers will need to work through independently.</p> <p>While AWS is providing the tools to help customer be compliant with the 12 PBMM guardrails (which were developed in collaboration with the GC) - it's up to each customers ITSec organization to assess and determine if the deployed controls actually meet their security requirements.</p> <p>Finally, while we started with a goal of delivering on the 12 guardrails, we believe we have extended well beyond those security controls, to further help customers move towards meeting the full PBMM technical control profile (official documentation is weak in this area at this time).</p>"},{"location":"faq/#does-the-alb-perform-ssl-offloading","title":"Does the ALB perform SSL offloading?","text":"<p>As configured - the perimeter ALB decrypts incoming traffic using its certificate and then re-encrypts it with the certificate for the back-end ALB. The front-end and back-end ALB's can use the same or different certs. If the Firewall needs to inspect the traffic, it also needs the backend certificate be manually installed.</p>"},{"location":"faq/#what-is-the-recommended-approach-to-manage-the-alb-certificates-deployed-by-the-accelerator","title":"What is the recommended approach to manage the ALB certificates deployed by the Accelerator?","text":"<p>The Accelerator installation process allows customers to provide their own certificates (either self-signed or generated by a CA), to enable quick and easy installation and allowing customers to test end-to-end traffic flows. After the initial installation, we recommend customers leverage AWS Certificate Manager (ACM) to easily provision, manage, and deploy public and private SSL/TLS certificates. ACM helps manage the challenges of maintaining certificates, including certificate rotation and renewal, so you don\u2019t have to worry about expiring certificates.</p> <p>The Accelerator provides 3 mechanisms to enable utilizing certificates with ALB's:</p> <ul> <li> <p>Method 1 - IMPORT a certificate into AWS Certificate Manager from a 3rd party product</p> <ul> <li>When using a certificate that does not have a certificate chain (usually this is the case with Self-Signed)</li> </ul> <pre><code>  \"certificates\": [\n{\n\"name\": \"My-Cert\",\n\"type\": \"import\",\n\"priv-key\": \"certs/example1-cert.key\",\n\"cert\": \"certs/example1-cert.crt\"\n}\n]\n</code></pre> <ul> <li>When using a certificate that has a certificate chain (usually this is the case when signed by a Certificate Authority with a CA Bundle)</li> </ul> <pre><code>  \"certificates\": [\n{\n\"name\": \"My-Cert\",\n\"type\": \"import\",\n\"priv-key\": \"certs/example1-cert.key\",\n\"cert\": \"certs/example1-cert.crt\",\n\"chain\": \"certs/example1-cert.chain\"\n}\n]\n</code></pre> <ul> <li>this mechanism allows a customer to generate certificates using their existing tools and processes and import 3rd party certificates into AWS Certificate Manager for use in AWS</li> <li>Self-Signed certificates should NOT be used for production (samples were provided simply to demonstrate functionality)</li> <li>both a <code>.key</code> and a <code>.crt</code> file must be supplied in the customers S3 input bucket</li> <li>\"cert\" must contain only the certificate and not the full chain</li> <li>\"chain\" is an optional attribute that contains the certificate chain. This is generally used when importing a CA signed certificate</li> <li>this will create a certificate in ACM and a secret in secrets manager named <code>accelerator/certificates/My-Cert</code> in the specified AWS account(s), which points to the newly imported certificates ARN</li> </ul> </li> <li> <p>Method 2 - REQUEST AWS Certificate Manager generate a certificate</p> <pre><code>\"certificates\": [\n{\n\"name\": \"My-Cert\",\n\"type\": \"request\",\n\"domain\": \"*.example.com\",\n\"validation\": \"DNS\",\n\"san\": [\"www.example.com\"]\n}\n]\n</code></pre> <ul> <li>this mechanism allows a customer to generate new public certificates directly in ACM</li> <li>both <code>DNS</code> and <code>EMAIL</code> validation mechanisms are supported (DNS recommended)</li> <li>this requires a Public DNS zone be properly configured to validate you are legally entitled to issue certificates for the domain</li> <li>this will also create a certificate in ACM and a secret in secrets manager named <code>accelerator/certificates/My-Cert</code> in the specified AWS account(s), which points to the newly imported certificates ARN</li> <li>this mechanism should NOT be used on new installs, skip certificate and ALB deployment during initial deployment (removing them from the config file) and simply add on a subsequent state machine execution</li> <li>Process:<ul> <li>you need a public DNS domain properly registered and configured to publicly resolve the domain(s) you will be generating certificates for (i.e. example.com)<ul> <li>domains can be purchased and configured in Amazon Route53 or through any 3rd party registrar and DNS service provider</li> </ul> </li> <li>in Accelerator phase 1, the cert is generated, but the stack does NOT complete deploying (i.e. it waits) until certificate validation is complete</li> <li>during deployment, go to the AWS account in question, open ACM and the newly requested certificate. Document the authorization CNAME record required to validate certificate generation</li> <li>add the CNAME record to the zone in bullet 1 (in Route53 or 3rd party DNS provider) (documented here)</li> <li>after a few minutes the certificate will validate and switch to <code>Issued</code> status</li> <li>Accelerator phase 1 will finish (as long as the certificate is validated before the Phase 1 credentials time-out after 60-minutes)</li> <li>the ALB will deploy in a later phase with the specified certificate</li> </ul> </li> </ul> </li> <li> <p>Method 3 - Manually generate a certificate in ACM</p> <ul> <li>this mechanism allows a customer to manually generate certificates directly in the ACM interface for use by the Accelerator</li> <li>this mechanism should NOT be used on new installs, skip certificate and ALB deployment during initial deployment (removing them from the config file) and simply add on a subsequent state machine execution</li> <li>Process:<ul> <li>go to the AWS account for which you plan to deploy an ALB and open ACM</li> <li>generate a certificate, documenting the certificates ARN</li> <li>open Secrets manager and generate a new secret of the format <code>accelerator/certificates/My-Cert</code> (of type <code>Plaintext</code> under <code>Other type of secrets</code>), where <code>My-Cert</code> is the unique name you will use to reference this certificate</li> </ul> </li> </ul> </li> <li> <p>In all three mechanisms a secret will exist in Secrets Manager named <code>accelerator/certificates/My-Cert</code> which contains the ARN of the certificate to be used.</p> </li> <li>In the Accelerator config file, find the definition of the ALB for that AWS account and specify <code>My-Cert</code> for the ALB <code>cert-name</code></li> </ul> <pre><code>\"alb\": [\n{\n\"cert-name\": \"My-Cert\"\n}\n]\n</code></pre> <ul> <li>The state machine will fail if you specify a certificate in any ALB which is not defined in Secrets Manager in the local account.</li> </ul> <p>We suggest the most effective mechanism for leveraging ACM is by adding CNAME authorization records to the relevant DNS domains using Method 2, but may not appropriate right for all customers.</p>"},{"location":"faq/#why-do-we-have-rsyslog-servers-i-thought-everything-was-sent-to-cloudwatch","title":"Why do we have rsyslog servers? I thought everything was sent to CloudWatch?","text":"<p>The rsyslog servers are included to accept logs for appliances and third party applications that do not natively support the CloudWatch Agent from any account within a customers Organization. These logs are then immediately forwarded to CloudWatch Logs within the account the rsyslog servers are deployed (Operations) and are also copied to the S3 immutable bucket in the log-archive account. Logs are only persisted on the rsyslog hosts for 24 hours. The rsyslog servers are required to centralize the 3rd party firewall logs (Fortinet Fortigate).</p>"},{"location":"faq/#can-you-deploy-the-solution-without-fortinet-firewall-licenses","title":"Can you deploy the solution without Fortinet Firewall Licenses?","text":"<p>Yes, if license files are not provided, the firewalls will come up configured and route traffic, but customers will have no mechanism to manage the firewalls/change the configuration until a valid license file is added. If invalid licence files are provided, the firewalls will fail to load the provided configuration, will not enable routing, will not bring up the VPN tunnels and will not be manageable. Customers will need to either remove and redeploy the firewalls, or manually configure them. If performing a test deployment, please work with your local Fortinet account team to discuss any options for temporary evaluation licenses.</p> <p>Additionally, several additional firewall options are now available, including using AWS Network Firewall, a native AWS service.</p>"},{"location":"faq/#i-installed-additional-software-on-my-accelerator-deployed-rdgw-rsyslog-host-where-did-it-go","title":"I installed additional software on my Accelerator deployed RDGW / rsyslog host, where did it go?","text":"<p>The RDGW and rsyslog hosts are members of auto-scaling groups. These auto-scaling groups have been configured to refresh instances in the pool on a regular basis (7-days in the current sample config files). This ensures these instances are always clean. Additionally, on every execution of the Accelerator state machine the ASG are updated to the latest AWS AMI for the instances. When the auto-scaling group refreshes its instances, they will be redeployed with the latest patch release of the AMI/OS. It is recommended that the state machine be executed monthly to ensure the latest AMI's are always in use.</p> <p>Customers wanting to install additional software on these instances should either a) update the automated deployment scripts to install the new software on new instance launch, or b) create and specify a custom AMI in the Accelerator configuration file which has the software pre-installed ensuring they are also managing patch compliance on the instance through some other mechanism.</p> <p>At any time, customers can terminate the RDGW or rsyslog hosts and they will automatically be re-created from the base images with the latest patch available at the time of the last Accelerator State Machine execution.</p>"},{"location":"faq/#some-sample-configurations-provide-nacls-and-security-groups-is-that-enough","title":"Some sample configurations provide NACLs and Security Groups. Is that enough?","text":"<p>Security group egress rules are often used in 'allow all' mode (<code>0.0.0.0/0</code>), with the focus primarily being on consistently allow listing required ingress traffic (centralized ingress/egress controls are in-place using the perimeter firewalls). This ensures day to day activities like patching, access to DNS, or to directory services access can function on instances without friction.</p> <p>The Accelerator provided sample security groups in the workload accounts offer a good balance that considers both security, ease of operations, and frictionless development. They allow developers to focus on developing, enabling them to simply use the pre-created security constructs for their workloads, and avoid the creation of wide-open security groups. Developers can equally choose to create more appropriate least-privilege security groups more suitable for their application, if they are skilled in this area. It is expected as an application is promoted through the SDLC cycle from Dev through Test to Prod, these security groups will be further refined by the extended customers teams to further reduce privilege, as appropriate. It is expected that each customer will review and tailor their Security Groups based on their own security requirements. The provided security groups ensures day to day activities like patching, access to DNS, or to directory services access can function on instances without friction, with the understanding further protections are providing by the central ingress/egress firewalls.</p> <p>The use of NACLs are general discouraged, but leveraged in this architecture as a defense-in-depth mechanism. Security groups should be used as the primary access control mechanism. As with security groups, we encourage customers to review and tailor their NACLs based on their own security requirements.</p>"},{"location":"faq/#can-i-deploy-the-solution-as-the-account-root-user","title":"Can I deploy the solution as the account root user?","text":"<p>No, you cannot install as the root user. The root user has no ability to assume roles which is a requirement to configure the sub-accounts and will prevent the deployment. As per the installation instructions, you require an IAM user with the <code>AdministratorAccess</code> policy attached.</p>"},{"location":"faq/#is-the-organizational-management-root-account-monitored-similarly-to-the-other-accounts-in-the-organization","title":"Is the Organizational Management root account monitored similarly to the other accounts in the organization?","text":"<p>Yes, all accounts including the Organization Management or root account have the same monitoring and logging services enabled. When supported, AWS security services like GuardDuty, Macie, and Security Hub have their delegated administrator account configured as the \"security\" account. These tools can be used within each local account (including the Organization Management account) within the organization to gain account level visibility or within the Security account for Organization wide visibility. For more information about monitoring and logging refer to architecture documentation.</p>"},{"location":"faq/#how-are-the-perimeter-firewall-configurations-and-licensing-managed-after-deployment","title":"How are the perimeter firewall configurations and licensing managed after deployment?","text":"<p>While you deploy the perimeter firewalls with the Accelerator you will continue to manage firewall updates, configuration changes, and license renewals from the respective firewall management interface and not from the Accelerator config file. As these changes are not managed by the Accelerator you do not need to rerun the state machine to implement or track any of these changes. You can update the AMI of the 3rd party firewalls using the Accelerator, you must first remove the existing firewalls and redeploy them (as the Elastic IP's (EIP's) will block a parallel deployment) or deploy a second parallel firewall cluster and de-provision the first cluster when ready.</p>"},{"location":"faq/#can-the-fortinet-firewall-deployments-use-static-private-ip-address-assignments","title":"Can the Fortinet Firewall deployments use static private IP address assignments?","text":"<p>Yes, the <code>\"port\"</code> stanza in the configuration file can support a private static IP address assignment from the AZ and subnet. Care must be exercised to assure the assigned IP address is within the correct subnet and availability zone. Consideration must also be given to the Amazon reserved IP addresses (first three addresses, and the last) within subnets when choosing an IP Address to assign.</p> <p>Using the <code>config.example.json</code> as a reference, static IP Assignments would look like this in the <code>ports:</code> stanza of the firewall deployment.</p> <pre><code>\"ports\": [\n{\n\"name\": \"Public\",\n\"subnet\": \"Public\",\n\"create-eip\": true,\n\"create-cgw\": true,\n\"private-ips\": [\n{\n\"az\": \"a\",\n\"ip\": \"100.96.250.4\"\n},\n{\n\"az\": \"b\",\n\"ip\": \"100.96.250.132\"\n}\n]\n},\n{\n\"name\": \"OnPremise\",\n\"subnet\": \"OnPremise\",\n\"create-eip\": false,\n\"create-cgw\": false,\n\"private-ips\": [\n{\n\"az\": \"a\",\n\"ip\": \"100.96.250.68\"\n},\n{\n\"az\": \"b\",\n\"ip\": \"100.96.250.196\"\n}\n]\n}\n...\n],\n</code></pre> <p>Where <code>private-ips</code> are not present for the subnet or availability zone an address will be assigned automatically from available addresses when the firewall instance is created.</p>"},{"location":"faq/#ive-noticed-cloudtrail-logs-and-in-certain-situation-vpc-flow-logs-are-stored-in-the-centralized-log-archive-account-logging-bucket-twice","title":"I've noticed CloudTrail logs and in certain situation VPC flow logs are stored in the centralized log-archive account logging bucket twice?","text":"<p>Yes. CloudTrail is configured to send its logs directly to S3 for centralized immutable log retention. CloudTrail is also configured to send it's logs to a centralized Organizational CloudWatch Log group such that the trail can be a) easily queried online using CloudWatch Insights across all AWS accounts in the organization, and b) to enable alerting based on undesirable API activity using CloudWatch Metrics and Alarms. All CloudWatch Log groups are also configured to be sent, using Amazon Kinesis, to S3 for centralized immutable log retention.</p> <p>VPC flow log destinations can be configured in the config file. The example config files are set to send the VPC flow logs to both S3 and CloudWatch Logs by default for the same reasons as CloudTrail.</p> <p>To reduce the duplicate long-term storage of these two specific CloudWatch Log types, customers can set <code>cwl-glbl-exclusions</code> under <code>central-log-services</code> to: <code>[\"/${ACCELERATOR_PREFIX_ND}/flowlogs/*\", \"/${ACCELERATOR_PREFIX_ND}/CloudTrail*\"]</code> to prevent these specifically named log groups from being stored on S3. This setting also prevents the Accelerator from setting the customer desired log group retention period defined in the config file, once implemented, for those log groups. Therefore, we do not recommend this exception be applied during the initial installation, as the retention setting on these CWL groups will remain the default (infinite). If <code>cwl-glbl-exclusions</code> is set after initial install, the defined retention will be configured during install and will remain set to the value present when the exception was applied to those log groups. This allows logs to be stored in CloudWatch Logs for quick and easy online access (short-retention only), and stored in S3 for long-term retention and access.</p> <p>Side note: CloudTrail S3 data plane logs are enabled at the Organizational level, meaning all S3 bucket access is logged. As CloudTrail is writing to a bucket within the Organization, CloudTrail itself is accessing the bucket, seemingly creating a cyclical loop. As CloudTrail writes to S3 in 5-10min batches, CloudTrail will actually only cause one extra log 'entry' every 5-10minutes and not per S3 event, mitigating major concerns. Today, with an Organization trail logging data plane events for all buckets - there is no way to exclude any one bucket. But - having clear view of who accessed/changed logs, including AWS services, is important.</p>"},{"location":"faq/#i-need-a-route53-private-hosted-zone-in-my-workload-account-how-shall-i-proceed","title":"I need a Route53 Private Hosted Zone in my workload account. How shall I proceed?","text":"<p>The workload account requires creating a temporary local VPC before creating the Private Hosted Zone (PHZ). Creating a PHZ in Route53 requires association with a VPC. You cannot specify a shared VPC when creating the PHZ, hence the need for this workaround.</p> <p>Create the temporary workload account VPC</p> <p>You can create the temporary VPC during AWS account creation via the ASEA config (preferred way). Insert the \"vpc\" JSON object like shown below when using the ASEA config to create an AWS account.</p> <p>If you don't use the ASEA config you will need to assume the proper ASEA elevated IAM role in the workload account in order to create the VPC manually.</p> <pre><code>\"mydevacct\": {\n\"account-name\": \"MyDev1\",\n\"email\": \"dev1-main@super-corp.co\",\n\"src-filename\": \"config.json\",\n\"ou\": \"dev\",\n\"vpc\": [\n{\n\"deploy\": \"local\",\n\"name\": \"Local\",\n\"description\": \"This VPC Temp VPC to create the local hosted zone.\",\n\"cidr-src\": \"provided\",\n\"cidr\": [\n{\n\"value\": \"192.168.100.0/24\"\n}\n],\n\"region\": \"${HOME_REGION}\"\n}\n]\n}\n</code></pre> <p>Create in the workload account a Private Hosted Zone</p> <p>Using an IAM role assumed in the workload account:</p> <p>List the VPCs.</p> <pre><code> aws ec2 describe-vpcs\n</code></pre> <p>Then retrieve the VpcId attribute for the newly created VPC as well as the Id for the shared VPC.</p> <p>Create the Private Hosted Zone</p> <pre><code>aws route53 create-hosted-zone --name &lt;MY_DOMAIN&gt; --hosted-zone-config PrivateZone=true --vpc VPCRegion=&lt;VPC_REGION&gt;,VPCId=&lt;VPC_ID&gt; --caller-reference &lt;YOUR_REFERENCE_ID&gt;\n</code></pre> <p>Insert the proper values for:</p> <ul> <li><code>&lt;MY_DOMAIN&gt;</code></li> <li><code>&lt;VPC_REGION&gt;</code></li> <li><code>&lt;VPC_ID&gt;</code> (id of new the local VPC)</li> <li><code>&lt;YOUR_REFERENCE_ID&gt;</code> (can be any value)</li> </ul> <p>Take note of the newly created hosted zone id by looking at the output of the command. The Id is the value after <code>/hostedzone/</code> from the Id attribute. For example, the value is <code>Z0123456NWOWQ4HNN40U</code> from <code>\"Id\": \"/hostedzone/Z0123456NWOWQ4HNN40U\"</code>.</p> <p>Create an authorization to associate with this new zone</p> <p>While still in the workload account; you need to create an association request authorization to allow the shared VPC to associate with this newly created Route53 PHZ.</p> <pre><code>aws route53 create-vpc-association-authorization --hosted-zone-id &lt;ZONE_ID&gt; --vpc VPCRegion=&lt;SHARED_VPC_REGION&gt;,VPCId=&lt;SHARED_VPC_ID&gt;\n</code></pre> <p>Insert the proper values for:</p> <ul> <li><code>&lt;ZONE_ID&gt;</code></li> <li><code>&lt;SHARED_VPC_REGiON&gt;</code></li> <li><code>&lt;SHARED_VPC_ID&gt;</code></li> </ul> <p>Confirm the association request for the shared VPC</p> <p>After switching to an IAM role in the SharedNetwork account associate the Private Hosted Zone from the workload account.</p> <pre><code>aws route53 associate-vpc-with-hosted-zone --hosted-zone-id &lt;ZONE_ID&gt; --vpc VPCRegion=&lt;SHARED_VPC_REGION&gt;,VPCId=&lt;SHARED_VPC_ID&gt;\n</code></pre> <p>Insert the proper values for:</p> <ul> <li><code>&lt;ZONE_ID&gt;</code></li> <li><code>&lt;SHARED_VPC_REGiON&gt;</code></li> <li><code>&lt;SHARED_VPC_ID&gt;</code></li> </ul> <p>Validate Association and clean-up</p> <p>Back in the workload account and assuming its IAM role, validate the association using the below command. You should see two VPCs attached. The local VPC and the shared VPC.</p> <pre><code>aws route53 get-hosted-zone --id &lt;ZONE_ID&gt;\n</code></pre> <p>Insert the proper values for:</p> <ul> <li><code>&lt;ZONE_ID&gt;</code></li> </ul> <p>You can now dissociate the local VPC from the zone.</p> <pre><code>aws route53 disassociate-vpc-from-hosted-zone --hosted-zone-id &lt;ZONE_ID&gt; --vpc VPCRegion=&lt;VPC_REGION&gt;,VPCId=&lt;VPC_ID&gt;\n</code></pre> <p>Insert the proper values for:</p> <ul> <li><code>&lt;ZONE_ID&gt;</code></li> <li><code>&lt;VPC_REGiON&gt;</code></li> <li><code>&lt;VPC_ID&gt;</code></li> </ul> <p>You can now delete the local VPC. We recommend you leverage the ASEA configuration file. Simply the remove the <code>vpc</code> section from the workload account:</p> <pre><code>\"mydevacct\": {\n\"account-name\": \"MyDev1\",\n\"email\": \"dev1-main@super-corp.co\",\n\"src-filename\": \"config.json\",\n\"ou\": \"dev\"\n}\n</code></pre> <p>and rerun the State Machine.</p>"},{"location":"faq/#how-do-i-create-a-role-which-has-read-access-to-the-log-archive-bucket-to-enabling-log-forwarding-to-my-favorite-siem-solution","title":"How do I create a role which has read access to the log-archive bucket to enabling log forwarding to my favorite SIEM solution?","text":"<p>You can update the ASEA config file to provision an IAM role that has cross-account access to the Log Archive S3 Buckets. Attempting to do this outside the ASEA config file is blocked by security guardrails. Additionally, even if the guardrails are bypassed, it is likely the ASEA will revert any manual changes on subsequent State Machine executions. The below example creates a Lambda role which is provided permissions to Amazon OpenSearch, S3 Read Only, Lambda VPC Execution, the Log Archive S3 buckets and the KMS key. Update the below example with the least-privilege policies needed to meet the requirements of your chosen SIEM solution.</p> <p>The primary trick, is the use of the <code>\"ssm-log-archive-read-only-access\": true</code> flag.</p> <p>As we generally recommend the SIEM be deployed into the Operations account, add the following to the roles array within the Operations account section in the ASEA config file:</p> <pre><code>{\n\"role\": \"SIEM-Lambda-Processor\",\n\"type\": \"lambda\",\n\"ssm-log-archive-read-only-access\": true,\n\"policies\": [\n\"AmazonOpenSearchServiceFullAccess\",\n\"service-role/AWSLambdaVPCAccessExecutionRole\",\n\"AmazonS3ReadOnlyAccess\"\n],\n\"boundary-policy\": \"Default-Boundary-Policy\"\n}\n</code></pre>"},{"location":"faq/#how-do-i-create-a-role-for-use-by-azure-sentinel-using-the-new-s3-connector-method","title":"How do I create a role for use by Azure Sentinel using the new S3 Connector method?","text":"<p>This process is very similar to FAQ #1.6.14, except we need to allow for a cross-cloud role assumption. This will be done in the Log Archive account, instead of the Operations account.</p> <p>The following config snippet should be added to the roles array within the Log Archive account section in the ASEA config file:</p> <pre><code>          {\n\"role\": \"MicrosoftSentinelRole\",\n\"type\": \"account\",\n\"ssm-log-archive-read-only-access\": true,\n\"policies\": [\n\"AmazonSQSReadOnlyAccess\",\n\"service-role/AWSLambdaSQSQueueExecutionRole\",\n\"AmazonS3ReadOnlyAccess\"\n],\n\"boundary-policy\": \"Default-Boundary-Policy\",\n\"trust-policy\": \"sentinel-trust-policy.json\",\n\"source-account\": \"log-archive\",\n\"source-account-role\": \"OrganizationAccountAccessRole\"\n}\n</code></pre> <ul> <li>The value of the <code>source-account-role</code> above needs to be replaced with the value of <code>organization-admin-role</code> from your config file (OrganizationAccountAccessRole, AWSCloudFormationStackSetExecutionRole, or AWSControlTowerExecution).</li> </ul> <p>The above role uses a custom trust policy, and also requires a file of the name <code>sentinel-trust-policy.json</code> be placed into the <code>iam-policy</code> folder of the customers S3 input bucket. This file must contain the following text:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"arn:aws:iam::197857026523:root\"\n},\n\"Action\": \"sts:AssumeRole\",\n\"Condition\": {\n\"StringEquals\": {\n\"sts:ExternalId\": \"{CUSTOMER-VALUE-HERE}\"\n}\n}\n}\n]\n}\n</code></pre> <ul> <li>The IAM account number listed above is a value provided by Microsoft in their documentation (hard-coded to the same value for all customers).</li> <li>The value of <code>sts:ExternalId</code>, shown as <code>{CUSTOMER-VALUE-HERE}</code> above, must be replaced with the ID of the Log Analytics Workspace in your Azure tenant.</li> <li>This information is based on the requirements published here as of 2022-03-10.</li> </ul>"},{"location":"faq/#does-the-asea-include-a-full-siem-solution","title":"Does the ASEA include a full SIEM solution?","text":"<p>We've found a diverse set of differing customer needs and requirements across our customer base. The ASEA:</p> <ul> <li>enables AWS security services like Amazon GuardDuty (a Cloud native IDS solution) and centralizes the consoles of these tools in the Security account;</li> <li>audits the entire environment for compliance and consolidates findings from AWS security services in the Security Hub console in the Security account;</li> <li>sends prioritized email alerts for Security Hub Findings, Firewall Manager alerts and customizable CloudWatch Alarms;</li> <li>centralizes logs across the environment in a central bucket in the Log Archive account;</li> <li>in addition, retains logs locally in CloudWatch Logs for simple query using CloudWatch Insights.</li> </ul> <p>This makes it extremely simple to layer a customer's preferred SIEM solution on top of the ASEA, enabling easy consumption of the comprehensive set of collected logs and security findings.</p> <p>Customers ask for examples of what this integration looks like. We've also had a number of customers ask for a reasonably functional and comprehensive open source SIEM-like solution to provide more advanced dashboarding, log correlation and search capabilities.</p> <p>While not a part of the ASEA, we've made the SIEM on Amazon OpenSearch Service available as an ASEA Add-on to satisfy these requirements.</p> <p>This independent solution can easily and quickly be deployed on top of the ASEA by following the documentation and using the scripts available here. This process takes less than an hour.</p> <p>The overall logging architecture is represented in this diagram:</p> <p></p>"},{"location":"faq/#why-are-only-select-interface-endpoints-provisioned-in-the-sample-configuration-files","title":"Why are only select interface endpoints provisioned in the sample configuration files?","text":""},{"location":"faq/#17-network-architecture","title":"1.7. Network Architecture","text":"1.7.1. We want to securely connect our on-premises networks/datacenters to our AWS Cloud PBMM tenancy, what does AWS you recommend? 1.7.2. Does this configuration violate PBMM / ITSG-22/38/33 principals? 1.7.3. Why do you NOT recommend using a VGW on the perimeter VPC? 1.7.4. Why do you NOT recommend connecting directly to the 3rd party firewall cluster in the perimeter account? (not GWLB, not NFW) 1.7.5. What if I really want to inspect this traffic inside AWS, but like the TGW architecture? 1.7.6. What does the traffic flow look like for an application running in a workload account? 1.7.7. How does CloudFront and API Gateway fit with the answer from question 1.7.6?"},{"location":"faq/#we-want-to-securely-connect-our-on-premises-networksdatacenters-to-our-aws-cloud-pbmm-tenancy-what-does-aws-you-recommend","title":"We want to securely connect our on-premises networks/datacenters to our AWS Cloud PBMM tenancy, what does AWS you recommend?","text":"<p>We recommend customers create a new AWS sub-account in your organization in the Infrastructure OU to \u201cown\u201d the Direct Connect (DX), segregating Direct Connect management and billing from other organization activities. Once provisioned you would create a Public VIF on the DX in this account. You can also create additional Private VIF\u2019s when and if required, and share them directly with any sub-account that needs to consume them.</p> <p>We recommend customers then inter-connect directly to the Transit Gateway, in the Shared Network sub-account, from your on-premises network/datacenters.</p> <ul> <li>Initiate IPSec VPN tunnels from on-premises to the TGW using BGP w/ECMP to scale and balance the traffic. Equal Cost Multi-Pathing (ECMP) is used to balance the traffic across the available VPN tunnels.</li> <li>You need to create as many VPN attachments to the TGW as is required to meet your bandwidth requirements or DX capacity. Today IPSec attachments are limited to 1.25 Gbps each (10 Gbps would require 8 attachments) and is scalable to 50 Gbps.</li> <li>Each VPN attachment would comprise two tunnels (active/passive), each connecting to a different on-premises firewall/VPN appliance.</li> </ul> <p>The VPN attachments would then be connected to an appropriately configured route table on the TGW. TGW route tables provide VRF like segregation capabilities, allowing customers to control which of their cloud based networks are allowed to communicate on-premises, or visa-versa.</p> <p>This architecture is fully managed and easy to manage, highly available, scalable, cost effective, and enables customers to reserve all their 3rd party Perimeter firewall capacity for public or internet facing traffic.</p> <p>(This guidance will be updated once MACSEC is broadly available across AWS transit centers)</p>"},{"location":"faq/#does-this-configuration-violate-pbmm-itsg-223833-principals","title":"Does this configuration violate PBMM / ITSG-22/38/33 principals?","text":"<p>No. Data center interconnects are not zoning boundaries (or ZIPs). Additionally, in many cases the on-premises VPN termination device used to interconnect to the cloud either contains, or is placed in-line with firewall and/or inspection devices. Customers insistent on placing a firewall between datacenters can enable the appropriate filtering or inspection on these on-premise devices. Enabling the same capabilities inside AWS would mean a customer is inspecting both ends of the same wire, a pointless activity. The TGW approach is being used by several gov\u2019t PBMM customers.</p> <p>Additionally, it should be noted that workloads in all the AWS accounts are fully protected using AWS Security Groups (stateful firewalls) wrapped around each and every instance comprising a workload.</p>"},{"location":"faq/#why-do-you-not-recommend-using-a-vgw-on-the-perimeter-vpc","title":"Why do you NOT recommend using a VGW on the perimeter VPC?","text":"<p>The VGW solution was not designed to support an enterprise cloud environment \u2013 it was designed to provide single VPC connectivity. The VGW solution offers lower availability than other options as it relies on VPC route tables to steer traffic, which need to be updated using custom scripts in the event the failure of an appliance or availability zone. The VGW solution is typically harder to maintain and troubleshoot. The VGW solution has limited scalability, as the VGW only supports a single active connection and does not support BGP or ECMP (i.e. supports a maximum bandwidth of 1.25Gbps). Most customers providing enterprise cloud connectivity have switch away from this approach. This approach is highly discouraged.</p>"},{"location":"faq/#why-do-you-not-recommend-connecting-directly-to-the-3rd-party-firewall-cluster-in-the-perimeter-account-not-gwlb-not-nfw","title":"Why do you NOT recommend connecting directly to the 3rd party firewall cluster in the perimeter account? (not GWLB, not NFW)","text":"<p>This approach was common with AWS customers before the TGW was introduced, with many customers upgrading or considering upgrading to the TGW approach. We also have some customers using this architecture based on a very specific limitation of the customer\u2019s Direct Connect architecture, these customers would also like to migrate to the TGW approach, if they could.</p> <p>While viable, this approach adds unneeded complexity, reduces cloud availability, is expensive to scale, and reduces bandwidth to internet facing workloads. This solution doubles the IPSec VPN tunnels using BGP w/ECMP requirements as it needs tunnels on both sides of the firewall. In this configuration each firewall appliance typically only provides a single pair of IPSec connections supporting marginally more bandwidth than the TGW VPN attachments. Adding tunnels and bandwidth requires adding firewall appliances. Stateful capabilities typically need to be disabled due to performance and asymmetric routing challenges. This typically means a very expensive device is being deployed inside AWS simply to terminate a VPN tunnel.</p>"},{"location":"faq/#what-if-i-really-want-to-inspect-this-traffic-inside-aws-but-like-the-tgw-architecture","title":"What if I really want to inspect this traffic inside AWS, but like the TGW architecture?","text":"<p>Customers who insist on inspecting the ground to cloud traffic inside AWS can do this with the proposed TGW architecture. The TGW route tables can be adjusted to hairpin the traffic through either a dedicated Inspection VPC, or to the Perimeter account firewall cluster for inspection. The Inspection VPC option could leverage 3rd party firewalls in an autoscaling group behind a Gateway Load Balancer, or leverage AWS network firewall to inspection traffic. To maximize internet throughput, the Inspection VPC option is generally recommended. While we do not feel inspection is needed in this situation, it is possible.</p>"},{"location":"faq/#what-does-the-traffic-flow-look-like-for-an-application-running-in-a-workload-account","title":"What does the traffic flow look like for an application running in a workload account?","text":"<p>The perimeter (ingress/egress) account typically contains two ALB's, one for production workloads and another for Dev/Test workloads. The Dev/Test ALB should be locked to restrict access to on-premises users (using a security group) or have authentication enabled to prevent Dev/Test workloads from being exposed to the internet. Additionally, each workload account (Dev/Test/Prod) contains a local (back-end) ALB.</p> <p>AWS Web Application Firewall (WAF) should be enabled on both front-end and back-end ALB's. The Front-end WAF would contain rate limiting, scaling and generic rules. The back-end WAF would contain workload specific rules (i.e. SQL injection). As WAF is essentially a temporary fix for broken applications before a developer can fix the issue, these rules typically require the close involvement of the application team. Rules can be centrally managed across all WAF instances using AWS Firewall Manager from the Security account.</p> <p>The front-end ALB is then configured to target the back-end ALB using the process described in the Post Installation section of the installation guide, step 2 <code>(Configure the new alb-forwarding feature (added in v1.5.0)</code>. This enables configuring different DNS names and/or paths to different back-end ALB's using the ASEA's alb-forwarder. We recommend moving away from the NAT to DNS mechanism used in previous released as it was too complex, does not work with bump-in-the-wire inspection devices (NFW, GWLB), and only available on a limited number of 3rd party firewalls.</p> <p>This implementation allows workload owners to have complete control of workloads in a local account including the ELB configuration, and allow site names and paths to be defined and setup at sub-account creation time (instead of during development) to enable publishing publicly or on-premises in a rapid agile manner.</p> <p>This overall flow is depicted in this diagram:</p> <p></p>"},{"location":"faq/#how-does-cloudfront-and-api-gateway-fit-with-the-answer-from-question-176","title":"How does CloudFront and API Gateway fit with the answer from question 1.7.6?","text":"<p>The perimeter account is focused on protecting legacy IaaS based workloads. Cloud Native applications including CloudFront and API Gateway should be provisioned directly in the same account as the workload and should NOT traverse the perimeter account.</p> <p>These services must still be appropriately configured. This includes ensuring both WAF and logging are enabled on each endpoint.</p> <p>The GC guidance on Cloud First patterns and anti-patterns can be downloaded here.</p>"},{"location":"guides/fortigate/index-NotUsed/","title":"ASEA - Fortinet Guides","text":"<p>Expose Public Facing Workload via Fortigate</p>"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/","title":"1. Public Facing Workload Configuration Sample","text":""},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#11-overview","title":"1.1. Overview","text":"<p>This page describes the steps needed to configure a public facing web application that is deployed within a workload AWS Account in the Secure Environment Accelerator (SEA).</p> <p>The high-level steps are the following:</p> <ol> <li>Create a SSL public certificate in AWS Certificate Manager.</li> <li>Create a DNS entry for the web application.</li> <li>Create Application Load Balancer Target Groups for the web application</li> <li>Create an Application Load Balancer Rule to forward traffic to the Firewalls.</li> <li>Configure the Firewalls.</li> </ol> <p>The screenshots and steps in this page are specific to the Fortigate Firewalls.</p>"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#12-perimeter-sea-aws-account","title":"1.2. Perimeter SEA AWS Account","text":""},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#121-ssl-certificate-configuration","title":"1.2.1. SSL Certificate Configuration","text":"<ol> <li> <p>Within the Perimeter SEA AWS Account, navigate to the Certificate Manager service.</p> </li> <li> <p>Follow the steps to request a new public certificate. This will be used to support https for the web application. Note that the SEA deploys 'example' certificates, but these should not be used at the perimeter. Here's an example showing a wildcard cert.</p> </li> </ol> <p></p> <ol> <li>Navigate to the ALBs and select the Load Balancer that will support the incoming requests for the web application. In this example, it will be the 'Public-DevTest-perimeter-alb'.</li> </ol> <p></p> <ol> <li>Select the 'Public-DevTest-perimeter-alb' ALB and click the View/edit certificates link button.</li> </ol> <p></p> <ol> <li> <p>Click the + button and select the new SSL Certificate. Click Add.</p> </li> <li> <p>Return back to the ALBs and select the 'Public-DevTest-perimeter-alb' ALB. Select the default HTTPS listener and click Edit</p> </li> </ol> <p></p> <ol> <li>Change the Default SSL certificate to the newly created public cert and update the settings.</li> </ol> <p></p>"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#122-alb-target-group-configuration","title":"1.2.2. ALB Target Group Configuration","text":"<ol> <li>Navigate to the EC2 Load Balancers and view the default Application Load Balancers (ALB).</li> </ol> <ol> <li>List the ALB Target Groups</li> </ol> <p>These are pairs of targets (one for each firewall) that direct traffic from the perimeter ALB to the firewall. The two pairs were created as part of the default configuration and provide health checks to the shared VPCs. For support a new web application, a new pair will be created. One for each firewall (i.e. one per AZ).</p> <ol> <li> <p>Click the Create target group button. (Note: This will be repeated for each Firewall).</p> </li> <li> <p>Enter the following parameter values:</p> <ul> <li>Target group name: Public-DevTest-SampleApp-azA</li> <li>Protocol: HTTPS</li> <li>Port: (pick an unused port on the Firewall). Example 7006</li> <li>VPC: Perimeter_VPC</li> </ul> </li> </ol> <p></p> <ol> <li>When Registering a target, pick the instance that aligns with the Availability Zone (AZ) that is being configured. Example: Firewall*az[A|B]. If creating 'Public-DevTest-SampleApp-azA', then choose Firewall instance 'Firewall*azA'.</li> </ol> <p></p> <ol> <li>Ensure that the port value is using the previous entered port value. Click the Include as pending below.</li> </ol> <p></p> <ol> <li> <p>Click the Create target group button when ready.</p> </li> <li> <p>Repeat for the additional firewalls.</p> </li> </ol>"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#123-alb-listener-rule-configuration","title":"1.2.3. ALB Listener Rule Configuration","text":"<ol> <li> <p>Create a DNS entry for the web application that resolves to the perimeter ALB being configured. For example: webapplication.mydomain.ca resolves to 'Public-DevTest-perimeter-alb-1616856287.ca-central-1.elb.amazonaws.com'</p> </li> <li> <p>Navigate to the ALBs and select the 'Public-DevTest-perimeter-alb' ALB. Click the View/edit rules link button.</p> </li> </ol> <p></p> <ol> <li>Click the + button to create a new rule. Then click the + Insert Rule button.</li> </ol> <p></p> <ol> <li>Configure a match condition on Host header.... enter the value of the DNS entry for the web application.</li> </ol> <p></p> <p></p> <ol> <li> <p>Click the checkmark to update it.</p> </li> <li> <p>Click the + Add action and select Forward to...</p> </li> </ol> <p></p> <ol> <li>Configure both Targets using the ones previously created (one per firewall). Adjust for 50% load balanced traffic.</li> </ol> <p></p> <p></p> <ol> <li>Click the checkmark to update and then click the Save button.</li> </ol>"},{"location":"guides/fortigate/public-facing-workload-via-fortigate/#13-fortigate-firewall-configuration","title":"1.3. Fortigate Firewall Configuration","text":"<p>The following configuration will be executed per Firewall instance (twice with the default SEA configuration).</p> <ol> <li>Log in to the firewall instance.</li> <li>Switch the Virtual Domain (vdom) to FG-traffic.</li> </ol> <p></p> <ol> <li>Navigate to Policy &amp; Objects and select Addresses</li> </ol> <p></p> <ol> <li> <p>Create a new entry using the following parameter values:</p> </li> <li> <p>Name: Dev1-SampleWebApplication-ALB-FQDN</p> </li> <li>Type: FQDN</li> <li>FQDN: (use the DNS value of the internal load balancer in front of the web application)</li> <li>Interface: tgw-vpn1</li> </ol> <p></p> <ol> <li>After saving the entry, refresh the Address grid and verify that the row colour is white.</li> </ol> <p></p> <ol> <li>Navigate to Policy &amp; Objects and select Virtual IPs</li> </ol> <p></p> <ol> <li> <p>Make note of the used ip address in the Details column. In the example above \u201c100.96.250.22\u201d.</p> </li> <li> <p>Click the CLI command icon in the top right corner. Note that the following must be done using the CLI.</p> </li> </ol> <p></p> <p></p> <ol> <li>Update the following script template replacing values for the following:    name, extip, mapped-addr, extport</li> </ol> <pre><code>config firewall vip\nedit \"Dev1-SampleWebApplication-ALB\"\n        set type fqdn\n        set extip 100.96.250.22\n        set extintf \"port1\"\n        set portforward enable\n        set mapped-addr \"Dev1-SampleWebApplication-ALB-FQDN\"\n        set extport 7006\n        set mappedport 443\n    next\nend\n</code></pre> <p></p> <ol> <li>Returning back to the UI interface shows the new entry.</li> </ol> <p></p> <ol> <li>Navigate to Policy &amp; Objects and select IPv4 Policy and expand public (port1)</li> </ol> <p></p> <ol> <li>Locate the desired policy (ex: Dev-Test #8 in the example below). Right-click and click Edit.</li> </ol> <p></p> <ol> <li>Locate the Destination field entry and click the + button.</li> </ol> <p></p> <ol> <li>Locate the newly created VirtualIP entry (ex: Dev1-SampleWbApplication-ALB) and save the changes. NOTE: The entry is NOT the Address/FQDN entry.</li> </ol> <p></p> <ol> <li>After refreshing the page, the row background should be white, and the new destination is visible.</li> </ol> <p></p>"},{"location":"installation/","title":"Accelerator Installation and Upgrades","text":"<p>This section contains information on the installation and upgrade procedures for ASEA.</p> <ul> <li> <p>Installation</p> <ul> <li>Installation Guide</li> <li>Sample Configurations and Customization</li> <li>State Machine Behavior</li> <li>Splitting the Config File</li> <li>Considerations with Existing Organizations</li> <li>Importing ALZ Accounts</li> <li>Open Releases</li> </ul> </li> <li> <p>Upgrades</p> <ul> <li>Upgrade Guide</li> <li>v1.5.0 Upgrade Instructions</li> </ul> </li> <li> <p>Functionality</p> <ul> <li>Services</li> <li>Pricing</li> <li>Architecture Diagrams</li> <li>Key Account &amp; Capability Overview</li> <li>Centralized Logging Details</li> <li>Accelerator Object Naming</li> <li>Open Roadmap</li> </ul> </li> </ul>"},{"location":"installation/customization-index/","title":"1. Accelerator Sample Configurations and Customization","text":""},{"location":"installation/customization-index/#11-summary","title":"1.1. Summary","text":"<ul> <li>Sample config files can be found in this folder<ul> <li>Most of the examples reflect a medium security profile (NIST, ITSG, FEDRAMP)</li> </ul> </li> <li>Unsure where to start, use config.lite-CTNFW-example.json (CT w/NFW variant of option 2)</li> <li>Frugal and want something comprehensive to experiment with, use config.test-example.json (option 5)</li> <li>Config file schema documentation (Draft)</li> <li>Estimated monthly pricing for sample configurations</li> </ul>"},{"location":"installation/customization-index/#12-sample-configuration-files-with-descriptions","title":"1.2. Sample Configuration Files with Descriptions","text":""},{"location":"installation/customization-index/#121-full-configuration-configexamplejson","title":"1.2.1. Full configuration (config.example.json)","text":"<ul> <li> <p>The full configuration file was based on feedback from customers moving into AWS at scale and at a rapid pace. Customers of this nature have indicated that they do not want to have to upsize their perimeter firewalls or add Interface endpoints as their developers start to use new AWS services. These are the two most expensive components of the deployed architecture solution.</p> </li> <li> <p>Default settings:</p> <ul> <li>AWS Control Tower: No</li> <li>Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP)</li> </ul> </li> </ul>"},{"location":"installation/customization-index/#122-lite-weight-configuration-files","title":"1.2.2. Lite weight configuration files","text":"<ul> <li> <p>Four variants with differing central ingress/egress firewalls</p> <ul> <li>Variant 1: Recommended starting point (config.lite-CTNFW-example.json)<ul> <li>Default Settings:<ul> <li>AWS Control Tower: Yes</li> <li>Firewall: AWS Network Firewall</li> </ul> </li> </ul> </li> <li>Variant 2: Recommended for new GC PBMM customers (config.lite-VPN-example.json)<ul> <li>requires 3rd party licensing (BYOL or PAYGO)</li> <li>Default Settings:<ul> <li>AWS Control Tower: No</li> <li>Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP)</li> </ul> </li> </ul> </li> <li>Variant 3: (config.lite-NFW-example.json)<ul> <li>Same as Variant 1 config without AWS Control Tower</li> <li>Default Settings:<ul> <li>AWS Control Tower: No</li> <li>Firewall: AWS Network Firewall</li> </ul> </li> </ul> </li> <li>Variant 4: (config.lite-GWLB-example.json)<ul> <li>requires 3rd party licensing (BYOL or PAYGO)</li> <li>Default Settings:<ul> <li>AWS Control Tower: No</li> <li>Firewall: Gateway Load Balancer with Checkpoint firewalls in an autoscaling group</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>To reduce solution costs and allow customers to grow into more advanced AWS capabilities, we created these lite weight configurations that does not sacrifice functionality, but could limit performance. These config files:</p> <ul> <li>only deploys the 9 required centralized Interface Endpoints (removes 50 from full config). All services remain accessible using the AWS public endpoints, but require traversing the perimeter firewalls</li> <li>removes the perimeter VPC Interface Endpoints</li> <li>reduces the Fortigate instance sizes from c5n.2xl to c5n.xl (VM08 to VM04) in Variant 2: IPSec VPN with Active/Active Fortinet cluster option</li> <li>removes the Unclass ou and VPC</li> <li>AWS Control Tower can be implemented in all sample configs using Variant 1: AWS Control Tower with AWS Network Firewall as an example (new installs only).</li> </ul> </li> <li> <p>The Accelerator allows customers to easily add or change this functionality in future, as and when required without any impact</p> </li> </ul>"},{"location":"installation/customization-index/#123-ultra-lite-sample-configuration","title":"1.2.3. Ultra-Lite sample configuration","text":"<ul> <li>Variant 1: (config.ultralite-CT-example.json)<ul> <li>AWS Control Tower: Yes</li> <li>Firewall: None</li> <li>Networking: None</li> </ul> </li> <li>Variant 2: (config.ultralite-example.json)<ul> <li>AWS Control Tower: No</li> <li>Firewall: None</li> <li>Networking: None</li> </ul> </li> <li> <p>This configuration file was created to represent an extremely minimalistic Accelerator deployment, simply to demonstrate the art of the possible for an extremely simple config. This example is NOT recommended as it violates many AWS best practices. This config has:</p> <ul> <li>no <code>shared-network</code> or <code>perimeter</code> accounts</li> <li>no networking (VPC, TGW, ELB, SG, NACL, endpoints) or route53 (zones, resolvers) objects</li> <li>no Managed AD, AD Connector, rsyslog cluster, RDGW host, or 3rd party firewalls</li> <li>only enables/deploys AWS security services in 2 regions (ca-central-1, us-east-1) (Not recommended)</li> <li>only deploys 2 AWS config rules w/SSM remediation</li> <li>renamed log-archive (Logs), security (Audit) and operations (Ops) account names</li> </ul> </li> </ul>"},{"location":"installation/customization-index/#124-multi-region-sample-configuration-configmulti-region-examplejson","title":"1.2.4. Multi-Region sample configuration (config.multi-region-example.json)","text":"<ul> <li> <p>This configuration file was created to represent a more advanced multi-region version of the Full configuration file from configuration 1 above. This config:</p> <ul> <li>adds a TGW in us-east-1, peered to the TGW in ca-central-1</li> <li>adds TGW static routes, including several dummy sample static routes</li> <li>adds a central Endpoint VPC in us-east-1 with us-east-1 endpoints configured</li> <li>adds a shared VPC for all UnClass OU accounts in us-east-1, connected to the us-east-1 TGW (accessible through ca-central-1)<ul> <li>creates additional zones and resolver rules</li> </ul> </li> <li>Sends us-east-1 CloudWatch Logs to the central S3 log-archive bucket in ca-central-1</li> <li>Deploys SSM documents to us-east-1 and remediates configured rules in UnClass OU</li> <li>adds a local account specific VPC, in us-east-1, in the account MyUnClass and connects it to the us-east-1 TGW (i.e. shares TGW)<ul> <li>local account VPC set to use central endpoints, associates appropriate centralized hosted zones to VPC (also creates 5 local endpoints)</li> </ul> </li> <li>adds a VGW for DirectConnect to the perimeter VPC</li> <li>adds the 3rd AZ in ca-central-1 (MAD &amp; ADC in AZ a &amp; b)</li> </ul> </li> <li> <p>Default Settings:</p> <ul> <li>AWS Control Tower: No</li> <li>Firewall: IPSec VPN with Active/Active Fortinet cluster (uses BGP+ECMP)</li> </ul> </li> </ul>"},{"location":"installation/customization-index/#125-test-configuration-configtest-examplejson-use-for-testing-or-low-security-profiles","title":"1.2.5. Test configuration (config.test-example.json) (Use for testing or Low Security Profiles)","text":"<ul> <li> <p>Further reduces solution costs, while demonstrating full solution functionality (NOT recommendend for production). This config file:</p> <ul> <li>uses the Lite weight configuration as the starting point (NFW variant)</li> <li>consolidates Dev/Test/Prod OU to a single Workloads OU/VPC</li> <li>only enables Security Hub, Config and Macie in ca-central-1 and us-east-1</li> <li>removes the Fortigate firewall cluster (per NFW variant)</li> <li>removes the rsyslog cluster</li> <li>reduces the RDGW instance sizes from t2.large to t2.medium</li> <li>reduces the size of the MAD from Enterprise to Standard edition</li> <li>removes the on-premise R53 resolvers (hybrid dns)</li> <li>reduced various log retention periods and the VPCFlow log interval</li> <li>removes the two example workload accounts</li> <li>adds AWS Network Firewall (NFW) and AWS NATGW for centralized ingress/egress (per NFW variant)</li> </ul> </li> <li> <p>Default Settings:</p> <ul> <li>AWS Control Tower: No</li> <li>Firewall: AWS Network Firewall</li> </ul> </li> </ul>"},{"location":"installation/customization-index/#13-deployment-customizations","title":"1.3. Deployment Customizations","text":""},{"location":"installation/customization-index/#131-multi-file-config-file-and-yaml-formatting-option","title":"1.3.1. Multi-file config file and YAML formatting option","text":"<ul> <li>The sample configuration files are provided as single, all encompassing, json files. The Accelerator also supports both splitting the config file into multiple component files and configuration files built using YAML instead of json. Details can be found in the linked document.</li> </ul>"},{"location":"installation/customization-index/#132-sample-snippets","title":"1.3.2. Sample Snippets","text":"<ul> <li>The sample configuration files do not include the full range of supported configuration file parameters and values, additional configuration file parameters and values can be found in the sample snippets document.</li> </ul>"},{"location":"installation/customization-index/#133-third-party-firewall-example-configs","title":"1.3.3. Third Party Firewall example configs","text":"<ul> <li>The Accelerator is provided with a sample 3rd party configuration file to demonstrate automated deployment of 3rd party firewall technologies. Given the code is vendor agnostic, this process should be able to be leveraged to deploy other vendors firewall appliances. When and if other options become available, we will add them here as well.<ul> <li>Automated firewall configuration customization possibilities</li> <li>Sample Fortinet Fortigate firewall config file</li> </ul> </li> </ul>"},{"location":"installation/customization-index/#14-other-configuration-file-hints-and-tips","title":"1.4. Other Configuration File Hints and Tips","text":"<ul> <li>It is critical that all accounts that are leveraged by other accounts (i.e. accounts that any workload accounts are dependant on), are included in the mandatory-accounts section of the config file (i.e. shared-network, log-archive, operations)</li> <li>Account pointers within the config file point to the account key (i.e. (<code>mandatory-account-configs\\account-key</code>) and NOT the account name field (<code>mandatory-account-configs\\account-key\\account-name: \"account name\"</code>). This allows for easy account names, duplicate account names, and no requirement to update account pointers during account renames.</li> <li>If any of the account pointers within <code>global-options</code> does not point to a valid mandatory account key, the State Machine will fail with the error <code>EnvironmentVariable value cannot be null</code> before starting CodeBuild Phase -1</li> <li>You cannot supply (or change) configuration file values to something not supported by the AWS platform<ul> <li>For example, CWL retention only supports specific retention values (not any number)</li> <li>Shard count - can only increase/reduce by half the current limit. i.e. you can change from <code>1</code>-<code>2</code>, <code>2</code>-<code>3</code>, <code>4</code>-<code>6</code></li> </ul> </li> <li>Always add any new items to the END of all lists or sections in the config file, otherwise<ul> <li>Update validation checks will fail (VPC's, subnets, share-to, etc.)</li> </ul> </li> <li>To skip, remove or uninstall a component, you can often simply change the section header, instead of removing the section<ul> <li>change \"deployments\"/\"firewalls\" to \"deployments\"/\"xxfirewalls\" and it will uninstall the firewalls and maintain the old config file settings for future use</li> <li>Objects with the parameter deploy: true, support setting the value to false to remove the deployment</li> </ul> </li> <li>As you grow and add AWS accounts, the Kinesis Data stream in the log-archive account will need to be monitored and have its capacity (shard count) increased by setting <code>\"kinesis-stream-shard-count\"</code> variable under <code>\"central-log-services\"</code> in the config file</li> <li>Updates to NACL's requires changing the rule number (<code>100</code> to <code>101</code>) or they will fail to update</li> <li>When adding a new subnet or subnets to a VPC (including enabling an additional AZ), you need to:<ul> <li>increment any impacted NACL id's in the config file (<code>100</code> to <code>101</code>, <code>32000</code> to <code>32001</code>) (CFN does not allow nacl updates)</li> <li>make a minor change to any impacted route table names (<code>MyRouteTable</code> to <code>MyRouteTable1</code>) (CFN does not allow updates to route table associated ids)</li> </ul> </li> <li>The sample VPN firewall configuration uses an instance with 4 NIC's, make sure you use an instance size that supports 4 ENI's</li> <li>Firewall names, CGW names, TGW names, MAD Directory ID, account keys, and OU's must all be unique throughout the entire configuration file (also true for VPC names given NACL and security group referencing design)</li> <li>The configuration file does have validation checks in place that prevent users from making certain major unsupported configuration changes</li> <li>The configuration file does NOT have extensive error checking. It is expected you know what you are doing. We eventually hope to offer a config file, wizard based GUI editor and add the validation logic in this separate tool. In most cases the State Machine will fail with an error, and you will simply need to troubleshoot, rectify and rerun the state machine.</li> <li>You cannot move an account between top-level OU's. This would be a security violation and cause other issues. You can move accounts between sub-ou. Note: The Control Tower version of the Accelerator does NOT support sub-ou's.</li> <li>When using YAML configuration files, we only support the subset of yaml that converts to JSON (we do not support anchors)</li> <li>Security Group names were designed to be identical between environments, if you want the VPC name in the SG name, you need to do it manually in the config file</li> <li>Adding more than approximately 50 new VPC Interface Endpoints across all regions in any one account in any single state machine execution will cause the state machine to fail due to Route 53 throttling errors. If adding endpoints at scale, only deploy 1 region at a time. In this scenario, the stack(s) will fail to properly delete, also based on the throttling, and will require manual removal.</li> <li>We do not support Directory unsharing or ADC deletion, delete methods were not implemented. We only support ADC creation in mandatory accounts.</li> <li>If <code>use-central-endpoints</code> is changed from true to false, you cannot add a local VPC endpoint on the same state machine execution (add the endpoint on a prior or subsequent execution)</li> <li>If you update the 3rd party firewall names, be sure to update the routes and alb's which point to them. Firewall licensing occurs through the management port, which requires a VPC route back to the firewall to get internet access and validate the firewall license.</li> <li>Removing the AWS NFW requires 2 state machine executions, in the first you must remove all routes that reference the NFW, and in the second you can remove or xx out the NFW (also true for the GWLB implementation ).</li> </ul>"},{"location":"installation/customization-index/#15-config-file-and-deployment-protections","title":"1.5. Config file and Deployment Protections","text":"<ul> <li>The config file is moved to AWS CodeCommit after the first execution of the state machine to provide strong configuration history, versioning and change control</li> <li>After each successful state machine execution, we record the commit id of the config file used for that execution in secrets manager</li> <li>On every state machine execution, before making any changes, the Accelerator compares the latest version of the config file stored in CodeCommit with the version of the config file from the last successful state machine execution (after replacing all variables)</li> <li>If the config file includes any changes we consider to be significant or breaking, we immediately fail the state machine<ul> <li>if a customer somehow accidentally uploads a different customers config file into their Accelerator CodeCommit repository, the state machine will fail</li> <li>if a customer makes what we consider to be a major change to the config file, the state machine will fail</li> <li>if a customer makes a change that we believe has a high likelihood to cause a deployment failure, the state machine will fail</li> </ul> </li> <li>If a customer believes they understand the full implications of the changes they are making (and has made any required manual changes to allow successful execution), we have provided protection override flags. These overrides should be used with extremely caution:<ul> <li>To provide maximum protection we have provided scoped override flags. Customers can provide a flag or flags to only bypass specific type(s) of config file validations or blocks. If using an override flag, we recommend customers use these scoped flags in most situations.</li> <li>If a customer is purposefully making extensive changes across the config file and wants to simply override all checks with a single override flag, we also have this option, but discourage it use.</li> <li>The various override flags and their format can be found in here.</li> </ul> </li> </ul>"},{"location":"installation/customization-index/#16-summary-of-example-config-file-minimum-changes-for-new-installs","title":"1.6. Summary of Example Config File Minimum Changes for New Installs","text":"<p>At a minimum you should consider reviewing the following config file sections and make the required changes.</p>"},{"location":"installation/customization-index/#161-global-options","title":"1.6.1. Global Options","text":"<ul> <li>S3 Central Bucket<ul> <li><code>global-options/central-bucket</code>: \"AWSDOC-EXAMPLE-BUCKET\"</li> <li>replace with <code>your-bucket-name</code> as referenced in the Installation Guide Step #5</li> </ul> </li> <li>Central Log Services SNS Emails<ul> <li><code>global-options/central-log-services/sns-subscription-emails</code>: \"myemail+notifyT-xxx@example.com\"</li> <li>update the 3 email addresses (high, medium and low) as required. Each address will receives alerts or alarms of the specified level. The same email address can be used for all three.</li> </ul> </li> <li>The default dynamic CIDR pools (<code>global-options/cidr-pools</code>) listed below are used to assign ranges based on the subnet mask set in each VPC and subnet throughout the configuration file.<ul> <li><code>global-options/cidr-pools/0/cidr</code>: \"10.0.0.0/13\"<ul> <li>The main address pool used to dynamically assign CIDR ranges for most VPCs</li> </ul> </li> <li><code>global-options/cidr-pools/1/cidr</code>: \"100.96.252.0/23\"<ul> <li>Address pool used to dynamically assign CIDR ranges for the Managed Active Directory subnets in the Ops account</li> </ul> </li> <li><code>global-options/cidr-pools/2/cidr</code>: \"100.96.250.0/23\"<ul> <li>Address pool used to dynamically assign CIDR ranges for the Perimeter VPC</li> </ul> </li> <li><code>global-options/cidr-pools/3/cidr</code>: \"10.249.1.0/24\"<ul> <li>A non-routable pool of addresses used to dynamically assign CIDR ranges for the Active Directory Connector subnets in the Organization Management/root account</li> </ul> </li> </ul> </li> </ul>"},{"location":"installation/customization-index/#162-mandatory-account-configs","title":"1.6.2. Mandatory Account Configs","text":"<ul> <li>All mandatory accounts specific to your config file, that are present under the <code>mandatory-account-config</code> section require you to assign a unique email address for each account listed below. Replace the email values in the JSON config file for these accounts with unique email addresses.<ul> <li><code>mandatory-account-configs/shared-network/email</code>: \"myemail+aseaT-network@example.com---------------------REPLACE------------\"</li> <li><code>mandatory-account-configs/operations/email</code>: \"myemail+aseaT-operations@example.com---------------------REPLACE------------\"</li> <li><code>mandatory-account-configs/perimeter/email</code>: \"myemail+aseaT-perimeter@example.com---------------------REPLACE------------\"</li> <li><code>mandatory-account-configs/management/email</code>: \"myemail+aseaT-management@example.com---------------------REPLACE------------\" (Note: This is the email of your root account)</li> <li><code>mandatory-account-configs/log-archive/email</code>: \"myemail+aseaT-log@example.com---------------------REPLACE------------\"</li> <li><code>mandatory-account-configs/security/email</code>: \"myemail+aseaT-sec@example.com---------------------REPLACE------------\"</li> </ul> </li> <li>Budget Alerts email addresses need to be replaced with an email address in your organization. It can be the same email address for all budget alerts. Config located at the following path (Multiple exist for different thresholds, update all under each account):<ul> <li><code>mandatory-account-configs/shared-network/budget/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> <li><code>mandatory-account-configs/perimeter/budget/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> <li><code>mandatory-account-configs/management/budget/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> </ul> </li> <li>For the <code>shared-network</code> account, review and update the following (or delete the sections):<ul> <li><code>mandatory-account-configs/shared-network/vpc/on-premise-rules/zone</code>: \"on-premise-privatedomain1.example.ca\" (qty 2)</li> <li><code>mandatory-account-configs/shared-network/vpc/zones/private</code>: \"cloud-hosted-privatedomain.example.ca\"</li> <li><code>mandatory-account-configs/shared-network/vpc/zones/public</code>: \"cloud-hosted-publicdomain.example.ca\"</li> </ul> </li> <li>For the <code>operations</code> account, review and update the following:<ul> <li><code>mandatory-account-configs/operations/deployments/mad/dns-domain</code>: \"example.local\"</li> <li><code>mandatory-account-configs/operations/deployments/mad/netbios-domain</code>: \"example\"</li> <li><code>mandatory-account-configs/operations/deployments/mad/log-group-name</code>: \"/${ACCELERATOR_PREFIX_ND}/MAD/example.local\" (replace example.local)</li> <li><code>mandatory-account-configs/operations/deployments/mad/ad-users</code> (update user, email and group of each user as required)<ul> <li>do not remove or change permissions on the <code>adconnector-usr</code></li> </ul> </li> </ul> </li> <li> <p>For <code>perimeter</code> account, review and update the following:</p> <ul> <li><code>mandatory-account-configs/perimeter/certificates/priv-key</code>: \"certs/example1-cert.key\"</li> <li><code>mandatory-account-configs/perimeter/certificates/cert</code>: \"certs/example1-cert.crt\"</li> <li> <p>If you are using VPN config:</p> <ul> <li><code>mandatory-account-configs/perimeter/deployments/firewalls/image-id</code>: \"ami-0d8e2e78e928def11\"<ul> <li>Update AMI with the AMI collected from the Marketplace for Fortinet FortiGate (BYOL) Next-Generation Firewall</li> </ul> </li> <li><code>mandatory-account-configs/perimeter/deployments/xxfirewall-manager/image-id</code>: \"ami-0e9f45c3ec34c3a9a\"<ul> <li>Update AMI with the AMI collected from the Marketplace for Fortinet FortiManager (BYOL) Centralized Security Management</li> <li>NOTE: Default config of \"xxfirewall-manager\" will prevent the firewall manager from being deployed. To deploy the firewall manager remove the \"xx\" to set the parameter to \"firewall-manager\"</li> </ul> </li> <li><code>mandatory-account-configs/perimeter/deployments/firewalls/license</code>: [\"firewall/license1.lic\", \"firewall/license2.lic\"]<ul> <li>Two Fortinet FortiGate firewall licenses, if you don't have any license files, update the config file with an empty array (\"license\": []). Do NOT use the following: [\"\"]</li> <li>Place files in a folder (eg. firewall) in the same S3 bucket in your Organization Management account as the deployment configuration file.</li> </ul> </li> <li><code>mandatory-account-configs/perimeter/deployments/firewalls/config</code>: \"firewall/firewall-example.txt\"<ul> <li>The Fortinet configuration file to initially configure the firewalls. Sample configuration files can be found in the reference-artifacts/Third-Party folder</li> <li>Place file in a folder (eg. firewall) in the same S3 bucket in your Organization Management account as the deployment configuration file.</li> </ul> </li> </ul> </li> <li> <p>If you are using GWLB config:</p> <ul> <li><code>mandatory-account-configs/perimeter/deployments/firewalls/Checkpoint-Firewall - image-id</code>: \"ami-0217611bf09d5b4c1\"<ul> <li>Update AMI with the AMI collected from the Marketplace for CloudGuard Network Security for GWLB - BYOL</li> </ul> </li> <li><code>mandatory-account-configs/perimeter/deployments/firewall-manager/image-id</code>: \"ami-0071a3b4ef9ac766a\"<ul> <li>Update AMI with the AMI collected from the Marketplace for Checkpoint Security Management</li> </ul> </li> <li><code>mandatory-account-configs/perimeter/deployments/firewall-manager/version</code>: \"R8110BYOLMGMT\"<ul> <li>Update version based on the selected ami-id version from the Private Marketplace</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>For <code>management</code>, review and update the following:</p> <ul> <li><code>mandatory-account-configs/management/account-name</code>: \"ASEA-Main\"<ul> <li>Update this field with your Organization Management (root) account name, if it is not set to ASEA-Main.</li> </ul> </li> <li><code>mandatory-account-configs/management/iam/users</code><ul> <li>the names of your break-glass and ASEA operation users</li> </ul> </li> </ul> </li> </ul>"},{"location":"installation/customization-index/#163-workload-account-configs","title":"1.6.3. Workload Account Configs","text":"<ul> <li> <p>As mentioned in the Installation Guide, we recommend not adding more than 1 or 2 workload accounts to the config file during the initial deployment as it will increase risks of hitting a limit. Once the Accelerator is successfully deployed, add the additional accounts back into the config file and rerun the state machine.</p> </li> <li> <p>Review the workload accounts in the config that you selected and change the name and email as desired</p> <ul> <li>Modify <code>mydevacct1</code> with the account name of your choosing</li> <li>Modify <code>mydevacct1/account-name</code>: \"MyDev1\" with the account name</li> <li>Modify <code>mydevacct1/email</code>: \"myemail+aseaT-dev1@example.com---------------------REPLACE------------\" with a unique email address for the account</li> <li>Modify <code>mydevacct1/description</code>: \"This is an OPTIONAL SAMPLE workload account...\" with a description relevant to your account</li> <li>Modify <code>mydevacct1/ou</code>: \"Dev\" with the OU that you would like the account to be attached to</li> </ul> </li> </ul>"},{"location":"installation/customization-index/#164-organization-units","title":"1.6.4. Organization Units","text":"<ul> <li>For all organization units, update the budget alerts email addresses:<ul> <li><code>organizational-units/core/default-budgets/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> <li><code>organizational-units/Central/default-budgets/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> <li><code>organizational-units/Dev/default-budgets/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> <li><code>organizational-units/Test/default-budgets/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> <li><code>organizational-units/Prod/default-budgets/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> <li><code>organizational-units/Sandbox/default-budgets/alerts/emails</code>: \"myemail+aseaT-budg@example.com\"</li> </ul> </li> <li>For organization units with <code>certificates</code>, review the certificates and update as you see fit. These certificates are used in the <code>alb</code> section under <code>alb/cert-name</code> of each OU</li> </ul>"},{"location":"installation/existing-orgs/","title":"1. Existing Organizations / Accounts","text":""},{"location":"installation/existing-orgs/#11-considerations-importing-existing-aws-accounts-deploying-into-existing-aws-organizations","title":"1.1. Considerations: Importing existing AWS Accounts / Deploying Into Existing AWS Organizations","text":"<ul> <li>The Accelerator can be installed into existing AWS Organizations<ul> <li>our early adopters have all successfully deployed into existing organizations</li> </ul> </li> <li>Existing AWS accounts can also be imported into an Accelerator managed Organization</li> <li>Caveats:<ul> <li>Per AWS Best Practices, the Accelerator deletes the default VPC's in all AWS accounts, worldwide. The inability to delete default VPC's in pre-existing accounts will fail the installation/account import process. Ensure default VPC's can or are deleted before importing existing accounts. On failure, either rectify the situation, or remove the account from Accelerator management and rerun the state machine</li> <li>The Accelerator will NOT alter existing (legacy) constructs (e.g. VPC's, EBS volumes, etc.). For imported and pre-existing accounts, objects the Accelerator prevents from being created using preventative guardrails will continue to exist and not conform to the prescriptive security guidance<ul> <li>Existing workloads should be migrated to Accelerator managed VPC's and legacy VPC's deleted to gain the full governance benefits of the Accelerator (centralized flow logging, centralized ingress/egress, no IGW's, Session Manager access, existing non-encrypted EBS volumes, etc.)</li> </ul> </li> <li>Existing AWS services will be reconfigured as defined in the Accelerator configuration file (overwriting existing settings)</li> <li>We do NOT support any workloads running or users operating in the Organization Management (root) AWS account. The Organization Management (root) AWS account MUST be tightly controlled</li> <li>Importing existing workload accounts is fully supported, we do NOT support, recommend and strongly discourage importing mandatory accounts, unless they were clean/empty accounts. Mandatory accounts are critical to ensuring governance across the entire solution</li> <li>We've tried to ensure all customer deployments are smooth. Given the breadth and depth of the AWS service offerings and the flexibility in the available deployment options, there may be scenarios that cause deployments into existing Organizations to initially fail. In these situations, simply rectify the conflict and re-run the state machine.</li> <li>If the Firewall Manager administrative account is already set for your organization, it needs to be unset before starting a deployment.</li> </ul> </li> </ul>"},{"location":"installation/existing-orgs/#12-process-to-import-existing-aws-accounts-into-an-accelerator-managed-organization","title":"1.2. Process to import existing AWS accounts into an Accelerator managed Organization","text":"<ul> <li>Newly invited AWS accounts in an Organization will land in the root ou</li> <li>Unlike newly created AWS accounts which immediately have a Deny-All SCP applied, imported accounts are not locked down as we do not want to break existing workloads (these account are already running without Accelerator guardrails)</li> <li>In AWS Organizations, select ALL the newly invited AWS accounts and move them all (preferably at once) to the correct destination OU (assuming the same OU for all accounts)<ul> <li>In case you need to move accounts to multiple OU's we have added a 2 minute delay before triggering the State Machine</li> <li>Any accounts moved after the 2 minute window will NOT be properly ingested, and will need to be ingested on a subsequent State Machine Execution</li> </ul> </li> <li>This will first trigger an automated update to the config file and then trigger the state machine after a 2 minute delay, automatically importing the moved accounts into the Accelerator per the destination OU configuration</li> <li>As previously documented, accounts CANNOT be moved between OU's to maintain compliance, so select the proper top-level OU with care</li> <li>If you need to customize each of the accounts configurations, you can manually update the configuration file either before or after you move the account to the correct ou<ul> <li>if before, you also need to include the standard 4 account config file parameters, if after, you can simply add your new custom parameters to the account entry the Accelerator creates</li> <li>if you add your imported accounts to the config file, moving the first account to the correct ou will trigger the state machine after a 2 minutes delay. If you don't move all accounts to their correct ou's within 2 minutes, your state machine will fail. Simply finish moving all accounts to their correct OU's and then rerun the state machine.</li> </ul> </li> <li>If additional accounts are moved into OUs while the state machine is executing, they will not trigger another state machine execution, those accounts will only be ingested on the next execution of the state machine<ul> <li>customers can either manually initiate the state machine once the current execution completes, or, the currently running state machine can be stopped and restarted to capture all changes at once</li> <li>Are you unsure if an account had its guardrails applied? The message sent to the state machine Status SNS topic (and corresponding email address) on a successful state machine execution provides a list of all successfully processed accounts.</li> </ul> </li> <li>The state machine is both highly parallel and highly resilient, stopping the state machine should not have any negative impact. Importing 1 or 10 accounts generally takes about the same amount of time for the Accelerator to process, so it may be worth stopping the current execution and rerunning to capture all changes in a single execution.</li> <li>We have added a 2 min delay before triggering the state machine, allowing customers to make multiple changes within a short timeframe and have them all captured automatically in the same state machine execution.</li> </ul>"},{"location":"installation/existing-orgs/#13-deploying-the-accelerator-into-an-existing-organization","title":"1.3. Deploying the Accelerator into an existing Organization","text":"<ul> <li>As stated above, if the ALZ was previously deployed into the Organization, please work with your AWS account team to find the best mechanism to uninstall the ALZ solution</li> <li>Ensure all existing sub-accounts have the role name defined in <code>organization-admin-role</code> installed and set to trust the Organization Management (root) AWS Organization account<ul> <li>prior to v1.2.5, this role must be named: <code>AWSCloudFormationStackSetExecutionRole</code></li> <li>if using the default role (<code>AWSCloudFormationStackSetExecutionRole</code>) we have provided a CloudFormation stack which can be executed in each sub-account to simplify this process</li> </ul> </li> <li>As stated above, we recommend starting with new AWS accounts for the mandatory functions (shared-network, perimeter, security, log-archive accounts).</li> <li>To better ensure a clean initial deployment, we also recommend the installation be completed while ignoring most of your existing AWS sub-accounts, importing them post installation:<ul> <li>create a new OU (i.e. <code>Imported-Accounts</code>), placing most of the existing accounts into this OU temporarily, and adding this OU name to the <code>global-options\\ignored-ous</code> config parameter;</li> <li>any remaining accounts must be in the correct ou, per the Accelerator config file;</li> <li>install the Accelerator;</li> <li>import the skipped accounts into the Accelerator using the above import process, paying attention to the below notes</li> </ul> </li> <li>NOTES:<ul> <li>Do NOT move any accounts from any <code>ignored-ous</code> to the root ou, they will immediately be quarantined with a Deny-All SCP, they need to be moved directly to their destination ou</li> <li>As stated above, when importing accounts, there may be situations we are not able to fully handle<ul> <li>If doing a mass import, we suggest you take a quick look and if the solution is not immediately obvious, move the account which caused the failure back to ignored-ous and continue importing the remainder of your accounts. Once you have the majority imported, you can circle back and import outstanding problem accounts with the ability to focus on each individual issue</li> <li>The challenge could be as simple as someone has instances running in a default VPC, which may require some cleanup effort before we can import (coming soon, you will be able to exclude single account/region combinations from default VPC deletion to gain the benefits of the rest of the guardrails while you migrate workloads out of the default VPC)</li> </ul> </li> </ul> </li> </ul>"},{"location":"installation/install/","title":"1. Accelerator Installation Guide","text":""},{"location":"installation/install/#11-overview","title":"1.1. Overview","text":"<p>We encourage customers installing the Accelerator to get the support of their local AWS account team (SA, TAM, CSM, ProServe) to assist with the installation of the Accelerator, as the Accelerator leverages, deploys, or orchestrates over 50 different AWS services.</p> <p>Users are strongly encouraged to also read the Accelerator Operations/Troubleshooting Guide before installation and the FAQ while waiting for the installation to complete. The Operations/Troubleshooting Guide provides details as to what is being performed at each stage of the installation process, including detailed troubleshooting guidance.</p> <p>These installation instructions assume one of the prescribed architectures is being deployed.</p>"},{"location":"installation/install/#12-prerequisites","title":"1.2. Prerequisites","text":""},{"location":"installation/install/#121-general","title":"1.2.1. General","text":"<ul> <li>Management or root AWS Organization account (the AWS Accelerator cannot be deployed in an AWS sub-account)<ul> <li>No additional AWS accounts need to be pre-created before Accelerator installation</li> </ul> </li> <li>If required, a limit increase to support your desired number of new AWS sub-accounts (default limit is 10 sub-accounts)<ul> <li>recent changes to new AWS account limits are causing accelerator installation failures, please work with your local account team to increase your limits</li> </ul> </li> <li>Valid Accelerator configuration file, updated to reflect your requirements (see below)</li> <li>Determine your primary or Accelerator <code>control</code> or <code>home</code> region, this is the AWS region in which you will most often operate</li> <li>Government of Canada customers are still required to do a standalone installation at this time, please request standalone installation instructions from your Account SA or TAM</li> <li>The Accelerator can be installed into existing AWS Organizations - see caveats and notes here</li> <li>Existing AWS Landing Zone Solution (ALZ) customers are required to remove their ALZ deployment before deploying the Accelerator. Scripts are available to assist with this process.</li> <li>Changes to the Accelerator codebase are strongly discouraged unless they are contributed and accepted back to the solution. Code customization will block the ability to upgrade to the latest release and upgrades are encouraged to be done between quarterly to semi-annually. The solution was designed to be extremely customizable without changing code, existing customers following these guidelines have been able to upgrade across more than 50 Accelerator releases, while maintaining their customizations and gaining the latest bug fixes, features and enhancements without any developer or professional services based support. Please see this FAQ for more details.</li> </ul>"},{"location":"installation/install/#13-production-deployment-planning","title":"1.3. Production Deployment Planning","text":""},{"location":"installation/install/#131-general","title":"1.3.1. General","text":"<p>For any deployment of the Accelerator which is intended to be used for production workloads, you must evaluate all these decisions carefully. Failure to understand these choices could cause challenges down the road. If this is a \"test\" or \"internal\" deployment of the Accelerator which will not be used for production workloads, you can leave the default config values.</p> <p>Config file schema documentation (Draft)</p>"},{"location":"installation/install/#132-ou-structure-planning","title":"1.3.2. OU Structure Planning","text":"<p>Plan your OU and core account structure carefully. By default, we suggest: <code>Security, Infrastructure, Central, Sandbox, Dev, Test, Prod</code>.</p> <ul> <li>The <code>Security</code> OU will contain the <code>Security</code> account, the <code>Log Archive</code> account, and the Organization <code>Management</code> account.</li> <li>The <code>Infrastructure</code> OU will hold the remainder of the accounts shared or utilized by the rest of the organization (<code>Shared Network</code>, <code>Perimeter</code>, and <code>Operations</code>).</li> <li>The remainder of the OUs correspond with major permission shifts in the SDLC cycle and NOT every stage an organization has in their SDLC cycle (i.e. QA or pre-prod would be included in one of the other OUs).</li> <li>The <code>Central</code> OU is used to hold accounts with workloads shared across Dev, Test, and Prod environments like centralized CI/CD tooling.</li> <li>The v1.5.0+ releases align the Accelerator OU and account structure with AWS multi-account guidance, splitting the <code>core</code> OU into the <code>Security</code> and <code>Infrastructure</code> OUs.</li> </ul> <p>Note: While OUs can be renamed or additional OUs added at a later point in time, deployed AWS accounts CANNOT be moved between top-level OUs (guardrail violation), nor can top-level OUs easily be deleted (requires deleting all AWS accounts from within the OU first).</p>"},{"location":"installation/install/#133-network-configuration-planning","title":"1.3.3. Network Configuration Planning","text":"<p>If deploying the prescriptive architecture using the Full or Lite sample config files, you will need the following network constructs:</p> <ol> <li> <p>Six (6) RFC1918 Class B address blocks (CIDR's) which do not conflict with your on-premise networks (a single /13 block works well)</p> <ul> <li>VPC CIDR blocks cannot be changed after installation, this is simply the way the AWS platform works, given everything is built on top of them. Carefully consider your address block selection.</li> <li>one block for each OU, except Sandbox which is not routable (Sandbox OU will use a 7th non-routed address block)</li> <li>the \"core\" Class B range will be split to support the Endpoint VPC and Perimeter VPC (with extra addresses remaining for future use)</li> <li>Given a shared VPC architecture is leveraged (prevents stranded islands of CIDR blocks and reduces networking costs), we have assigned a class B address block to each VPC to future proof the deployment. Smaller customers can successfully deploy with a half class B CIDR block per shared VPC.</li> </ul> </li> <li> <p>Two (2) RFC6598 /23 address blocks (Government of Canada (GC) requirement only)</p> <ul> <li>Used for AWS Managed Active Directory (MAD) deployment and perimeter underlay network</li> <li>non-GC customers can replace the RFC6598 address space with the extra unused addresses from the above RFC1918 CIDR range above (the App2 subnets in the Central VPC and the Perimeter VPC address space)</li> </ul> </li> <li> <p>BGP ASN's for network routing, one for each of:</p> <ul> <li>Transit Gateway (one unique ASN per TGW, multi-region example requires a second ASN)</li> <li>IPSec VPN Firewall Cluster (if deployed)</li> <li> <p>VGW for Direct Connect connectivity (only shown in the config.multi-region-example.json)</p> </li> <li> <p>For example: the Control Tower with Network Firewall example config requires a single BGP ASN for the TGW, the IPSec VPN example requires two BGP ASN's, and the multi-region example requires five unique BGP ASN's.</p> </li> </ul> </li> </ol> <p>NOTE: Prior to v1.5.0 CIDR ranges were assigned to each VPC and subnet throughout the config file. This required customers to perform extensive updates across the config file when needing to move to specific IP ranges compatible with a customer's existing on-premise networks.</p> <p>While this is still supported for those wanting to control exactly what address is used on every subnet, the solution has added support for dynamic CIDR assignments and the sample config files have been updated to reflect. New installs will have CIDR's pulled from CIDR pools, defined in the global-options section of the config file with state maintained in DynamoDB.</p> <p>The v1.5.0 custom upgrade guide will provides details on the upgrade process and requirements to migrate to the new CIDR assignment system, if desired. A script was created to assist with this migration.</p>"},{"location":"installation/install/#134-dns-domain-name-tls-certificate-planning","title":"1.3.4. DNS, Domain Name, TLS Certificate Planning","text":"<p>If deploying the prescriptive architecture, you must decide on:</p> <ol> <li>A unique Windows domain name (<code>organizationaws</code>/<code>organization.aws</code>, <code>organizationcloud</code>/<code>organization.cloud</code>, etc.). Given this is designed as the primary identity store and used to domain join all cloud hosted workloads, changing this in future is difficult. Pick a Windows domain name that does NOT conflict with your on-premise AD domains, ensuring the naming convention conforms to your organizations domain naming standards to ensure you can eventually create a domain trust between the MAD and on-premise domains/forests</li> <li>DNS Domain names and DNS server IP's for on-premise private DNS zones requiring cloud resolution (can be added in future)</li> <li>DNS Domain for a cloud hosted public zone <code>\"public\": [\"organization.cloud-nuage.canada.ca\"]</code> (can be added in future)</li> <li>DNS Domain for a cloud hosted private zone <code>\"private\": [\"organization.cloud-nuage.gc.ca\"]</code> (can be added in future)</li> <li>Wildcard TLS certificate for each of the 2 previous zones (can be added/changed in future)</li> </ol>"},{"location":"installation/install/#135-email-address-planning","title":"1.3.5. Email Address Planning","text":"<ol> <li>While you require a minimum of 6 unique email addresses (1 per sub-account being created), we recommend at least 20 unique email ALIASES associated with a single mailbox, never used before to open AWS accounts, such that you do not need to request new email aliases every time you need to create a new AWS account and they can all be monitored via a single mailbox. These email addresses can never have been used to previously open an AWS account.</li> <li>You additionally require email addresses for the following additional purposes (these can be existing monitored mailboxes and do not need to be unique):<ul> <li>Accelerator execution (state machine) notification events (1 address)</li> <li>High, Medium and Low security alerts (3 addresses if you wish to segregate alerts)</li> <li>Budget notifications</li> </ul> </li> </ol>"},{"location":"installation/install/#136-centralized-ingressegress-firewalls","title":"1.3.6. Centralized Ingress/Egress Firewalls","text":"<p>As of v1.5.0 the Accelerator offers multiple automated firewall deployment options:</p> <p>a) AWS Network Firewall (native AWS Cloud service)</p> <pre><code>- Defined in the config file as part of a VPC\n</code></pre> <p>b) 3rd party firewalls interconnected to the cloud tenancy via IPSec VPN (Active/Active using BGP + ECMP)</p> <ul> <li>Defined in the config file under deployments w/TGW VPN attachments</li> <li>this was the only automated option prior to v1.5.0</li> <li>a sample Fortinet Fortigate configuration is provided (both PAYGO and BYOL supported)</li> <li>For Fortinet BYOL, requires minimum 2 valid license files (evaluation licenses adequate) (can be added in future)</li> </ul> <p>c) 3rd party firewalls interconnected to the cloud tenancy via Gateway Load Balancer (GWLB) in an auto-scaling group</p> <ul> <li>Defined in the config file under both deployments and load balancers</li> <li>a sample Checkpoint CloudGuard configuration is provided (both PAYGO and BYOL supported)</li> </ul> <p>d) Customer gateway (CGW) creation, to enable connectivity to on-premises firewalls or manually deployed cloud firewalls</p> <ul> <li>Defined in the config file under deployments w/TGW VPN attachments (but without an AMI or VPC association)</li> </ul> <p>Examples of each of the firewall options have been included as variants of the Lite config file example.</p> <p>Note: While we only provide a single example for each 3rd party implementation today, the implementations are generic and should be usable by any 3rd party firewall vendor, assuming they support the required features and protocols. The two examples were driven by customer demand and heavy lifting by the 3rd party vendor. We look forward to additional vendors developing and contributing additional sample configurations. For new 3rd party integrations, we encourage the use of the GWLB approach.</p>"},{"location":"installation/install/#137-other","title":"1.3.7. Other","text":"<ol> <li>We recommend installing with the default Accelerator Name (<code>ASEA</code>) and Accelerator Prefix (<code>ASEA-</code>), but allow customization. Prior to v1.5.0 the defaults were (<code>PBMM</code>) and (<code>PBMMAccel-</code>) respectively.<ul> <li>the Accelerator name and prefix CANNOT be changed after the initial installation;</li> <li>the Accelerator prefix including the mandatory dash cannot be longer than 10 characters.</li> </ul> </li> <li>New installations, which now leverage Control Tower, require the <code>organization-admin-role</code> be set to <code>AWSControlTowerExecution</code>. Existing standalone installations will continue to utilize their existing role name for the <code>organization-admin-role</code>, typically <code>OrganizationAccountAccessRole</code>, as this role is used by AWS Organizations by default when no role name is specified while creating AWS accounts through the AWS console.<ul> <li>the Accelerator leverages this role name to create all new accounts in the organization;</li> <li>this role name, as defined in the config file, MUST be utilized when manually creating all new sub-accounts in the Organization;</li> <li>existing installs wishing to change the role name are required to first deploy a new role with a trust to the root account, in all accounts in the organization.</li> </ul> </li> </ol>"},{"location":"installation/install/#14-accelerator-pre-install-steps","title":"1.4. Accelerator Pre-Install Steps","text":""},{"location":"installation/install/#141-general","title":"1.4.1. General","text":"<p>Before installing, you must first:</p> <ol> <li>Login to the Organization Management (root) AWS account with <code>AdministratorAccess</code>.</li> <li>Set the region to your desired <code>home</code> region (i.e. <code>ca-central-1</code>)</li> <li>Install AWS Control Tower:<ul> <li>Government of Canada customers are required to skip this step</li> <li>OU and account names can ONLY be customized during initial installation. These values MUST match with the values supplied in the Accelerator config file.<ol> <li>Go to the AWS Control Tower console and click <code>Set up landing zone</code></li> <li>Select your <code>home</code> region (i.e. <code>ca-central-1</code>) - the Accelerator home region must match the Control Tower home region</li> <li>Leave the Region deny setting set to <code>Not enabled</code> - the Accelerator needs a customized region deny policy</li> <li>Select all regions for <code>Additional AWS Regions for governance</code>, click <code>Next</code><ul> <li>The Control Tower and Accelerator regions MUST be properly aligned</li> <li>If a region is not <code>governed</code> by Control Tower, it must NOT be listed in <code>control-tower-supported-regions</code></li> <li>To manage a region requires the region:<ul> <li>be enabled in Control Tower (if supported)</li> <li>added to the config file <code>control-tower-supported-regions</code> list (if supported)</li> <li>added to the config file <code>supported-regions</code> list (even if not supported by Control Tower, as the Accelerator can manage regions not yet supported by Control Tower, but only when NOT listed in <code>control-tower-supported-regions</code>)</li> <li>While we highly recommend guardrail deployment for all AWS enabled by default regions, at minimum<ul> <li>the home region MUST be enabled in Control Tower and must be listed in <code>control-tower-supported-regions</code></li> <li>both the home-region and ${GBL*REGION} must be listed in <code>supported-regions</code></li> </ul> </li> </ul> </li> </ul> </li> <li>For the <code>Foundational OU</code>, leave the default value <code>Security</code></li> <li>For the <code>Additional OU</code> provide the value <code>Infrastructure</code>, click <code>Next</code></li> <li>Enter the email addresses for your <code>Log Archive</code> and <code>Audit</code> accounts, change the <code>Audit</code> account name to <code>Security</code>, click <code>Next</code> - OU and account names can ONLY be customized during initial installation. OU names, account names and email addresses _must* match identically with the values supplied in the Accelerator config file.</li> <li>Select <code>Enabled</code> for AWS CloudTrail configuration (if not selected), click <code>Next</code></li> <li>Click <code>Set up landing zone</code> and wait ~60 minutes for the Control Tower installation to complete</li> <li>Select <code>Add or register organizational units</code>, Click <code>Add an OU</code></li> <li>Type <code>Dev</code>, click <code>Add</code>, wait until the OU is finished provisioning (or it will error)</li> <li>Repeat step 9 for each OU (i.e. <code>Test</code>, <code>Prod</code>, <code>Central</code>, <code>Sandbox</code>)</li> <li>Select <code>Account factory</code>, Edit, Subnets: 0, Deselect all regions, click <code>Save</code></li> <li>In AWS Organizations, move the Management account from the <code>root</code> OU into the <code>Security</code> OU</li> </ol> </li> </ul> </li> <li>Verify:<ol> <li>AWS Organizations is enabled in <code>All features</code> mode<ul> <li>if required, navigate to AWS Organizations, click <code>Create Organization</code>, <code>Create Organization</code></li> </ul> </li> <li>Service Control Policies are enabled<ul> <li>if required, in Organizations, select <code>Policies</code>, <code>Service control policies</code>, <code>Enable service control policies</code></li> </ul> </li> </ol> </li> <li>Verify the Organization Management (root) account email address<ul> <li>In AWS Organizations, Settings, \"Send Verification Request\"</li> <li>Once it arrives, complete the validation by clicking the validation link in the email</li> </ul> </li> <li>Create a new KMS key to encrypt your source configuration bucket (you can use an existing key)<ul> <li>AWS Key Management Service, Customer Managed Keys, Create Key, Symmetric, and then provide a key name (<code>ASEA-Source-Bucket-Key</code>), Next</li> <li>Select a key administrator (Admin Role or Group for the Organization Management account), Next</li> <li>Select key users (Admin Role or Group for the Organization Management account), Next</li> <li>Validate an entry exists to \"Enable IAM User Permissions\" (critical step if using an existing key)<ul> <li><code>\"arn:aws:iam::123456789012:root\"</code>, where <code>123456789012</code> is your Organization Management account ID.</li> </ul> </li> <li>Click Finish</li> <li>Select the new key, Select <code>Key Rotation</code>, <code>Automatically rotate this CMK every year</code>, click Save.</li> </ul> </li> <li>Enable <code>\"Cost Explorer\"</code> (My Account, Cost Explorer, Enable Cost Explorer)<ul> <li>With recent platform changes, Cost Explorer may now be auto-enabled (unable to confirm)</li> </ul> </li> <li>Enable <code>\"Receive Billing Alerts\"</code> (My Account, Billing Preferences, Receive Billing Alerts)</li> <li> <p>It is extremely important that all the account contact details be validated in the Organization Management (root) account before deploying any new sub-accounts.</p> <ul> <li>This information is copied to every new sub-account on creation.</li> <li>Subsequent changes to this information require manually updating it in each sub-account.</li> <li>Go to <code>My Account</code> and verify/update the information lists under both the <code>Contact Information</code> section and the <code>Alternate Contacts</code> section.</li> <li>Please ESPECIALLY make sure the email addresses and Phone numbers are valid and regularly monitored. If we need to reach you due to suspicious account activity, billing issues, or other urgent problems with your account - this is the information that is used. It is CRITICAL it is kept accurate and up to date at all times.</li> </ul> </li> </ol>"},{"location":"installation/install/#142-create-github-personal-access-token-and-store-in-secrets-manager","title":"1.4.2. Create GitHub Personal Access Token and Store in Secrets Manager","text":"<p>As of v1.5.0, the Accelerator offers deployment from either GitHub or CodeCommit:</p> <p>GitHub (recommended)</p> <ol> <li>You require a GitHub access token to access the code repository</li> <li>Instructions on how to create a personal access token are located here.</li> <li>Select the scope <code>public_repo</code> underneath the section <code>repo: Full control over private repositories</code>.</li> <li>Store the personal access token in Secrets Manager as plain text. Name the secret <code>accelerator/github-token</code> (case sensitive).<ul> <li>Via AWS console<ul> <li>Store a new secret, and select <code>Other type of secrets</code>, <code>Plaintext</code></li> <li>Paste your secret with no formatting no leading or trailing spaces (i.e. completely remove the example text)</li> <li>Select the key you created above (<code>ASEA-Source-Bucket-Key</code>),</li> <li>Set the secret name to <code>accelerator/github-token</code> (case sensitive)</li> <li>Select <code>Disable rotation</code></li> </ul> </li> </ul> </li> </ol> <p>CodeCommit (alternative option)</p> <p>Multiple options exist for downloading the GitHub Accelerator codebase and pushing it into CodeCommit. As this option is only for advanced users, detailed instructions are not provided.</p> <ol> <li>In your AWS Organization Management account, open CodeCommit and create a new repository named <code>aws-secure-environment-accelerator</code></li> <li>Go to GitHub and download the repository <code>Source code</code> zip or tarball for the release you wish to deploy<ul> <li>Do NOT download the code off the main GitHub branch, this will leave you in a completely unsupported state (and with beta code)</li> </ul> </li> <li>Push the extracted codebase into the newly created CodeCommit repository, maintaining the file/folder hierarchy</li> <li>Set the default CodeCommit branch for the new repository to main</li> <li>Create a branch following the Accelerator naming format for your release (i.e. <code>release/v1.5.5</code>)</li> </ol>"},{"location":"installation/install/#143-aws-internal-employee-accounts-only","title":"1.4.3. AWS Internal (Employee) Accounts Only","text":"<p>If deploying to an internal AWS employee account and installing the solution with a 3rd party firewall, you need to enable Private Marketplace (PMP) before starting:</p> <ol> <li>In the Organization Management account go here: https://aws.amazon.com/marketplace/privatemarketplace/create</li> <li>Click <code>Create a Private Marketplace</code>, and wait for activation to complete</li> <li>Go to the \"Account Groups\" sub-menu, click <code>Create account group</code></li> <li>Enter an Account Group Title (i.e. <code>Default</code>) and <code>Add</code> the Management (root) account number in <code>Associate AWS account</code></li> <li>Associate the default experience <code>New Private Marketplace</code>, then click <code>Create account group</code> and wait for it to create</li> <li>Go to \"Experiences\" sub-menu, select <code>New Private Marketplace</code></li> <li>Select the \"Settings\" sub-tab, and click the <code>Not Live</code> slider to make it <code>Live</code> and wait for it to complete</li> <li>Ensure the \"Software requests\" slider is set to <code>Requests off</code> and wait for it to complete</li> <li>Change the name field (i.e. append <code>-PMP</code>) and change the color, so it is clear PMP is enabled for users, click <code>Update</code></li> <li>Go to the \"Products\" sub-tab, then select the <code>All AWS Marketplace products</code> nested sub-tab</li> <li> <p>Search Private Marketplace for the Fortinet or Checkpoint products and select</p> <ul> <li><code>Fortinet FortiGate (BYOL) Next-Generation Firewall</code> and</li> <li><code>Fortinet FortiManager (BYOL) Centralized Security Management</code> or</li> <li><code>CloudGuard Network Security for Gateway Load Balancer - BYOL</code> and</li> <li><code>Check Point Security Management (BYOL)</code></li> </ul> </li> <li> <p>Select \"Add\" in the top right</p> <ul> <li>Due to PMP provisioning delays, this sometimes fails when attempted immediately following enablement of PMP or if adding each product individually - retry after 20 minutes.</li> </ul> </li> <li> <p>While not used in this account, you must now subscribe to the two subscriptions and accept the EULA for each product (you will need to do the same in the perimeter account, once provisioned below)</p> <ul> <li>To subscribe, select the \"Approved products\" tab</li> <li>Click on the product you want to subscribe, in this case <code>Fortinet FortiGate (BYOL) Next-Generation Firewall</code> and <code>Fortinet FortiManager (BYOL Centralized Security Management</code> or <code>CloudGuard Network Security for Gateway Load Balancer - BYOL</code> and <code>Check Point Security Management (BYOL)</code></li> <li>Click on \"Continue to Subscribe\"</li> <li>Click on \"Accept Terms\" and wait for subscription to be completed</li> <li>If you are deploying in any region except ca-central-1 or wish to switch to a different license type, you need the new AMI IDs. After successfully subscribing, continue one more step and click the \u201cContinue to Configuration\u201d. When you get the below screen, select your region and version (Fortinet <code>v6.4.7</code>, Checkpoint Mgmt <code>R81.10-335.883</code> and CloudGuard <code>R80.40-294.374</code> recommended at this time). Marketplace will provide the required AMI ID. Document the two AMI IDs, as you will need to update them in your config.json file below.</li> </ul> </li> </ol> <p></p>"},{"location":"installation/install/#15-basic-accelerator-configuration","title":"1.5. Basic Accelerator Configuration","text":"<ol> <li>Select a sample config file as a baseline starting point<ul> <li>IMPORTANT: Use a config file from the GitHub code branch you are deploying from, as valid parameters change over time. The <code>main</code> branch is NOT the current release and often will not work with the GA releases.</li> <li>sample config files can be found in this folder;</li> <li>descriptions of the sample config files and customization guidance can be found here;</li> <li>unsure where to start, use the <code>config.lite-CTNFW-example.json</code>, where CTNFW is for Control Tower w/NFW;</li> <li>These configuration files can be used, as-is, with only minor modification to successfully deploy the sample architectures;</li> <li>On upgrades, compare your deployed configuration file with the latest branch configuration file for any new or changed parameters;</li> </ul> </li> <li>At minimum, you MUST update the AWS account names and email addresses in the sample file:<ul> <li>For existing accounts, they must match identically to both the account names and email addresses defined in AWS Organizations (including the management account);</li> <li>For new accounts, they must reflect the new account name/email you want created;</li> <li>All new AWS accounts require a unique email address which has never before been used to create an AWS account;</li> <li>When updating the budget or SNS notification email addresses within the sample config, a single email address for all is sufficient;</li> <li>Update the IP address in the \"alarm-not-ip\" variable with your on-premise IP ranges (used for the AWS-SSO-Authentication-From-Unapproved-IP alarm);</li> <li>If deploying the Managed AD, update the dns-domain, netbios-domain, log-group-name, as well as the AD users and groups that will be created;</li> <li>For a test deployment, the remainder of the values can be used as-is;</li> <li>While it is generally supported, we recommend not adding more than 1 or 2 workload accounts to the config file during the initial deployment as it will increase risks of hitting a limit. Once the Accelerator is successfully deployed, add the additional accounts to the config file and rerun the state machine.</li> <li>More information here on the fields in the config file that need to be updated.</li> </ul> </li> <li>A successful deployment of the prescriptive architecture requires VPC access to 9 AWS endpoints, you cannot remove both the perimeter firewalls (all public endpoints) and the 9 required central VPC endpoints from the config file (ec2, ec2messages, ssm, ssmmessages, cloudformation, secretsmanager, kms, logs, monitoring).</li> <li>When deploying to regions other than <code>ca-central-1</code>, you need to modify your config file as follows (for Canada Central 1, the AMI IDs are pre-populated for you):<ol> <li>Update the firewall and firewall manager AMI IDs to reflect your home regions regional AMI IDs (see 2.3.3, item 13), making sure you select the right version and region per the recommendations.</li> <li>Validate all the Interface Endpoints defined in your config file are supported in your home region (i.e. Endpoint VPC). Remove unsupported endpoints from the config file, add additional endpoints as available.</li> <li>If you are installing into a home region which is explicitly named in any of the replacements\\addl_regions_x, remove it from the list. If deploying in us-east-1, remove ${GBL_REGION}.</li> </ol> </li> <li>Create an S3 bucket in your Organization Management account <code>your-bucket-name</code><ul> <li>you must supply this bucket name in the CFN parameters and in the config file (<code>global-options\\central-bucket</code>)</li> <li>the bucket name must be the same in both spots</li> <li>the bucket must have versioning enabled</li> <li>the bucket must be <code>S3-KMS</code> encrypted using the <code>ASEA-Source-Bucket-Key</code> created above</li> </ul> </li> <li>Place your customized config file(s), named <code>config.json</code> (or <code>config.yaml</code>), in your new bucket</li> <li>If required, place the firewall configuration and license files in the folder and path defined in the config file<ul> <li>For AWS Network Firewall: <code>nfw/nfw-example-policy.json</code></li> <li>For Fortinet: <code>firewall/firewall-example.txt</code>, <code>firewall/license1.lic</code> and <code>firewall/license2.lic</code><ul> <li>We have made a sample available here: <code>./reference-artifacts/Third-Party/</code></li> <li>the sample configures an active / active firewall pair with two tunnels per firewall</li> <li>If you updated your perimeter VPC subnet names, you must also make these changes in your firewall-example.txt file</li> <li>If you don't have any license files, update the config file with an empty array (<code>\"license\": []</code>). Do NOT use the following: <code>[\"\"]</code>.</li> </ul> </li> <li>The basic Checkpoint configuration is stored directly in config.json</li> </ul> </li> <li>Place any defined certificate files in the folder and path defined in the config file<ul> <li>i.e. <code>certs/example1-cert.key</code>, <code>certs/example1-cert.crt</code></li> <li>Sample available here: <code>./reference-artifacts/Certs-Sample/*</code></li> <li>Ideally you would generate real certificates using your existing certificate authority</li> <li>Should you wish, instructions are provided to aid in generating your own self-signed certificates (Self signed certificates are NOT secure and simply for demo purposes)</li> <li>Use the examples to demonstrate Accelerator TLS functionality only</li> </ul> </li> <li>Detach ALL SCPs (except <code>FullAWSAccess</code> which remains in place) from all OU's and accounts before proceeding<ul> <li>For Control Tower based installs do NOT detach Control Tower SCPs (i.e. aws-guardrails-xxxxxx)</li> <li>Installation will fail if this step is skipped</li> </ul> </li> </ol>"},{"location":"installation/install/#16-installation","title":"1.6. Installation","text":"<ol> <li>You can find the latest release in the repository here.<ul> <li>We only support new installations of v1.5.5 or above (older releases continue to function)</li> </ul> </li> <li>Download the CloudFormation (CFN) template for the release you plan to install (either <code>AcceleratorInstallerXXX.template.json</code> for GitHub or <code>AcceleratorInstallerXXX-CodeCommit.template.json</code> for CodeCommit)</li> <li>Use the provided CloudFormation template to deploy a new stack in your Management (root) AWS account<ul> <li>As previously stated we do not support installation in sub-accounts</li> </ul> </li> <li>Login to your Organization Management account and make sure you are in your desired <code>home</code> region (i.e. <code>ca-central-1</code>) (your desired primary or control region)</li> <li>Navigate to CloudFormation in the AWS Console and click <code>Create stack with new resources (standard)</code>, then<ul> <li>Select \"Template is ready\"</li> <li>For the \"Specify template\" select \"Upload a template file\"</li> <li>Select the <code>*.template.json</code> file you downloaded in step 2 above</li> <li>Click Next</li> </ul> </li> <li>Fill out the required parameters - LEAVE THE DEFAULTS UNLESS SPECIFIED BELOW<ul> <li>Specify <code>Stack Name</code> STARTING with <code>ASEA-</code> (case sensitive) suggest a suffix of <code>orgname</code> or <code>username</code></li> <li>Change <code>ConfigS3Bucket</code> to the name of the bucket you created above <code>your-bucket-name</code></li> <li>Add an <code>Email</code> address to be used for State Machine Status notification</li> <li>The <code>GitHub Branch</code> should point to the release you selected<ul> <li>if upgrading, change it to point to the desired release</li> <li>the latest stable branch is currently <code>release/v1.5.5</code>, case sensitive</li> <li>click <code>Next</code></li> </ul> </li> </ul> </li> <li>Finish deploying the stack<ul> <li>Apply a tag on the stack, Key=<code>Accelerator</code>, Value=<code>ASEA</code> (case sensitive).</li> <li>ENABLE STACK TERMINATION PROTECTION under <code>Stack creation options</code></li> <li>Click <code>Next</code>, Acknowledge resource creation, and click <code>Create stack</code></li> <li>The stack typically takes under 5 minutes to deploy.</li> </ul> </li> <li>Once deployed, you should see a CodePipeline project named <code>ASEA-InstallerPipeline</code> in your account. This pipeline connects to GitHub, pulls the code from the prescribed branch and deploys the Accelerator state machine.<ul> <li>if the CloudFormation fails to deploy with an <code>Internal Failure</code>, or, if the pipeline fails connecting to GitHub, then:<ul> <li>fix the issue with your GitHub secret created in section 2.3.2, then delete the Installer CloudFormation stack you just deployed, and restart at step 3 of this section.</li> </ul> </li> </ul> </li> <li>For new stack deployments, when the stack deployment completes, the Accelerator state machine will automatically execute (in Code Pipeline). When upgrading you must manually <code>Release Change</code> to start the pipeline.</li> <li>While the pipeline is running:<ul> <li>review the list of Known Installation Issues in the section below</li> <li>review the Accelerator Basic Operation and Frequently Asked Questions (FAQ) Document</li> </ul> </li> <li>Once the pipeline completes (~10 mins), the main state machine, named <code>ASEA-MainStateMachine_sm</code>, will start in Step Functions</li> <li>The state machine time is dependent on the quantity of resources being deployed. On an initial installation of a more complex sample configuration files, it takes approximately 2 hours to execute (depending on the configuration file). Timing for subsequent executions depends entirely on what resources are changed in the configuration file, but often takes as little as 20 minutes.<ul> <li>While you can watch the state machine in Step Functions, you will also be notified via email when the State Machine completes (or fails). Successful state machine executions include a list of all accounts which were successfully processed by the Accelerator.</li> </ul> </li> <li>The configuration file will be automatically moved into Code Commit (and deleted from S3). From this point forward, you must update your configuration file in CodeCommit.</li> <li>You will receive an email from the State Machine SNS topic and the 3 SNS alerting topics. Please confirm all four (4) email subscriptions to enable receipt of state machine status and security alert messages. Until completed, you will not receive any email messages (must be completed within 7-days).</li> <li> <p>If the state machine fails:</p> <ul> <li>Refer to the Troubleshooting Guide for instructions on how to inspect and retrieve the error</li> <li>You can also refer to the FAQ and Known Installation Issues</li> <li>Once the error is resolved, re-run the step function <code>ASEA-MainStateMachine_sm</code> using <code>{\"scope\": \"FULL\",\"mode\": \"APPLY\"}</code> as input</li> </ul> </li> <li> <p>If deploying a prescriptive architecture with 3rd party firewalls, after the perimeter account is created in AWS Organizations, but before the Accelerator reaches Stage 2:</p> <ol> <li>NOTE: If you miss the step, or fail to execute it in time, no need to be concerned, you will simply need to re-run the main state machine (<code>ASEA-MainStateMachine_sm</code>) to deploy the firewall (no SM input parameters required)</li> <li>Login to the perimeter sub-account (Assume your <code>organization-admin-role</code>)</li> <li>Activate the 3rd party vendor firewall and firewall manager AMI's in the AWS Marketplace<ul> <li>Navigate back to your private marketplace</li> <li>Note: Employees should see the private marketplace, including the custom color specified in prerequisite step 4 above.</li> <li>Select \"Discover products\" from the side bar, then in the \"Refine Results\" select \"Private Marketplace =&gt; Approved Products\"</li> <li>Subscribe and Accept the Terms for each product (firewall and firewall manager)</li> <li>When complete, you should see the marketplace products as subscriptions in the Perimeter account:</li> </ul> </li> </ol> <p></p> </li> <li> <p>If deploying the prescriptive architecture, once the main state machine (<code>ASEA-MainStateMachine_sm</code>) completes successfully, confirm the status of your perimeter firewall deployment</p> <ul> <li>If you have t2.micro ec2 instances running in any account which had the account-warming flag set to true, they will be removed on the next state machine execution;</li> <li>If your perimeter firewalls were defined but not deployed on first run, you will need to rerun the state machine. This happens when:<ol> <li>you were unable to activate the firewall AMI's before stage 2 (step 16)</li> <li>we were not able to fully activate your account before we were ready to deploy your firewalls. This case can be identified by a running EC2 micro instance in the account, or by looking for the following log entry 'Minimum 15 minutes of account warming required for account'.</li> <li>In these cases, simply select the <code>ASEA-MainStateMachine_sm</code> in Step Functions and select <code>Start Execution</code> (no SM input parameters required)</li> </ol> </li> </ul> </li> </ol>"},{"location":"installation/install/#161-known-installation-issues","title":"1.6.1. Known Installation Issues","text":"<p>Current Issues:</p> <ul> <li> <p>If dns-resolver-logging is enabled, VPC names containing spaces are not supported at this time as the VPC name is used as part of the log group name and spaces are not supported in log group names. By default in many of the sample config files, the VPC name is auto-generated from the OU name using a variable. In this situation, spaces are also not permitted in OU names (i.e. if any account in the OU has a VPC with resolver logging enabled and the VPC is using the OU as part of its name)</p> </li> <li> <p>On larger deployments we are occasionally seeing state machine failures when <code>Creating Config Recorders</code>. Simply rerun the state machine with the input of <code>{\"scope\": \"FULL\", \"mode\": \"APPLY\"}</code>.</p> </li> <li> <p>Occasionally CloudFormation fails to return a completion signal. After the credentials eventually fail (1 hr), the state machine fails. Simply rerun the state machine with the input of <code>{\"scope\": \"FULL\", \"mode\": \"APPLY\"}</code></p> </li> <li> <p>If the State Machine fails on an initial execution of NEW-ACCOUNTS, it must be re-run to target the failed accounts (i.e. with <code>{\"scope\": \"FULL\", \"mode\": \"APPLY\"}</code>) or the new sub-accounts will fail to be properly guardrailed</p> </li> </ul> <p>Issues in Older Releases:</p> <ul> <li>New installs to releases prior to v1.5.5 are no longer supported.</li> <li>Upgrades to releases prior to v1.5.5 are no longer supported.</li> <li>Upgrades to v1.3.9 in preparation for an upgrade to v1.5.5 may be possible with manual workarounds.</li> <li> <p>FROM 2022-08-07 to 2022-10-12: An issue with the version of cfn-init in the \"latest\" AWS standard Windows AMI will cause the state machine to fail during a new installation when deploying an RDGW host. RDGW hosts in existing deployments will fail to fully initialize if the state machine is or has been recently run and the auto-scaling group subsequently refreshes the host (default every 7 days).</p> <ul> <li>To temporarily workaround this issue, assume an administrative role in your <code>operations</code> account, open Systems Manager Parameter Store, and <code>Create parameter</code> with a Name of <code>/asea/windows-ami</code> and a value of <code>ami-0d336ea070bc06fb8</code> (which is the previous good AMI in ca-central-1), accepting the other default values. Update your config file to point to this new parameter by changing <code>image-path</code> (under \\deployments\\mad) to <code>/asea/windows-ami</code> instead of <code>/aws/service/ami-windows-latest/Windows_Server-2016-English-Full-Base</code>. Rerun your state machine. If you have an existing RDGW instance it should be terminated to allow the auto-scaling group to redeploy it. In other regions you will need to lookup the previous working ami-id (you cannot use <code>ami-0d336ea070bc06fb8</code>)</li> <li>This issue was resolved with the 2022-10-12 Windows AMI release. Customers that implemented this workaround must revert the above config file entry and rerun their state machines (the above AMI has been deprecated).</li> </ul> </li> </ul>"},{"location":"installation/install/#17-post-installation","title":"1.7. Post-Installation","text":"<p>The Accelerator installation is complete, but several manual steps remain:</p> <ol> <li> <p>Enable and configure AWS SSO in your <code>home</code> region (i.e. ca-central-1)</p> <ul> <li> <p>NOTE: AWS SSO has been renamed to AWS IAM Identity Center (IdC). The IdC GUI has also been reworked. The below steps are no longer click-by-click accurate. An update to the below documentation is planned, which will also include instructions to delegate AWS IdC administration to the Operations account enabling connecting IdC directly to MAD, rather than through an ADC.</p> </li> <li> <p>Login to the AWS Console using your Organization Management account</p> </li> <li>Navigate to AWS Single Sign-On, click <code>Enable SSO</code></li> <li>Set the SSO directory to AD (\"Settings\" =&gt; \"Identity Source\" =&gt; \"Identity Source\" =&gt; click <code>Change</code>, Select Active Directory, and select your domain from the list)</li> <li>Under \"Identity Source\" section, Click <code>Edit</code> beside \"Attribute mappings\", then set the <code>email</code> attribute to: <code>${dir:email}</code> and click <code>Save Changes</code></li> <li>Configure Multi-factor authentication, we recommend the following minimum settings:<ul> <li>Every time they sign in (always-on)</li> <li>Security key and built-in authenticators</li> <li>Authenticator apps</li> <li>Require them to provide a one-time password sent by email to sign in</li> <li>Users can add and manage their own MFA devices</li> </ul> </li> <li>Create all the default permission sets and any desired custom permission sets<ul> <li>e.g. Select <code>AWS accounts</code> from the side bar, select \"Permission sets\" tab then <code>Create permission set</code><ul> <li><code>Use an existing job function policy</code> =&gt; Next</li> <li>Select job function policy <code>AdministratorAccess</code></li> <li>Add Tags, Review and Create</li> <li>repeat for each default permission set and any required custom permission sets</li> </ul> </li> </ul> </li> <li>For Control Tower based installations, remove the orphaned Permission Sets from each AWS accounts (select the account, expand Permission Sets, click Remove for each)</li> <li>Map MAD groups to permission sets and accounts<ul> <li>Select <code>AWS accounts</code> from the side bar and select <code>AWS organization</code> tab</li> <li>Select the accounts you want to map to each MAD group and click <code>Assign users</code></li> <li>Select your DNS domain e.g. <code>example.local</code>, and search for the group you would like to assign (e.g. <code>aws-</code> for the pre-created groups) and click <code>Search connected directory</code></li> <li>Select the desired group <code>aws-log-archive-View</code></li> <li>Select the permission set you would like to assign to the MAD group to (e.g. <code>ViewOnlyAccess</code>)</li> <li>Click <code>Finish</code> (Note: if it fails during provisioning, simply select the failed accounts and click on \"Retry changes\")</li> </ul> </li> <li>AWS SSO should be the primary mechanism for all access to all AWS accounts in your Organization, to determine or update the login page for your organization:<ul> <li>Click on <code>Dashboard</code> within the AWS SSO console and note the <code>User portal URL</code></li> <li>Share this url with all your users</li> <li>NOTE: the url prefix can only be changed ONCE (ever) using the settings tab, so update with caution</li> </ul> </li> <li>Any pre-created AWS MAD users passwords are available in secrets manager in the AWS management account. To reset these passwords login to the Operations account through AWS SSO, then:<ul> <li>Navigate to \"Directory Service\" , select the directory =&gt; Actions =&gt; Reset user password</li> <li>Users can change their passwords from any MAD domain connected instance</li> </ul> </li> </ul> </li> <li> <p>Configure the new alb-forwarding feature (added in v1.5.0)</p> <ul> <li>AWS ALB's are published using DNS names which resolve to backing IP's which could silently change at any time due to a scaling event, maintenance, or a hardware failure. While published as a DNS name, ALB's can only target IP addresses. This presents a challenge as we need the ALB's in the perimeter account to target ALB's in the various back-end workload accounts;</li> <li>Alb-forwarding solves this problem by executing a small snippet of code every 60 seconds which updates managed ALB listeners with any IP changes, ensuring any managed flows do not go offline. This removes the requirement to leverage a 3rd party appliance to perform NAT to a DNS name;</li> <li>Alb-forwarding is enabled on the perimeter VPC in the perimeter account in many of the sample configs and can be used with Accelerator created or customer created ALB's. This setting causes a new DynamoDB table named <code>ASEA-Alb-Ip-Forwarding-vpc-*</code> to be created in the perimeter account. Additionally the parameter store in the perimeter account contains the details of all Accelerator deployed ALB's across the environment for easy reference;</li> <li>Steps to configure:<ul> <li>First you need to manually create a listener on the front-end ALB (without a target group), multiple listeners are supported;</li> <li>Next, for each application that needs to be published, a record needs to be added to the DynamoDB table, see sample below;</li> <li>Records can be added to the table for any ALB in the account running the alb-forwarding tool. Records can be added at any time. DDB change logs will trigger the initial creation of the appropriate target group(s) and IP addresses will be verified and updated every 60 seconds thereafter.</li> </ul> </li> </ul> <p>Sample DynamoDB JSON to add an entry to the table: <pre><code>{\n\"id\": \"App1\",\n\"targetAlbDnsName\": \"internal-Core-mydevacct1-alb-123456789.ca-central-1.elb.amazonaws.com\",\n\"targetGroupDestinationPort\": 443,\n\"targetGroupProtocol\": \"HTTPS\",\n\"vpcId\": \"vpc-0a6f44a80514daaaf\",\n\"rule\": {\n\"sourceListenerArn\": \"arn:aws:elasticloadbalancing:ca-central-1:123456789012:listener/app/Public-DevTest-perimeter-alb/b1b12e7a0c412bf3/ef9b022a4fdd8bdf\",\n\"condition\": {\n\"paths\": [\"/img/*\", \"/myApp2\"],\n\"hosts\": [\"aws.amazon.com\"],\n\"priority\": 30\n}\n}\n}\n</code></pre> <pre><code>-   where `id` is any unique text, `targetAlbDnsName` is the DNS address for the backend ALB for this application (found in parameter store), `vpcId` is the vpc ID containing the front-end ALB (in this account), `sourceListenerArn` is the arn of the listener of the front-end ALB, `paths` and `hosts` are both optional, but one of the two must be supplied. Finally, `priority` must be unique and is used to order the listener rules. Priorities should be spaced at least 40 apart to allow for easy insertion of new applications and forwarder rules.\n\n-   the provided `targetAlbDnsName` must resolve to addresses within a [supported](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html) IP address space.\n</code></pre> <li> <p>On a per role basis, you need to enable the CWL Account Selector in the Security and the Operations accounts, in each account:</p> <ul> <li>Go to CloudWatch, Settings, Under <code>Cross-account cross-region</code> select <code>Configure</code>, Under <code>View cross-account cross-region</code> select <code>Edit</code>, choose <code>AWS Organization account selector</code>, click <code>Save changes</code></li> </ul> </li> <li> <p>Configure central Ingress/Egress firewalls, if deployed</p> <ul> <li>Layer 3/4 <code>appliance</code> based inspection is an optional feature</li> </ul> <p>General</p> <ul> <li>If deployed, login to any 3rd party firewalls and firewall manager appliances and update any default passwords;</li> <li>Tighten security groups on the 3rd party firewall instances (using the Accelerator configuration file), further limiting access to firewall management interfaces to a set of designated and controlled CIDR ranges;</li> <li>Update the firewall configuration per your organization's security requirements and best practices;</li> <li>Diagrams reflecting perimeter traffic flows when NFW and/or GWLB are used can be found here on slides 6 through 9.</li> </ul> <p>AWS Network Firewall</p> <ul> <li>The AWS Network Firewall policies and rules deployed by the Accelerator, can only be updated using the Accelerator. Customers wishing to manage the AWS Network Firewall from the console GUI, must create a new policy with new rules created through the console and then manually associate this new policy to the Accelerator deployed Network Firewall. Customers can choose either option, but they cannot be mixed to ensures that Accelerator updates do not overwrite console based updates.</li> </ul> <p>Fortinet</p> <ul> <li>Manually update firewall configuration to forward all logs to the Accelerator deployed NLB addresses fronting the rsyslog cluster<ul> <li>login to each firewall, select <code>Log Settings</code>, check <code>Send logs to syslog</code>, put the NLB FQDN in the <code>IP Address/FQDN</code> field (stored in parameter store of perimeter account)</li> </ul> </li> <li>Manually update the firewall configuration to connect perimeter ALB high port flows through to internal account ALB's<ul> <li>Note: while this option is still available, a new alb-forwarding mechanism is available in v1.5.0+ (see section 2 above) which simplifies and eliminates this more complicated \"NAT to DNS name\" option;</li> <li>login to each firewall, switch to <code>FG-traffic</code> vdom, select <code>Policies &amp; Objects</code>, select <code>Addresses</code>, Expand <code>Addresses</code><ul> <li>Set <code>Prod1-ALB-FQDN</code> to point to a reliable sub-account ALB FQDN, this is used for full-path health checks on all ALB's</li> <li>Set additional <code>DevX-ALB-FQDN</code>, <code>TestX-ALB-FQDN</code> and <code>ProdX-ALB-FQDN</code> to point to workload account ALB FQDNs<ul> <li>Two of each type of ALB FQDN records have been created, when you need more, you need to create BOTH an additional FQDN and a new VIP, per ALB</li> <li>Each new VIP will use a new high port (i.e. 7007, 7008, etc.), all of which map back to port 443</li> </ul> </li> <li>Detailed steps can be read here.</li> </ul> </li> </ul> </li> </ul> <p>Checkpoint</p> <ul> <li>Download and install Checkpoint SmartConsole client on your PC (only available for Windows, if you have a Mac, you can use Workspaces to install the client)</li> <li>Retrieve the firewall and firewall manager SSL certificate from secrets manager in the perimeter account and save to a pem file (convert to ppk on Windows)<ul> <li>Login to the <code>AWS Console</code> for the <code>Perimeter</code> account</li> <li>Navigate to <code>Secrets Manager</code> and select <code>Secrets</code></li> <li>Look for Secret name <code>ASEA/keypair/ASEA-Appliances</code> and click on it</li> <li>Go to the <code>Secret value</code> section and click on <code>Retrieve secret value</code></li> <li>Make sure you have the <code>Plaintext</code> tab selected and copy the content to a .pem file using your preferred text editor. (the file will need to be converted to ppk format on Windows)</li> </ul> </li> <li>Wait approx. 25 min. after the managers \"Launch time\" and then SSH into the Firewall Manager using the SSL certificate (.pem file retrieved above) and login user <code>admin</code><ul> <li>The firewall manager EC2 instance name is <code>ASEA-Checkpoint-FirewallMgr</code></li> <li>Once you SSH successfully, execute the following commands:<ul> <li><code>set user admin password</code></li> <li><code>set expert-password</code></li> <li><code>set user admin shell /bin/bash</code></li> <li><code>save config</code></li> </ul> </li> </ul> </li> <li>The following commands are useful for troubleshooting (in expert mode):<ul> <li><code>autoprov_cfg -v</code> (check cme at Take 155 or greater)</li> <li><code>autoprov_cfg show all</code> (check cme configuration)</li> <li><code>cat /var/log/aws-user-data.log</code> (validate bootstrap, file should end with <code>\"Publish operation\" succeeded (100%)</code>)</li> <li><code>tail -f /var/log/CPcme/cme.log</code> (watch to ensure it finds the instances, establishes SIC and adds the nodes)</li> </ul> </li> <li>Login to SmartConsole, and update the firewall policy per your organizations security requirements<ul> <li>An outbound rule allowing http and https should exist</li> <li>From the RDGW host in Operations, test to see if outbound web browsing is enabled</li> </ul> </li> <li>NOTES:<ul> <li>No best practice or security configuration has been configured on the Checkpoint firewalls. These firewalls have been configured to work with GWLB, but otherwise have the default/basic Checkpoint out-of-box configuration installed</li> <li>Do NOT reboot the Checkpoint appliances until bootstrap is complete (~25 minutes for the manager), or you will be required to redeploy the instance</li> </ul> </li> </ul> </li> <li> <p>Recover root passwords for all sub-accounts and apply strong passwords</p> <ul> <li>Process documented here</li> </ul> </li> <li> <p>Enable MFA for all IAM users and all root account users, recommendations:</p> <ul> <li>Yubikeys provide the strongest form of MFA protection and are strongly encouraged for all account root users and all IAM users in the Organization Management (root) account</li> <li>the Organization Management (root) account requires a dedicated Yubikey (if access is required to a sub-account root user, we do not want to expose the Organization Management accounts Yubikey)</li> <li>every ~50 sub-accounts requires a dedicated Yubikey (minimize the required number of Yubikeys and the scope of impact should a Yubikey be lost or compromised)</li> <li>each IAM breakglass user requires a dedicated Yubikey, as do any additional IAM users in the Organization Management (root) account. While some CSPs do not recommend MFA on the breakglass users, it is strongly encouraged in AWS</li> <li>all other AWS users (AWS SSO, IAM in sub-accounts, etc.) can leverage virtual MFA devices (like Google Authenticator on a mobile device)</li> </ul> </li> <li> <p>Customers are responsible for the ongoing management and rotation of all passwords on a regular basis per their organizational password policy. This includes the passwords of all IAM users, MAD users, firewall users, or other users, whether deployed by the Accelerator or not. We do NOT automatically rotate any passwords, but strongly encourage customers do so, on a regular basis.</p> </li> <li> <p>During the installation we request required limit increases, resources dependent on these limits will not be deployed</p> <ol> <li>Limit increase requests are controlled through the Accelerator configuration file <code>\"limits\":{}</code> setting</li> <li>The sample configuration file requests increases to your EIP count in the perimeter account and to the VPC count and Interface Endpoint count in the shared-network account</li> <li>You should receive emails from support confirming the limit increases</li> <li>On the next state machine execution, resources blocked by limits should be deployed (i.e. additional VPC's and Endpoints)</li> <li>If more than 2 days elapses without the limits being increased, on the next state machine execution, they will be re-requested</li> </ol> </li> <li> <p>Note: After a successful install the Control Tower <code>Organizational units'</code> dashboard will indicate <code>2 of 3</code> in the <code>Accounts enrolled</code> column for the Security OU, as it does not enable enrollment of the management account in guardrails. The Accelerator compliments Control Tower and enables guardrails in the management account which is important to high compliance customers.</p> </li>"},{"location":"installation/install/#18-other-operational-considerations","title":"1.8. Other Operational Considerations","text":"<ul> <li>The Organization Management (root) account does NOT have any preventative controls to protect the integrity of the Accelerator codebase, deployed objects or guardrails. Do not delete, modify, or change anything in the Organization Management (root) account unless you are certain as to what you are doing. More specifically, do NOT delete, or change any buckets in the Organization Management (root) account.</li> <li>While generally protected, do not delete/update/change S3 buckets with cdk-asea-, or asea- in any sub-accounts.</li> <li>ALB automated deployments only supports Forward and not redirect rules.</li> <li>AWS generally discourages cross-account KMS key usage. As the Accelerator centralizes logs across an entire organization as a security best practice, this is an exception/example of a unique situation where cross-account KMS key access is required.</li> <li>Only 1 auto-deployed MAD in any mandatory-account is supported today.</li> <li>VPC Endpoints have no Name tags applied as CloudFormation does not currently support tagging VPC Endpoints.</li> <li>If the Organization Management (root) account coincidentally already has an ADC with the same domain name, we do not create/deploy a new ADC. You must manually create a new ADC (it won't cause issues).</li> <li>3rd party firewall updates are to be performed using the firewall OS based update capabilities. To update the AMI using the Accelerator, you must first remove the firewalls and then redeploy them (as the EIP's will block a parallel deployment), or deploy a second parallel FW cluster and de-provision the first cluster when ready.</li> <li>When adding more than 100 accounts to an OU which uses shared VPC's, you must first increase the Quota <code>Participant accounts per VPC</code> in the shared VPC owner account (i.e. shared-network). Trapping this quota before the SM fails has been added to the backlog.</li> <li>The default limit for Directory Sharing is 125 accounts for an Enterprise Managed Active Directory (MAD), a quota increase needs to be manually requested through support from the account containing the MAD before this limit is reached. Standard MAD has a sharing limit of 5 accounts (and only supports a small quota increase). The MAD sharing limit is not available in the Service Quota's tools.</li> </ul>"},{"location":"installation/log-file-locations/","title":"1. Accelerator Central Logging Implementation and File Structures","text":"<p>The following diagram details the ASEA central logging implementation:</p> <p></p>"},{"location":"installation/log-file-locations/#11-accelerator-central-logging-buckets","title":"1.1. Accelerator Central Logging Buckets","text":"Bucket Type Bucket Name Purpose AES Encrypted Bucket pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4cdwuxu ALB Logs - ALB's do not support logging to a KMS bucket KMS Encrypted Bucket pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo All other AWS Accelerator initiated logs AES or KMS Encrypted aws-controltower-logs-123456789012-ca-central-1 All Control Tower initiated logs AES or KMS Encrypted aws-controltower-s3-access-logs-123456789012-ca-central-1 S3 Access logs for the Control Tower logs bucket"},{"location":"installation/log-file-locations/#111-notes","title":"1.1.1. Notes","text":"<ul> <li>Every customer has two Accelerator logging buckets</li> <li>Control Tower installations have an additional two Control Tower logging buckets</li> <li>Customers could use any account name for their central logging account</li> <li>Bucket name format is: {Accel-Prefix}-{Account-Name}-{Accel-Phase}-xxx{Region}-{Random}<ul> <li>{Accel-Prefix} defaults to 'asea' (previously 'pbmmaccel' for Canada)</li> <li>{Accel-Phase} should always be 'phase0'</li> <li>{region} should always be 'cacentral1' for Canada</li> <li>{account} is likely to be 'log-archive'</li> <li>xxx is either \"aes\" or \"\" (nothing)</li> </ul> </li> </ul>"},{"location":"installation/log-file-locations/#12-accelerator-bucket-folders","title":"1.2. Accelerator Bucket Folders","text":"Log Type Folder Path Example ELB (in AES bucket) {account#}/elb-{elbname}/AWSLogs/{account#}/* <ul><li> s3://pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4ucwuxu/123456789012/elb-Core-mydevacct1-alb/AWSLogs/123456789012/ELBAccessLogTestFile </li></ul><ul><li>s3://pbmmaccel-logarchive-phase0-aescacentral1-1py9vr4ucwuxu/123456789013/elb-Public-Prod-perimeter-alb/AWSLogs/123456789013/ELBAccessLogTestFile </li></ul> VPC Flow Logs {account#}/{vpc-name}/AWSLogs/{account#}/vpcflowlogs/{region}/{year}/{month}/{day}/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789012/Test-East-lcl/AWSLogs/123456789012/vpcflowlogs/us-east-1/2020/08/31/123456789012_vpcflowlogs_us-east-1_fl-04af3543c74402594_20200831T1720Z_73d3922a.log.gz </li></ul> Macie Reports {account#}/macietestobject <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789014/macie-test-object </li></ul> Cost and Usage Reports {account#}/cur/Cost-and-Usage-Report/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/123456789015/cur/Cost-and-Usage-Report/* </li></ul> Config History* AWSLogs/{account#}/Config/{region}/{year}/{month}/{day}/ConfigHistory/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789016/Config/ca-central-1/2020/8/31/ConfigHistory/123456789016_Config_ca-central-1_ConfigHistory_AWS::CloudFormation::Stack_20200831T011226Z_20200831T025845Z_1.json.gz </li></ul> Config Snapshot* AWSLogs/{account#}/Config/{region}/{year}/{month}/{day}/ConfigSnapshot/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789016/Config/ca-central-1/2020/8/30/ConfigSnapshot/123456789016_Config_ca-central-1_ConfigSnapshot_20200830T193058Z_5d173149-e6d0-41e4-af7f-031ff736f8c8.json.gz </li></ul> GuardDuty AWSLogs/{account#}/GuardDuty/{region}/{year}/{month}/{day}/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/AWSLogs/123456789014/GuardDuty/ca-central-1/2020/09/02/294c9171-4867-3774-9756-f6f6c209616f.jsonl.gz </li></ul> CloudWatch Logs**** CloudWatchLogs/{year}/{month}/{day}/{hour}/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/CloudWatchLogs/2020/08/30/00/PBMMAccel-Kinesis-Delivery-Stream-1-2020-08-30-00-53-33-35aeea4c-582a-444b-8afa-848567924094 </li></ul> CloudTrail Digest*** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail-Digest/{region}/{year}/{month}/{day}/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789016/CloudTrail-Digest/ca-central-1/2020/08/30/123456789016_CloudTrail-Digest_ca-central-1_PBMMAccel-Org-Trail_ca-central-1_20200830T190938Z.json.gz </li></ul> CloudTrail Insights** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail-Insights/{region}/{year}/{month}/{day}/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789015/CloudTrail-Insight/ca-central-1/2020/09/23/123456789015_CloudTrail-Insight_ca-central-1_20200923T0516Z_KL5e9VCV2SS7IqzB.json.gz </li></ul> CloudTrail*** {org-id}/AWSLogs/{org-id}/{account#}/CloudTrail/{region}/{year}/{month}/{day}/* <ul><li> s3://pbmmaccel-logarchive-phase0-cacentral1-1tr23emhncdzo/o-fxozgwu6rc/AWSLogs/o-fxozgwu6rc/123456789016/CloudTrail/ca-central-1/2020/08/30/123456789016_CloudTrail_ca-central-1_20200830T0115Z_3YQJxwt5qUaOzMtL.json.gz </li></ul> CT S3 Access Logs {no folders} <ul><li> s3://aws-controltower-s3-access-logs-123456789012-ca-central-1/2021-04-26-18-11-21-8647E1080048E5CB </li></ul> SSM Inventory ssm-inventory/{ssm-inventory-type}/accountid={account#}/region={region}/resourcetype={rt}/* <ul><li> s3://asea-logarchive-phase0-cacentral1-1tr23emhncdzo/ssm-inventory/AWS:Application/accountid=123456789012/region=ca-central-1/resourcetype=ManagedInstanceInventory/i-001188b4e152aecaf.json"},{"location":"installation/log-file-locations/#121-notes","title":"1.2.1. Notes","text":"<p>* Located in Control Tower bucket when installed, Control Tower adds the {org-id} (i.e. o-h9ho05hcxl/) as the top level folder</p> <p>** Only available in Accelerator Standalone deployments</p> <p>*** CloudTrail control plane logs located in Control Tower bucket when installed, Control Tower drops the {org-id} (i.e. o-h9ho05hcxl/) from the middle of the folder path. This may change when Control Tower migrates to Organization Trails. CloudTrail data plane logs remain in the Accelerator bucket.</p> <p>**** v1.5.1 introduces the capability to split CloudWatch log groups starting with specific prefixes out into customer named subfolders. The folder/file structure is otherwise identical. The v1.5.1 example config files separate out MAD, RQL, Security Hub, NFW, rsyslog, and SSM logs by default. Example: Security Hub logs will be in the following structure: <code>CloudWatchLogs/security-hub/{year}/{month}/{day}/{hour}/</code></p> <ul> <li>Account number is sometimes duplicated in path because logs replicated from another account always need to start with the source account number</li> <li>Macie reports will only appear in the {account#} for the central security account, and only if a customer schedules PII discovery reports</li> <li>All CloudWatch Logs from all accounts are mixed in the same folder, the embedded log format contains the source account information as documented here: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/ValidateLogEventFlow.html</li> <li>With the exception of CloudWatch Logs, all logs are in the original format provided by the log source/service.</li> </ul>"},{"location":"installation/multi-file-config-capabilities/","title":"1. Multi-file Accelerator Config file and YAML Support Details","text":""},{"location":"installation/multi-file-config-capabilities/#11-customers-would-like-the-ability-to-specify-their-configuration-in-yaml-this-facilitates","title":"1.1. Customers would like the ability to specify their configuration in YAML. This facilitates","text":"<ul> <li>commenting out entire sections, which is unavailable in standard JSON</li> <li>annotating aspects of configuration (e.g. cidr: \"10.100.0.0/16\" # We chose this for \\$reason.)</li> <li>aligning the Accelerator with CloudFormation, which supports JSON/YAML as input format</li> </ul>"},{"location":"installation/multi-file-config-capabilities/#12-customers-would-like-the-configuration-file-split-into-multiple-files","title":"1.2. Customers would like the configuration file split into multiple files","text":"<ul> <li>one file for Global options + Mandatory accounts</li> <li>one file per OU</li> <li>one file for every approx. 2000 lines of workload accounts (Code Commit diff stops working at 3000 lines, allow for adding to each file)</li> </ul>"},{"location":"installation/multi-file-config-capabilities/#13-benefits","title":"1.3. Benefits","text":"<ol> <li>Easier cut/paste/comparison of OU configurations</li> <li>Allow CodeCommit diff functionality to function (File currently too large)</li> <li>Allow easier updates to workload accounts (simple append)</li> <li>Smaller scoped updates (de-risk accidentally changing the wrong section)</li> <li>Both a customer request and something the team thought was a good idea</li> </ol>"},{"location":"installation/multi-file-config-capabilities/#14-steps-for-yaml","title":"1.4. Steps FOR YAML","text":"<ul> <li>The loadAcceleratorConfig functionality should no longer assume config.json as the config filename in the config repo and/or S3, instead it should look for config.yaml and config.json</li> <li>Check for the existence of config.yaml and config.json (initially in S3, but also in CodeCommit on future executions)</li> <li>If both files exist, fail with an error message</li> <li>Infer the file type from the extension, and parse accordingly</li> <li>Any other failure should also be an error, fail with an error message</li> <li>The accelerator will continue to use JSON formatting internally, if a yaml file is supplied, we are simply converting it to JSON for use by the Accelerator</li> <li>All examples throughout this document use config.json as the example, but also apply to YAML</li> <li>Both JSON and YAML input files will be equally supported</li> <li>Only one file format is supported across all config files, either JSON or YAML, customers can NOT mix YAML and JSON file formats</li> </ul>"},{"location":"installation/multi-file-config-capabilities/#15-steps-for-file-split","title":"1.5. Steps For File Split","text":"<ul> <li>When the <code>__LOAD</code> keyword is encountered, search relatively (from the same location as root config file) for the file, and insert into the config tree, recursively following <code>__LOAD</code> if necessary (to max depth of 2). Any file referenced in <code>__LOAD</code> must parse successfully in one of the two formats, otherwise FAIL.</li> </ul> <pre><code>{\n\"core\": {\n\"__LOAD\": \"ous/core.json\"\n}\n}\n</code></pre> <p>Note that while we will provide sensible examples, there is no prescriptive requirement for file organization within a customer's configuration, customers can use the feature to break-out sections as is most effective for their deployment. Breaking out large repeatable sections like security groups is a good example and could be included off the main file, an account file, or off an ou file:</p> <pre><code>\"security-groups\": [ \"__LOAD\": \"global/security-groups.json\" ]\n</code></pre> <p>Examples:</p> <ol> <li>All in one (single file like today):</li> </ol> <pre><code>.\n\u251c\u2500\u2500 config.json\n</code></pre> <ol> <li>Split along major sections:</li> </ol> <pre><code>.\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 ous\n\u2502 \u251c\u2500\u2500 core.json\n\u2502 \u251c\u2500\u2500 dev.json\n\u2502 \u2514\u2500\u2500 test.json ---&gt; could be one per ou, could only be for some ou's as determined by customer\n\u251c\u2500\u2500 accounts\n\u2502 \u251c\u2500\u2500 workload-accounts-group1.json\n\u2502 \u251c\u2500\u2500 workload-accounts-group2.json\n\u2502 \u251c\u2500\u2500 my-workload-accounts.json\n\u2502 \u2514\u2500\u2500 more-accounts.json ----&gt;we will encourage each file being as close to 2000 lines as possible (not one per account, not all in one file)\n\u2514\u2500\u2500 global\n\u251c\u2500\u2500 global-options.json\n\u251c\u2500\u2500 security-groups.json\n\netc\n</code></pre> <ul> <li>Max depth of 2 means config.json can load ou/dev.json, which can load global/security-groups.json.</li> <li>security-groups.json CANNOT load another sub-file (unless security-groups.json was only directly loaded from config.json).</li> </ul>"},{"location":"installation/multi-file-config-capabilities/#16-dealing-with-accelerator-automatic-config-file-updates","title":"1.6. Dealing with Accelerator Automatic Config File Updates","text":"<p>When customers create AWS accounts directly through AWS Organizations, the Accelerator automatically updates the config file, adding these new accounts. If a customer renames an OU we automatically update the config file. With multi-part files, how do we know what source file to update? We require two mechanisms:</p> <ol> <li> <p>Add the following new parameters to the global-options section of the config file</p> <pre><code>    \"workloadaccounts-param-filename\": \"accounts/more-accounts2.json\",\n    \"workloadaccounts-prefix\" : \"accounts/more-accounts\",\n    \"workloadaccounts-suffix\" : 3,\n</code></pre> <ul> <li>filename is set to <code>config.json</code>, and prefix to <code>config</code> in a single file configuration scenario (suffix is not used)</li> <li>While OU contents can be moved into <code>__LOADED</code> sub-files, it was decided the OU object itself must remain in the main config file</li> <li>The above parameters:<ul> <li>are required to be in the main config file and cannot be <code>__LOAD</code>'ed</li> <li>Must be present or SM fails</li> <li>Are used to decide where to add new accounts to the config file</li> </ul> </li> </ul> </li> <li> <p>Add the following new parameter to each mandatory and workload account config</p> <pre><code>    \"src-filename\": \"accounts/my-workload-accounts.json\",\n</code></pre> </li> </ol>"},{"location":"installation/multi-file-config-capabilities/#17-accelerator-internal-operations","title":"1.7. Accelerator Internal Operations","text":"<ul> <li>when updating an account in the config file, we use the <code>\"src-filename\"</code> parameters to find and update an accounts <code>ou</code>, <code>ou-path</code>, <code>account-name</code>, and <code>email</code> parameters</li> <li>When creating new accounts (inserting into config file):<ul> <li>if the update is not going to make the file larger than 2000 lines, insert the new account into the config file <code>\"workloadaccounts-param-filename\"</code></li> <li>if the insert will push the file over 2000 lines:<ul> <li>create the next unused filename for the given prefix in Code Commit (<code>{\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format}</code>), i.e. <code>\"accounts/more-accounts3.json\"</code></li> <li>insert the new account into the new file in it's entirety</li> <li>update <code>\"workloadaccounts-param-filename\"</code> to: <code>{\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format}</code></li> <li>add a new load stmt to the workload-accounts section of the config file with the name <code>{\"workloadaccounts-prefix\"}{\"workloadaccounts-suffix\"}.{customer file format}</code></li> <li>update <code>\"workloadaccounts-suffix\"</code> to: <code>{\"workloadaccounts-suffix\"}</code> + <code>1</code></li> <li>be careful with comma's between files (JSON sections) when appending/connecting</li> </ul> </li> </ul> </li> </ul>"},{"location":"installation/multi-file-config-capabilities/#18-example","title":"1.8. Example","text":"<p>The entire main config file could be reduced to this:</p> <pre><code>{\n\"global-options\": {\n\"workloadaccounts-param-filename\": \"accounts/more-accounts2.json\",\n\"workloadaccounts-prefix\": \"accounts/more-accounts\",\n\"workloadaccounts-suffix\": 3,\n\"__LOAD\": \"global/global-options.json\"\n},\n\"mandatory-account-configs\": {\n\"__LOAD\": \"accounts/mandatory-accounts.json\"\n},\n\"workload-account-configs\": {\n\"__LOAD\": [\n\"accounts/workload-accounts1.json\",\n\"accounts/my-other-accounts.json\",\n\"accounts/workload-accounts2.json\"\n]\n},\n\"organizational-units\": {\n\"core\": {\n\"__LOAD\": \"ous/core.json\"\n},\n\"Central\": {\n\"__LOAD\": \"ous/central.json\"\n},\n\"Dev\": {\n\"__LOAD\": \"ous/dev.json\"\n},\n\"Test\": {\n\"__LOAD\": \"ous/test.json\"\n},\n\"Prod\": {\n\"__LOAD\": \"ous/prod.json\"\n},\n\"UnClass\": {\n\"__LOAD\": \"ous/unclass.json\"\n},\n\"Sandbox\": {\n\"__LOAD\": \"ous/sandbox.json\"\n}\n}\n}\n</code></pre>"},{"location":"installation/multi-file-config-capabilities/#19-acceptance-criteria","title":"1.9. Acceptance Criteria","text":"<ul> <li>A new customer may start an Accelerator deployment with a config.json or config.yaml, and have it deploy as expected so long as the file is semantically correct according to structure and expected keys (and of course syntactically correct in either YAML or JSON)</li> <li>Accelerator should continue to function as it does today     o i.e. on startup creates repo and copies all referenced config files, not just config.json to repo (json or YAML)     o leverages config files in CodeCommit repo from this point forward (json or YAML as provided by customer)     o SM runs against the commit id of each file at the start of the SM (i.e. don't allow changes to any file during execution)</li> <li>Accelerator leverages multiple config files to receive the same input parameters it previously did from one file</li> <li>All accelerator functionality both ALZ and Standalone versions continue to function as previously defined</li> <li>Customer can successfully provides multiple config files with the same result as the current one file</li> </ul>"},{"location":"installation/object-naming/","title":"Object Naming","text":""},{"location":"installation/object-naming/#accelerator-object-naming","title":"Accelerator Object Naming","text":"<ul> <li>Resources will have the 'Name' tag assigned, where Name={name}{suffix}<ul> <li>No prefix or suffix will be applied to DNS records/zones (as that breaks them)</li> <li>When _ is not supported, a - will be used</li> </ul> </li> <li>Stacks/stacksets/functions and non-end user accessed objects deployed in all accounts will also start with the {AcceleratorPrefix} prefix (i.e. \"PBMMAccel-\" or \"ASEA-\")<ul> <li>The prefix does not apply to objects like VPC's, subnets, or TGW's which customers need to directly access. This is for objects deployed to build the customer accessible objects</li> <li>This prefix will be protected by SCP's so customers don't break 'managed' features</li> </ul> </li> <li>Resources will have the tag 'Accelerator={AcceleratorName}' assigned when tags are supported</li> <li>Stacks will have the tag 'AcceleratorName={AcceleratorName}' assigned, which will often (but not always) be inherited by objects created by the stack (due to TGW duplicate tag issue)</li> </ul>"},{"location":"installation/object-naming/#defaults","title":"Defaults","text":"<pre><code>- the default {AcceleratorName} is 'PBMM' before v1.5.0 and 'ASEA' after v1.5.0\n- the default {AcceleratorPrefix} is 'PBMMAccel-' before v1.5.0 and 'ASEA-' after v1.5.0\n</code></pre>"},{"location":"installation/object-naming/#suffixs","title":"Suffix's","text":"suffix object type _vpc VPC _azN_net Subnet _azN_rt RouteTable _tgw Transit Gateway -key KMS key _pcx Peering Connection _sg Security Group _nacl NACL _alb Application Load Balancer _nlb Network Load Balancer _agw Appliance Gateway _vpce VPC Endpoint _AMI AMI _dhcp DHCP option set _snap snapshot _ebs Block storage _igw internet gateway _lgw Local gateway _nat NAT gateway _vpg Virtual private gateway _cgw Customer gateway _vpn VPN Connection _sm Step Functions _rule CW Event Rule _pl CodeBuild"},{"location":"installation/object-naming/#no-suffix","title":"No Suffix","text":"suffix object type None Stacks None CFN_Stack_Sets None Lambda None Cloud Trails None CWL Groups None Config Rules None OU None Service Control Policy"},{"location":"installation/services-list/","title":"Accelerator Service List","text":""},{"location":"installation/services-list/#services","title":"Services","text":"<p>This table indicates whether services are leveraged and/or orchestrated by the Accelerator.</p> CATEGORY SERVICE LEVERAGED ORCHESTRATED Compute AWS Lambda X Amazon Elastic Compute Cloud (EC2) X Monitoring &amp; Alerts Amazon CloudTrail X AWS Config X Amazon CloudWatch X X Amazon EventBridge X X Amazon Simple Notification Service (SNS) X AWS Budgets X Systems Manager Inventory X Infrastructure AWS CodeCommit X AWS CodeBuild X AWS CodePipeline X AWS CloudFormation X AWS Cloud Development Kit (CDK) / Software Development Kit (SDK) X AWS Step Functions X Amazon Kinesis Data Stream X Amazon Kinesis Data Firehose X Amazon Simple Queue Service (SQS) X Data Amazon Simple Storage Service (S3) X X Amazon DynamoDB X Amazon Elastic Container Registry (ECR) (incl. ECR Public) X Systems Manager Parameter Store X X AWS Secrets Manager X Networking Amazon Virtual Private Cloud (VPC) X AWS Transit Gateway X AWS PrivateLink X Elastic Load Balancer (ELB) (incl. ALB, NLB, GWLB) X Route53 X Route53 Resolver X Management AWS Organizations X X AWS Resource Access Manager (RAM) X AWS Identity and Access Management (IAM) X X AWS Single Sign-On (SSO) X AWS Directory Service (incl. AWS Managed AD and AD Connector) X AWS Control Tower X X AWS IAM Access Analyzer X AWS Cost and Usage Reports X AWS Service Quotas X Security AWS GuardDuty X AWS Security Hub X Amazon Macie X Systems Manager Automation X Systems Manager Session Manager X AWS Key Management Service (KMS) X X AWS Security Token Service (STS) X AWS Firewall Manager X AWS Network Firewall X AWS Certificate Manager (ACM) X Third-Party Fortinet FortiGate and FortiManager (Firewall &amp; Mgmt) X Checkpoint CloudGuard and Manager (Firewall &amp; Mgmt) X <code>rsyslog</code> on Amazon Linux 2 X Windows Remote Desktop Gateway Bastion X <p>If we missed a service, let us know!</p>"},{"location":"installation/sm_inputs/","title":"1. State Machine Behavior and Inputs","text":""},{"location":"installation/sm_inputs/#11-state-machine-behavior","title":"1.1. State Machine Behavior","text":"<p>Accelerator v1.3.0 makes a significant change to the manner in which the state machine operates. These changes include:</p> <ol> <li>Reducing the <code>default scope</code> of execution of the state machine to only target newly created AWS accounts and AWS accounts listed in the mandatory accounts section of the config file.<ul> <li><code>default scope</code> refers to running the state machine without any input parameters;</li> <li>This new default scope disallows any changes to the config file outside new accounts;</li> <li>NOTE: it is critical that accounts for which others are dependant upon, MUST be located within the <code>mandatory-account-configs</code> section of the config file (i.e. management, log-archive, security, operations, shared-network, perimeter, etc.).</li> </ul> </li> <li>The state machine now accepts a new input parameter, <code>scope</code>, which accepts the following values: <code>FULL</code> | <code>NEW-ACCOUNTS</code> | <code>GLOBAL-OPTIONS</code> | <code>ACCOUNT</code> | <code>OU</code>.<ul> <li>when the <code>scope</code> parameter is supplied, you must also supply the <code>mode</code> parameter. At this time <code>mode</code> only accepts the value <code>APPLY</code>. To be specific <code>\"mode\":\"APPLY\"</code> is mandatory when running the state machine with the <code>\"scope\":</code> parameter.</li> </ul> </li> <li>Starting the state machine with <code>{\"scope\":\"FULL\",\"mode\":\"APPLY\"}</code> makes the state machine execute as it did in v1.2.6 and below.<ul> <li>The state machine targets all AWS accounts and allows changes across any section of the config file;</li> <li>The blocks and overrides described in section 1.4 above remain valid;</li> <li><code>FULL</code> mode must be run at least once immediately after any Accelerator version upgrade. Code Pipeline automatically starts the state machine with <code>{\"scope\":\"FULL\",\"mode\":\"APPLY\"}</code>. If the state machine fails for any reason after upgrade, the state machine must be restarted with these parameters until a successful execution of the state machine has completed.</li> </ul> </li> <li>Starting the state machine with <code>{\"scope\":\"NEW-ACCOUNTS\",\"mode\":\"APPLY\"}</code> is the same as operating the state machine with the <code>default scope</code> as described in the first bullet</li> <li>Starting the state machine with <code>{\"scope\":\"GLOBAL-OPTIONS\",\"mode\":\"APPLY\"}</code> restricts changes to the config file to the <code>global-options</code> section.<ul> <li>If any other portion of the config file was updated or changed, the state machine will fail;</li> <li>The global options scope executes the state machine on the entire managed account footprint.</li> </ul> </li> <li>Starting the state machine with <code>{\"scope\":\"OU\",\"targetOus\":[X],\"mode\":\"APPLY\"}</code> restricts changes to the config file to the specified <code>organizational-units</code> section(s) defined by <code>targetOus</code>.<ul> <li>When <code>scope=OU</code>, <code>targetOus</code> becomes a mandatory parameter;</li> <li><code>X</code> can be any one or more valid OU names, or the value <code>\"ALL\"</code>;</li> <li>When <code>[\"ALL\"]</code> is specified, the state machine targets all AWS accounts, but only allows changes to the <code>organizational-units</code> section of the config file;</li> <li>When OUs are specified (i.e. <code>[\"Dev\",\"Test\"]</code>), the state machine only targets mandatory accounts plus accounts in the specified OUs (Dev, Test), and only allows changes to the specified OUs sections (Dev, Test) of the config file;</li> <li>If any other portion of the config file was updated or changed, the state machine will fail.</li> </ul> </li> <li>Starting the state machine with <code>{\"scope\":\"ACCOUNT\",\"targetAccounts\":[X],\"mode\":\"APPLY\"}</code> restricts changes to the config file to the specified <code>xxx-account-configs</code> section(s) defined by <code>targetAccounts</code>.<ul> <li>When <code>scope=ACCOUNT</code>, <code>targetAccounts</code> becomes a mandatory parameter;</li> <li><code>X</code> can be any one or more valid account numbers, the value <code>\"NEW\"</code>, or the value <code>\"ALL\"</code>;</li> <li>When <code>[\"ALL\"]</code> is specified, the state machine targets all AWS accounts, but only allows changes to the <code>xxx-account-configs</code> sections of the config file;</li> <li>When specific accounts and/or <code>NEW</code> is specified (i.e. <code>[\"NEW\", \"123456789012\", \"234567890123\"]</code>), the state machine only targets mandatory accounts plus the listed accounts and any newly created accounts. It also only allows changes to the specified accounts sections (New, 123456789012, 234567890123) of the config file;</li> <li>If any other portion of the config file was updated or changed, the state machine will fail.</li> </ul> </li> </ol> <p>Starting in v1.3.0, we recommend running the state machine with the parameters that most tightly scope the state machines execution to your planned changes and minimizing the use of <code>FULL</code> scope execution.</p> <ul> <li>should you accidentally change the wrong section of the config file, you will be protected;</li> <li>as you grow and scale to hundreds or thousands of accounts, your state machine execution time will remain fast.</li> </ul> <p>NOTE 1: The <code>scope</code> setting has no impact on SCP application, limit requests, custom tagging, or directory sharing.</p> <p>NOTE 2: All comparisons for config file changes are assessed AFTER all replacements have been made. Changing variable names which result in the same end outcome do NOT appear as a change to the config file.</p>"},{"location":"installation/sm_inputs/#12-accelerator-state-machine-inputs","title":"1.2. Accelerator State Machine Inputs","text":""},{"location":"installation/sm_inputs/#121-rebuild-dynamodb-table-contents","title":"1.2.1. Rebuild DynamoDB table contents","text":"<p>With the exception of the Outputs table, the contents of the Accelerator DynamoDB tables are rebuilt on every state machine execution. We recently started depending on the Outputs DynamoDB tables to ensure the parameters in parameter store are consistently maintained in the same order as objects are created and deleted. Should the CONTENTS of the tables be destroyed or corrupted, customers can force a rebuild of the CloudFormation Outputs in DynamoDB by starting the state machine with the parameter:</p> <pre><code>{ \"storeAllOutputs\": true }\n</code></pre> <p>This should be completed BEFORE running the state machine with a corrupt or empty DynamoDB table or the Accelerator is likely to reorder a customers parameters. If the DynamoDB tables were completely destroyed, they must be recreated before running the state machine with this parameter.</p>"},{"location":"installation/sm_inputs/#122-bypass-all-config-file-validation-checks","title":"1.2.2. Bypass ALL config file validation checks","text":"<p>This parameter should be specified with extreme caution, as it bypasses all config file validation. The state machine typically has protections enabled preventing customers from making breaking changes to the config file. Under certain conditions with the support of a trained expert, bypassing these checks is required. Start the state machine with the parameter:</p> <pre><code>{ \"overrideComparison\": true }\n</code></pre> <p>Customers are encouraged to use the specific override variables below, rather than the all-inclusive override, to ensure they only bypasses intended config changes.</p>"},{"location":"installation/sm_inputs/#123-bypassing-specific-config-file-validation-checks","title":"1.2.3. Bypassing SPECIFIC config file validation checks","text":"<p>Providing any one or more of the following flags will only override the specified check(s):</p> <pre><code>{\n\"configOverrides\": {\n\"ov-global-options\": true,\n\"ov-del-accts\": true,\n\"ov-ren-accts\": true,\n\"ov-acct-email\": true,\n\"ov-acct-ou\": true,\n\"ov-acct-vpc\": true,\n\"ov-acct-subnet\": true,\n\"ov-acct-vpc-optin\": true,\n\"ov-tgw\": true,\n\"ov-mad\": true,\n\"ov-ou-vpc\": true,\n\"ov-ou-subnet\": true,\n\"ov-share-to-ou\": true,\n\"ov-share-to-accounts\": true,\n\"ov-nacl\": true,\n\"ov-nfw\": true\n}\n}\n</code></pre>"},{"location":"installation/sm_inputs/#124-generate-verbose-logging-within-state-machine","title":"1.2.4. Generate verbose logging within state machine","text":"<ul> <li>Added \"verbose\": \"1\" state machine input options</li> <li>parameter is optional</li> <li>parameter defaults to 0</li> </ul> <pre><code>{ \"scope\": \"FULL\", \"mode\": \"APPLY\", \"verbose\": \"1\" }\n</code></pre>"},{"location":"installation/sm_inputs/#125-state-machine-scoping-inputs","title":"1.2.5. State Machine scoping inputs","text":"<p>Summary of inputs, per section 1.1 above:</p> <pre><code>{ \"scope\": \"FULL\", \"mode\": \"APPLY\" }\n</code></pre> <pre><code>{ \"scope\": \"NEW-ACCOUNTS\", \"mode\": \"APPLY\" }\n</code></pre> <pre><code>{ \"scope\": \"GLOBAL-OPTIONS\", \"mode\": \"APPLY\" }\n</code></pre> <pre><code>{ \"scope\": \"OU\", \"targetOus\": [\"ou-name\", \"ou-name\"], \"mode\": \"APPLY\" }\n</code></pre> <pre><code>{ \"scope\": \"ACCOUNT\", \"targetAccounts\": [\"123456789012\", \"234567890123\"], \"mode\": \"APPLY\" }\n</code></pre>"},{"location":"installation/sm_inputs/#126-example-of-combined-inputs","title":"1.2.6. Example of combined inputs","text":"<pre><code>{\n\"scope\": \"FULL\",\n\"mode\": \"APPLY\",\n\"configOverrides\": { \"ov-ou-vpc\": true, \"ov-ou-subnet\": true, \"ov-acct-vpc\": true }\n}\n</code></pre>"},{"location":"installation/upgrades/","title":"1. Accelerator Upgrade Guide","text":""},{"location":"installation/upgrades/#11-general-upgrade-considerations","title":"1.1. General Upgrade Considerations","text":"<ul> <li>Due to some breaking dependency issues, customers can only upgrade to v1.3.8 or above (older releases continue to function, but cannot be installed).</li> <li>While an upgrade path is planned, customers with a standalone Accelerator installation can upgrade to v1.5.x but need to continue with a standalone installation until the Control Tower upgrade option becomes available.</li> <li>Always compare your configuration file with the config file from the release you are upgrading to in order to validate new or changed parameters or changes in parameter types / formats.<ul> <li>do NOT update to the latest firewall AMI - see the last bullet in section 1.8. Other Operational Considerations of the installation guide</li> <li>do NOT update the <code>organization-admin-role</code> - see item 2 in section 1.3.7. Other</li> <li>do NOT update account-keys (i.e. existing installations cannot change the internal values to <code>management</code> from <code>master</code>)</li> <li>do NOT make changes outside those required for the upgrade (those stated in the release notes or found through the comparison with the sample config file(s)). Customers wishing to change existing Accelerator configuration should either do so before their upgrade, ensuring a clean/successful state machine execution, or after a successful upgrade.</li> </ul> </li> <li>The Accelerator name and prefix CANNOT be changed after the initial installation</li> <li>Customers which customized any of the Accelerator provided default configuration files (SCPs, rsyslog config, ssm-documents, iam-policies, etc.) must manually merge the latest Accelerator provided updates with deployed customizations:<ul> <li>it is important customers assess the new defaults and integrate them into their custom configuration, or Accelerator functionality could break or Accelerator deployed features may be unprotected from modification</li> <li>if customers don't take action, we continue to utilize the deployed customized files (without the latest updates)</li> </ul> </li> <li>The below release specific considerations need to be cumulatively applied (an upgrade from v1.2.3 to v1.2.5 requires you to follow both v1.2.4 and v1.2.5 considerations)</li> </ul>"},{"location":"installation/upgrades/#12-release-specific-upgrade-considerations","title":"1.2. Release Specific Upgrade Considerations:","text":"<ul> <li>Upgrades to <code>v1.5.1-a and above</code> from <code>v1.5.0</code> or <code>v1.5.1</code>:<ul> <li>Do not add the parameter: <code>\"ssm-inventory-collection\": true</code> to OUs or accounts which already have SSM Inventory configured or the state machine will fail</li> <li>Follow the standard upgrade steps detailed in section 1.3 below</li> </ul> </li> <li><code>v1.5.1</code> was replaced by v1.5.1-a and is no longer supported for new installs or upgrades</li> <li>Upgrades to <code>v1.5.0</code> and <code>v1.5.1-a and above</code> from <code>v1.3.8 through v1.3.9</code>:<ul> <li>We recommend upgrading directly to v1.5.1-a</li> <li>Due to the size and complexity of this upgrade, we require all customers to upgrade to <code>v1.3.8 or above</code> before beginning this upgrade</li> <li>While v1.5.0 supports Control Tower for NEW installs, existing Accelerator customers CANNOT add Control Tower to their existing installations at this time (planned enhancement for 22H1)<ul> <li>Attempts to install Control Tower on top of the Accelerator will corrupt your environment (both Control Tower and the Accelerator need minor enhancements to enable)</li> </ul> </li> <li>The v1.5.x custom upgrade guide can be found here</li> </ul> </li> <li>Upgrades to <code>v1.3.9 and above</code> from <code>v1.3.8-b and below</code>:<ul> <li>All interface endpoints containing a period must be removed from the config.json file either before or during the upgrade process<ul> <li>i.e. ecr.dkr, ecr.api, transfer.server, sagemaker.api, sagemaker.runtime in the full config.json example</li> <li>If you remove them on a pre-upgrade State Machine execution, you can put them back during the upgrade, if you remove them during the upgrade, you can put them back post upgrade.</li> </ul> </li> </ul> </li> <li> <p>Upgrades to <code>v1.3.3 and above</code> from <code>v1.3.2 and below</code>:</p> <ul> <li>Requires mandatory config file schema changes as documented in the release notes.<ul> <li>These updates cause the config file change validation to fail and require running the state machine with the following input to override the validation checks on impacted fields: <code>{\"scope\": \"FULL\", \"mode\": \"APPLY\", \"configOverrides\": {\"ov-ou-vpc\": true, \"ov-ou-subnet\": true, \"ov-acct-vpc\": true }}</code></li> <li>Tightens VPC interface endpoint security group permissions and enables customization. If you use VPC interface endpoints that requires ports/protocols other than TCP/443 (such as email-smtp), you must customize your config file as described here</li> </ul> </li> </ul> </li> <li> <p>Upgrades from <code>v1.3.0 and below</code>:</p> <ul> <li>Please review the <code>Release Specific Upgrade Considerations</code> from ASEA v1.5.0 or below, they were removed from this release.</li> </ul> </li> </ul>"},{"location":"installation/upgrades/#13-summary-of-upgrade-steps-all-versions-except-v150","title":"1.3. Summary of Upgrade Steps (all versions except v1.5.0)","text":"<ol> <li>Login to your Organization Management (root) AWS account with administrative privileges</li> <li>Either:    a) Ensure a valid Github token is stored in secrets manager (per the installation guide), or    b) Ensure the latest release is in a valid branch of CodeCommit in the Organization Management account</li> <li>Review and implement any relevant tasks noted in the General Upgrade Considerations section above</li> <li>Update the config file in CodeCommit with new parameters and updated parameter types based on the version you are upgrading to (this is important as features are iterating rapidly)<ul> <li>An automated script is available to help convert config files to the new v1.5.0 format</li> <li>Compare your running config file with the sample config file from the latest release</li> <li>Review the <code>Config file changes</code> section of the release notes for all Accelerator versions since your current deployed release</li> </ul> </li> <li>If you customized any of the other Accelerator default config files by overriding them in your S3 input bucket, merge the latest defaults with your customizations before beginning your upgrade</li> <li>Download the latest installer template (<code>AcceleratorInstallerXYZ.template.json</code> or <code>AcceleratorInstallerXXX-CodeCommit.template.json</code>) from the <code>Assets</code> section of the latest release</li> <li>Do NOT accidentally select the <code>ASEA-InitialSetup</code> CloudFormation stack below</li> <li>If you are replacing your GitHub Token:<ul> <li>Take note of the <code>AcceleratorName</code>, <code>AcceleratorPrefix</code>, <code>ConfigS3Bucket</code> and <code>NotificationEmail</code> values from the Parameters tab of your deployed Installer CloudFormation stack (<code>ASEA-what-you-provided</code>)</li> <li>Delete the Installer CloudFormation stack (<code>ASEA-what-you-provided</code>)</li> <li>Redeploy the Installer CloudFormation stack using the template downloaded in step 6, providing the values you just documented (changes to <code>AcceleratorName</code> or <code>AcceleratorPrefix</code> are not supported)</li> <li>The pipeline will automatically run and trigger the upgraded state machine</li> </ul> </li> <li> <p>If you are using a pre-existing GitHub token, or installing from CodeCommit:</p> <ul> <li>Update the Installer CloudFormation stack using the template downloaded in step 5, updating the <code>GithubBranch</code> to the latest release (eg. <code>release/v1.5.1-a</code>)<ul> <li>Go to AWS CloudFormation and select the stack: <code>ASEA-what-you-provided</code></li> <li>Select Update, select Replace current template, Select Upload a template file</li> <li>Select Choose File and select the template you downloaded in step 6 (<code>AcceleratorInstallerXYZ.template.json</code> or <code>AcceleratorInstallerXXX-CodeCommit.template.json</code>)</li> <li>Select Next, Update <code>GithubBranch</code> parameter to <code>release/vX.Y.Z</code> where X.Y.Z represents the latest release</li> <li>Click Next, Next, I acknowledge, Update</li> <li>Wait for the CloudFormation stack to update (<code>Update_Complete</code> status) (Requires manual refresh)</li> </ul> </li> <li>Go To Code Pipeline and Release the ASEA-InstallerPipeline</li> </ul> </li> </ol>"},{"location":"installation/v150-Upgrade/","title":"1. Accelerator v1.5.x Custom Upgrade Instructions","text":""},{"location":"installation/v150-Upgrade/#11-overview","title":"1.1. Overview","text":"<p>The upgrade from v1.3.8/v1.3.9 to v1.5.x is generally the same as any previous Accelerator upgrades, with a couple of key differences:</p> <ul> <li>the magnitude of this release has resulted in a requirement for significant updates to the config file<ul> <li>we have provided a script to assist with this process. A manual verification of the changes and customer custom updates are often still required.</li> </ul> </li> <li>we are re-aligning the OU structure with AWS guidance and that of AWS Control Tower (optional, but highly recommended)<ul> <li>the core OU is being split into a \"Security\" OU and an \"Infrastructure\" OU</li> </ul> </li> <li>we've added the capability to manage your IP addresses in DynamoDB, rather than with the config file<ul> <li>this includes the ability to dynamically allocate CIDR ranges to VPCs and subnets</li> <li>more information on this features design can be found on this ticket</li> <li>the config file conversion script will:<ul> <li>update your config file in a manner that supports both CIDR management schemes (but continues to leverage the previous mechanism)</li> <li>copy your currently configured CIDR ranges into the appropriate DynamoDB tables (optional, but recommended)</li> </ul> </li> <li>you can change your IP address mechanism for any VPC at any time</li> <li>customers can mix and match IP address management mechanisms as they choose (<code>provided</code>, <code>lookup</code>, and <code>dynamic</code>)</li> </ul> </li> </ul>"},{"location":"installation/v150-Upgrade/#12-upgrade-caveats","title":"1.2. Upgrade Caveats","text":"<ol> <li> <p>While an upgrade path is planned, customers with a Standalone Accelerator installation can upgrade to v1.5.x but need to continue with a Standalone installation until the Control Tower upgrade option becomes available.</p> </li> <li> <p>The script to assist with config file conversion and DynamoDB population only supports single file json based config files, customers that leverage YAML and/or multi-part config files, have several options:</p> <ul> <li>manually update your yaml or multi-part json config file to reflect the config file format for the latest release (similar to all previous upgrades)</li> <li>use the config.json file found in the <code>raw</code> folder of your CodeCommit repo to run the conversion script<ul> <li>this version of the config file has resolved all variables with their final values, all variables will be removed from config.json in this scenario</li> <li>the new config file can be converted back to json/multi-part format before being placed back into your CodeCommit repository</li> <li>or it could be used to simply validate the changes you made using option a</li> <li>do not manually update the config file in the <code>raw</code> folder, as it will be overwritten based on the json or yaml file in the root of your repository</li> </ul> </li> <li>use a 3rd party tool to manually convert your yaml / multi-part config files to a single file json file to run the conversion script<ul> <li>the new config file can be converted back to json/multi-part format before being placed back into your CodeCommit repository</li> </ul> </li> </ul> </li> <li> <p>Config files which are significantly different than the example config files may not be properly converted. This includes config files which use different mandatory account keys or renamed the core OU.</p> </li> <li>This guide and its examples assume the existing accelerator deployment uses the <code>PBMMAccel-</code> accelerator prefix, if a different prefix is used on the existing installation, it is important it is specified when execution section 1.6 below.</li> </ol>"},{"location":"installation/v150-Upgrade/#13-config-file-conversion","title":"1.3. Config File Conversion","text":"<ul> <li>You must first upgrade to Accelerator v1.3.8 or v1.3.9</li> <li>Login to your AWS Organization Management account</li> <li>Pull your current config.json file from CodeCommit and save as a text file</li> <li> <p>Locate the python conversion script and review its readme here</p> <ul> <li>To convert your configuration file execute: (completely offline process)</li> </ul> <p><code>python update.py --Region ca-central-1 --LoadConfig --ConfigFile config.json</code></p> <ul> <li>This will output a new config file named: <code>update-config.json</code></li> <li>Save both the original v13.8 and the new v1.5.0 config files for future reference/use</li> <li>After conversion, we recommend running the updated config file back prettier to simplify file comparisons</li> </ul> </li> <li> <p>While the conversion script often does much of the heavy lifting, we still require customers to manually verify the changes and make manual adjustments as appropriate:</p> <ul> <li>If you use a relatively standard config file you MAY not need to make any changes manually</li> <li> <p>Ensure the value of <code>account-name</code> for the Organization Management account matches the actual account name of the Organization management account (the account key is generally either <code>management</code> or <code>master</code>).</p> </li> <li> <p>we recommend you change your <code>rdgw-instance-type</code> and <code>rsyslog-instance-type</code> from t2.to t3. (they will auto-replace on the next instance refresh) (Optional).</p> </li> <li>optionally remove the <code>\"API_GW_EXECUTION_LOGGING_ENABLED\"</code> config rule throughout, as it overlaps with an identical Security Hub config rule.</li> <li>we added the capability to deploy a Config aggregator in any of the central services accounts (i.e. Log-archive, Security, Operations), by adding <code>\"config-aggr\": true</code> to either: <code>central-security-services</code>, <code>central-operations-services</code>, or <code>central-log-services</code>. The existing aggregator in the Org management account will remain. Do not set it in all 3 sections, as AWS only supports a maximum of 3 config aggregators.</li> <li>the optional attribute <code>endpoint-port-orverides</code> has been properly renamed to <code>endpoint-port-overrides</code>. If you have the <code>endpoint-port-orverides</code> in your config file you must rename it to <code>endpoint-port-overrides</code>.</li> <li>the new example config files also introduced several new internally resolvable variables (<code>${CONFIG::OU_NAME}</code> and <code>${CONFIG::VPC_NAME}</code>), which when used thoughtfully along with the new dynamic CIDR feature, enables multi-part config file customers to define the VPCs for multiple OU's in a single shared nested config file. These new variables should be ignored during an upgrade.</li> <li>the accelerator supports 3 types of CIDR ranges <code>provided</code>, <code>lookup</code>, and <code>dynamic</code>. The upgrade script sets the <code>cidr-src</code> to <code>provided</code>, meaning it uses the CIDR ranges provided in the config file, as per the previous release. The upgrade script also adds the additional required fields (<code>pool</code> and <code>size</code>) to every CIDR range defined in the config file to leverage the <code>lookup</code> type, but when set to <code>provided</code> these fields are NOT required and could be removed. They were added by the script for the sole purpose of making it easy to switch from <code>provided</code> to <code>lookup</code> in future. Once a customer switches to <code>lookup</code>, the <code>cidr\\value</code> field is no longer used and can be removed from the config file. The <code>cidr-src</code> for should remain set at <code>provided</code> during upgrade.</li> <li>do not add the <code>cidr-pools</code> section to the config file during or before the upgrade, this section is only used for new installations.</li> <li>New description fields have been added to the config file to help provide context to certain objects. These will be used by a future GUI that is under development, and serve no functional purpose at this time. Customers can alter this text as they please.</li> <li>Most of the example config files have been converted to <code>dynamic</code> cidr-src as it provides simplier CIDR management for new customers. Two example config files ending in <code>-oldIP.json</code> have been maintained to aid upgrading customers in config file comparison.</li> <li>Be advised - in v1.5.0 we restructured the SCPs based on a) customer requests, and b) the addition of Control Tower support for new installs.<ul> <li>customers are responsible for reviewing the SCPs to ensure they have not been altered in a manner that no longer meets an organizations security requirements;</li> <li>we reorganized and optimized our SCP's from 4 SCP files down to 3 SCP files, without removing any protections or guardrails;</li> <li>these optimizations have resulted in minor enhancements to the SCP protections and in some cases better scoping;</li> <li>the first two SCP files (Part-0 and Part-1) contain the controls which protect the integrity of the Accelerator itself;</li> <li>the third file (Sensitive, Unclass, Sandbox) contains customer data protection specific guardrails, which may change based on workload data classification or customer profiles and requirements;</li> <li>this frees the fourth SCP for use by Control Tower, or for use by customers for custom guardrails (Standalone installs only). As Control Tower leverages 2 SCP files on the Security OU, we have moved some of our SCP's to the account level (Control Tower installations only).</li> </ul> </li> <li>The script and upgrade instructions above do not include the new config file parameters added in v1.5.1+. These new parameters can be added either during or after the upgrade. New parameters include: <code>\"rdgw-enforce-imdsv2\": true</code>, <code>\"rsyslog-enforce-imdsv2\": true</code>, <code>\"ssm-inventory-collection\": true</code> on each ou, and <code>\"dynamic-s3-log-partitioning\": [{values}]</code></li> </ul> </li> </ul>"},{"location":"installation/v150-Upgrade/#14-upgrade-process","title":"1.4. Upgrade process","text":"<ul> <li>Before proceeding with your upgrade please review the General and Release Specific Upgrade Considerations in the Upgrade Guide<ul> <li>upgrades directly from v1.3.8 need to ensure they include the extra step required for v1.3.9 upgrades (removal of endpoints with periods)</li> </ul> </li> <li>Login to your AWS Organization Management account, in your home or default region</li> <li>Place your updated and validated config file back in the root folder of your CodeCommit repository<ul> <li>json, yaml and multi-part config files continue to be supported</li> </ul> </li> <li>Change to the us-east-1 region and open CloudWatch<ul> <li>Select Rules under events in the left navigation pane</li> <li>Select the <code>PBMMAccel-MoveAccount_rule</code>, select <code>actions</code>, select <code>Disable</code></li> <li>Select the <code>PBMMAccel-PolicyChanges_rule</code>, select <code>actions</code>, select <code>Disable</code></li> </ul> </li> <li>Open AWS Organizations<ul> <li>Select the <code>core</code> OU and rename it to <code>Security</code> (case sensitive) (i.e. Actions, Rename)</li> <li>In the <code>Root</code> ou, create a new OU named `Infrastructure (case sensitive) (i.e. Actions, Create new)</li> <li>Expand the <code>Security</code> OU, select all EXCEPT the <code>security</code>, <code>log-archive</code>, and Organization <code>Management</code> account<ul> <li>(i.e. the <code>Operations</code>, <code>Perimeter</code>, and <code>SharedNetwork</code> accounts)</li> </ul> </li> <li>Select Actions, Move, then select the newly created <code>Infrastructure</code> OU (note these accounts temporarily missing guardrails)<ul> <li>NOTE: the key to this being a possible move/change, is the new <code>Infrastructure</code> OU is defined identically to the old <code>core</code> OU. Typically you CANNOT move accounts between OU's without breaking something and violating security guardrails.</li> </ul> </li> <li>Select Policies from the left navigation pane, then Service COntrol Policies<ul> <li>Click the <code>PBMMAccel-Guardrails-Part-2</code> policy, and Select Targets</li> <li>Detach the policy from ALL OUs</li> </ul> </li> </ul> </li> <li>Change to the us-east-1 region and open CloudWatch<ul> <li>Select Rules under events in the left navigation pane</li> <li>Select the <code>PBMMAccel-MoveAccount_rule</code>, select <code>actions</code>, select <code>Enable</code></li> <li>Select the <code>PBMMAccel-PolicyChanges_rule</code>, select <code>actions</code>, select <code>Enable</code></li> </ul> </li> <li>Follow the Standard Upgrade instructions from the section <code>Summary of Upgrade Steps (all versions)</code> of the Installation and Upgrade guide, repeated verbatim below for ease of reference</li> </ul>"},{"location":"installation/v150-Upgrade/#15-summary-of-upgrade-steps-all-versions-copied-from-upgrade-guide","title":"1.5. \"Summary of Upgrade Steps (all versions)\" (Copied from upgrade guide)","text":"<ol> <li>Login to your Organization Management (root) AWS account with administrative privileges</li> <li> <p>Either:</p> <p>a) Ensure a valid Github token is stored in secrets manager, or</p> <p>b) Ensure the latest release is in a valid branch of CodeCommit in the Organization Management account. See this (section) of the installation guide for more details.</p> </li> <li> <p>Review and implement any relevant tasks noted in the upgrade consideration sections (sections 1.1 and 1.2) of the Upgrade Guide</p> </li> <li>Update the config file in CodeCommit with new parameters and updated parameter types based on the version you are upgrading to (this is important as features are iterating rapidly)<ul> <li>An automated script is available to help convert config files to the new v1.5.0 format</li> <li>Compare your running config file with the sample config file from the latest release</li> <li>Review the <code>Config file changes</code> section of the release notes for all Accelerator versions since your current deployed release</li> </ul> </li> <li>If you customized any of the other Accelerator default config files by overriding them in your S3 input bucket, merge the latest defaults with your customizations before beginning your upgrade</li> <li>Download the latest installer template (<code>AcceleratorInstallerXYZ.template.json</code> or <code>AcceleratorInstallerXXX-CodeCommit.template.json</code>) from the <code>Assets</code> section of the latest release</li> <li>Do NOT accidentally select the <code>PBMMAccel-InitialSetup</code> CloudFormation stack below</li> <li>If you are replacing your GitHub Token:<ul> <li>Take note of the <code>AcceleratorName</code>, <code>AcceleratorPrefix</code>, <code>ConfigS3Bucket</code> and <code>NotificationEmail</code> values from the Parameters tab of your deployed Installer CloudFormation stack (<code>PBMMAccel-what-you-provided</code>)</li> <li>Delete the Installer CloudFormation stack (<code>PBMMAccel-what-you-provided</code>)</li> <li>Redeploy the Installer CloudFormation stack using the template downloaded in step 6, providing the values you just documented (changes to <code>AcceleratorName</code> or <code>AcceleratorPrefix</code> are not supported)</li> <li>The pipeline will automatically run and trigger the upgraded state machine</li> </ul> </li> <li> <p>If you are using a pre-existing GitHub token, or installing from CodeCommit:</p> <ul> <li>Update the Installer CloudFormation stack using the template downloaded in step 5, updating the <code>GithubBranch</code> to the latest release (eg. <code>release/v1.5.0</code>)<ul> <li>Go to AWS CloudFormation and select the stack: <code>PBMMAccel-what-you-provided</code></li> <li>Select Update, select Replace current template, Select Upload a template file</li> <li>Select Choose File and select the template you downloaded in step 6 (<code>AcceleratorInstallerXYZ.template.json</code> or <code>AcceleratorInstallerXXX-CodeCommit.template.json</code>)</li> <li>Select Next, Update <code>GithubBranch</code> parameter to <code>release/vX.Y.Z</code> where X.Y.Z represents the latest release</li> <li>Click Next, Next, I acknowledge, Update</li> <li>Wait for the CloudFormation stack to update (<code>Update_Complete</code> status) (Requires manual refresh)</li> </ul> </li> <li>Go To Code Pipeline and Release the PBMMAccel-InstallerPipeline</li> </ul> </li> </ol>"},{"location":"installation/v150-Upgrade/#16-post-upgrade-follow-up-tasks-for-v15x-upgrade","title":"1.6. Post Upgrade Follow-up Tasks for v1.5.x Upgrade","text":"<ul> <li>Once the State Machine finishes in AWS Organizations, delete the <code>PBMMAccel-Guardrails-Part-2</code> SCP, as we do not remove policies from Organizations given they could be used by customers for other purposes.</li> <li> <p>Optionally, populate DynamoDB with your allocated CIDR ranges:</p> <ul> <li>You must perform the following steps if you plan to:<ul> <li>move from <code>\"cidr-src\": \"provided\"</code> to <code>\"cidr-src\": \"lookup\"</code> for your existing cidr blocks; or</li> <li>dynamically assign CIDR ranges to new VPCs using pools that overlap with existing VPC's currently defined in your config file.</li> </ul> </li> <li>this step populates the DynamoDB tables (<code>PBMMAccel-cidr-vpc-assign</code> and <code>PBMMAccel-cidr-subnet-assign</code>) with your EXISTING utilized CIDR ranges using the upgrade script a second time after the upgrade is complete.</li> <li> <p>Using the converted version of your config file (update-config.json)</p> <ul> <li>Validate you are happy with the <code>pool</code> names assigned to each VPC and subnet throughout the config file. Update as appropriate, pool names can be any alpha-numeric string, but a subnets pool must match one of its VPCs pools.</li> <li>Locate the python conversion script and review its readme here</li> <li>To load DynamoDB with your CIDR ranges, execute: (online, requires credentials to the Organization Management account)</li> </ul> <p><code>python update.py --Region ca-central-1 --LoadDB --ConfigFile update-config.json --AcceleratorPrefix PBMMAccel-</code></p> <ul> <li>Note the use of the --LoadDB switch, the UPDATED config filename, and that the Accelerator prefix is specified<ul> <li>If you run this command using your v1.3.8 or v1.3.9 config file, the <code>\\cidr\\pool:</code> values will be improperly assigned in DynamoDB.</li> </ul> </li> <li>In the Organization Management account, in DynamoDB, select Items, and Verify that the tables have been properly populated.<ul> <li>the script should have populated the following two DynamoDB tables: <code>PBMMAccel-cidr-vpc-assign</code> and <code>PBMMAccel-cidr-subnet-assign</code> with all your existing vpc and subnet assignments.</li> </ul> </li> </ul> </li> <li> <p>if you plan to dynamically assign CIDR ranges for any new VPCs, you need to manually create the CIDR pools by adding new item(s) to the DynamoDB Table <code>PBMMAccel-cidr-pool</code>. The <code>PBMMAccel-cidr-pool</code> table stores CIDR ranges to select from for new CIDR assignments. This table works together with the other two DynamoDB tables to track, assign and maintain non-overlapping CIDR ranges based on a pool name and region.</p> </li> </ul> </li> </ul> Sample DynamoDB JSON to add an entry to the `PBMMAccel-cidr-pool` table: <pre><code>{\n\"id\": {\n\"S\": \"1\"\n},\n\"cidr\": {\n\"S\": \"10.0.0.0/13\"\n},\n\"region\": {\n\"S\": \"ca-central-1\"\n},\n\"pool\": {\n\"S\": \"main\"\n}\n}\n</code></pre>      - where `id` is any unique text, `cidr` is the main cidr block from which VPC cidrs are taken. `region` is the AWS region where the pool is used. `pool` is the name of the pool   <p>NOTES:</p> <ul> <li>You can populate the <code>cidr-pools</code> section of the config file/DynamoDB with values that overlap with the existing assigned ranges in your config file. In this situation, it is CRITICAL that you execute this entire process, to avoid issueing duplicate or overlapping CIDR ranges with those already issued. Alternatively, leverage new unique ranges when populating the <code>cidr-pools</code>.</li> <li><code>cidr-pools</code> only needs to be populated when a VPC has a <code>cidr-src</code> set to <code>dynamic</code>.</li> <li>Optionally, change all the <code>cidr-src</code> values throughout your config file to <code>lookup</code>, and remove all the <code>cidr\\value</code> fields. Once changed, CIDR values will be provided by DynamoDB. Switching to <code>lookup</code> requires completion of the previous optional step to first load DynamoDB.<ul> <li>run the state machine with the input parameters <code>{\"scope\": \"FULL\",\"mode\": \"APPLY\",\"verbose\": \"0\"}</code></li> <li>during the state machine execution, the Accelerator will compare the values returned by DynamoDB with the values from the previous successful state machine execution. If the DynamoDB values were incorrectly populated, the state machine will catch it with a comparison failure message and gracefully fail.</li> </ul> </li> </ul>"},{"location":"installation/what-we-do-where/","title":"AWS Secure Environment Accelerator Deployment Capabilities","text":""},{"location":"installation/what-we-do-where/#overview","title":"Overview","text":"<p>Deploys, creates, manages and updates the following objects across a multi-region, multi-account AWS environment</p> TASK Accelerator - What happens, WHERE, under what condition, on each state machine execution AWS Accounts - Creates mandatory accounts (accounts which other accounts are dependent on) organization management (root) account, global scope - Creates workload accounts (individually or in bulk), base personality determined by ou placement organization management (root) account, global scope - Supports native AWS Organization account and OU activities (OU and account rename, move account between OU's, create accounts, etc.) organization management (root) account, global scope - Applies a Deny All SCP on any newly created account(s) until successfully guardrailed organization management (root) account, new account scope (failure to apply guardrails fails the Accelerator and leaves account blocked until remediated) - Allows bulk parallel* account creation, configuration, updates and guardrail application creates, guardrails and configures new accounts and regions in parallel per defined personas, organization management (root) account. Control Tower account ingestion is sequential at this time. - Performs 'account warming' to establish initial limits, when required state Machine region only, defined accounts (per region potential) - Checks limit increases, when required (complies with initial limits until increased) per account, per region (supported limits only) - Automatically submits limit increases, when required state Machine region only, defined accounts (per region potential) - Leverages AWS Control Tower Accelerator and Control Tower home regions must match, the Accelerator supports all on-by-default regions and will require a standalone install in regions not yet supported by Control Tower Networking - Creates Transit Gateways and TGW route tables incl. static routes and inter-region TGW peering in the defined region(s), defined account(s) - Creates centralized and/or local account (bespoke) VPC's in the defined region(s), defined account(s) ...all completely and individually customizable (per account, VPC, subnet, or OU), Static or Dynamic VPC and subnet CIDR assignments - Creates Subnets, Route tables, NACLs, Security groups, NATGWs, IGWs, VGWs, CGWs (per customer specs) part of any VPC, in the defined region(s), defined account(s) - allows detailed CIDR allocation, and cross-account security group referencing - Deletes default VPC's (worldwide) in all regions, in all accounts, can disable regions (all accounts or specific account) - Creates VPC Endpoints (Gateway and Interface) part of any VPC, in the defined region(s), defined account(s) - Configures centralized endpoints (R53 zones populated, shared and attached to local and cross-account VPC's) configures regional central endpoints (only one 'central' VPC per region) - Creates Route 53 Private and Public Zones in the defined account(s), defined region(s), defined VPC(s), global scope - Creates Resolver Rules and Resolver (inbound/outbound) Endpoints part of a specific VPC(s), in the defined region(s), defined account(s) (i.e. per region possible) ...including MAD R53 DNS resolver rule creation created in same region as MAD only, shared to same region VPC's when use-central-endpoints set - Automatically creates R53 VPC Endpoint Overloaded Zones same region(s), same account(s) as the endpoint and VPC(s) - Deploys and configures AWS Network Firewall on any VPC, any region, any account Cross-Account Object Sharing - VPC and Subnet sharing, including account level retagging/naming (and per account security group 'replication') VPC's are shared to accounts within the SAME REGION as the source VPC onlyAn OU could have additional VPC's defined for additional regions and would be shared to the appropriate accounts in the same additional regions - VPC peering and TGW attachments (local and cross-account) in the defined region, no cross-region attachments or peering supported - Managed Active Directory sharing state machine region only (consider same region as the MAD only)(unshare method not implemented) - Automated TGW inter-region peering cross-region, cross-account or same-account - Shares SSM remediation documents from defined account(s), to defined OU's, in defined regions Zone sharing and VPC associations - Public Hosted Zones no sharing, no association required (any account, any VPC, any region) - Private Hosted Zones - i.e. Cloud DNS domains associated worldwide to all VPCs with use-central-endpoints - Endpoint Private Hosted Zones associate within region, for all VPC use-central-endpoints (including cross-account) - On-premise resolver rules associate within region, for all VPC use-central-endpoints (including cross-account) - MAD resolver rule association same region as the MAD resolver only, assoc. w/all VPC use-central-endpoints Identity - Creates Directory services (Managed Active Directory and Active Directory Connectors) in a specific VPC, in the defined region, defined account - only 1 per account, therefore can't have a second region in the same account (ADC creation only supported in mandatory accounts) - Creates Windows admin bastion host auto-scaling group once per above MAD (once per account), same region as MAD - Set Windows domain password policies (initial installation only) once per above MAD (once per account), same region as MAD - Set IAM account password policies once per account, global scope - Creates Windows domain users and groups (initial installation only) once per above MAD (once per account), same region as MAD - Creates IAM Policies, Roles, Users, and Groups once per account, global scope Cloud Security Services - Enables and configs the following AWS services, worldwide w/central specified admin account: (each service can have specified regions disabled) - GuardDuty w/S3 protection enabled all regions, all accounts, admin account per region - Security Hub (Enables specified security standards, and disables specified individual controls) enabled all regions, all accounts, admin account per region - Firewall Manager enabled once per account (global scope), single admin account - CloudTrail w/Insights and S3 data plane logging enabled all regions (using Organization trail, stored in Organization Management account) - Config Recorders/Aggregator enabled all regions, all regions include global events, aggregator set to specified region in Organization Management account - Macie enabled all regions, admin account per region - IAM Access Analyzer enabled once per account (global scope), single admin account - Enables CloudWatch access from central specified admin account enabled once per account (global scope), two admin accounts (Ops &amp; Security) - Deploys customer provided SSM remediation documents (four provided out-of-box today) customized per OU, defined regions, defined accounts ...remediates S3 buckets without KMS CMK encryption and ALB's without centralized logging customized per OU, all regions, integrated w/SSM remediation, when desired - Deploys AWS Config rules (managed and custom) including AWS Conformance packs (NIST 800-53 deployed by default + 2 custom) customized per OU, all regions, all accounts integrated w/SSM remediation, when desired Other Security Capabilities - Creates, deploys and applies Service Control Policies at the top OU level only, sub-ou's managed directly through AWS Organizations - Creates Customer Managed KMS Keys w/automatic key rotation (SSM, EBS, S3) SSM and EBS keys are created if a VPC exists in the region, S3 if we need an Accelerator bucket in the region, per account - Enables account level default EBS KMS CMK encryption set if a VPC exists in the region, per account - Enables S3 Block Public Access once per account, global scope - Configures Systems Manager Session Manager w/KMS CMK encryption and centralized logging set if a VPC exists in the region, per account - Imports or requests certificates into AWS Certificate Manager State Machine region only (per region potential, required for ALB deployments) - Deploys both perimeter and account level ALB's w/Lambda health checks, certs &amp; TLS policies State Machine region only (per region potential) - Deploys &amp; configures 3rd party firewall clusters and management instances in the defined region(s), defined account(s) ...Gateway Load Balancer w/auto-scaling (NEW) and VPN IPSec BGP ECMP deployment options - Configuration is fully managed and maintained in AWS CodeCommit - full multi-account configuration history organization management (root) account ...breaking configuration changes block Accelerator execution Idempotent - extensive error handling and failure cleanup - Accelerator can be stopped, started, and rerun without implication Centralized Logging - Deploys an rsyslog auto-scaling cluster behind an NLB, all syslogs forwarded to CWL State Machine region only (per region potential) - Centralizes logging to a single centralized S3 KMS CMK encrypted bucket (enables, configures and centralizes) incl: Sets S3 ownership flag, sets bucket retentions - VPC Flow logs (w/Enhanced metadata fields and optional CWL destination) part of a specific VPC, in the defined region, defined account (to local account bucket in state machine region, replicated to log-archive primary region) - Organizational Cost and Usage Reports once per organization, global scope (to local account bucket in state machine region, replicated to log-archive primary region) - CloudTrail Logs including S3 Data Plane Logs (also sent to CWL) directly back to log-archive, specified primary region - All CloudWatch Logs (includes rsyslog logs) (and setting Log group retentions) State machine region, plus configured regions - Config History and Snapshots directly back to log-archive account specified primary region - Route 53 Public Zone Logs, DNS Resolver Query Logs to CloudWatch Logs in us-east-1 (which are sent to S3) - GuardDuty Findings directly back to log-archive, specified primary region - Macie Discovery results directly back to security, specified primary region, replicated to log-archive - ALB Logs State Machine region only (same as ALB deployment) - SSM Session Logs (also sent to CWL) All regions currently send back to central region, log-archive account Extensibility - Populates each accounts Parameter Store with the Accelerator deployed objects (allows customer IaC to extend/leverage) each account, defined regions (all ELB's across the environment are populated in specified accounts, i.e. perimeter, to enable automated end-to-end plumbing) - Every execution outputs the execution status and a list of successfully guardrailed accounts to a SNS topic allows 3rd party framework to execute after every Accelerator execution by hooking to SNS topic ...which emails a customer defined email address ...or hooking to the email alert - Deploys roles with customized access (read-only,write) to the log-archive buckets (enabling customer SIEM deployments, SSM, EC2 CWL) defined account, global scope - Designed for Day 1, 2 and day 10. Customers get new features without any customization effort no matter the deployed architecture Upgradable from any version to any version, no customization or professional services required (Customer production proven across multiple releases) Alerting - Deploys global High, Medium, Low, Ignore priority SNS topics and email subscriptions in the defined account, org accessible regional topics, each region subscribed to a single defined central region which has the email subscriptions - Deploys customer defined CloudWatch Log Metrics and Alarms w/prioritized alarms (19 out-of-box) all accounts, home region only, as this is where the Org/account CloudTrail exists - Creates and configures AWS budgets w/alerting (customizable per OU and per account) once per account, global scope - Configures email alerting for CloudTrail Metric Alarms, Firewall Manager Events, Security Hub Findings incl. GuardDuty Findings"},{"location":"installation/what-we-do-where/#general","title":"General","text":"<ul> <li>\"defined\" region, \"defined\" account, means \"customer defined\", either at installation, upgrade, or any time they decide to reconfigure</li> <li>all items are created per customer defined parameters and configurations and are fully customizable without changing a single line of code</li> <li>security services are enabled and deployed globally, but, each service can be disabled per region. A single region deployment is possible.</li> <li>customer can enable/disable features, or change the configuration of each feature in the Accelerator config file</li> <li>customers can evolve their configurations over time, as they evolve and as their requirements change, without the requirement for code changes or professional services</li> </ul>"},{"location":"installation/what-we-do-where/#region-support","title":"Region support","text":"<ul> <li>All AWS commercial regions are supported. Lack of availability of CodeBuild, CodeCommit, or AWS Organizations in the Accelerator primary or installation region will prevent installation directly in that region. In these cases, customers can select a different installation region and the Accelerator can remotely deploy configurations and guardrails to that unsupported installation region.</li> <li>Prior to v1.2.5, we utilized a single StackSet, which blocked several additional installation regions. The Accelerator no longer leverages any StackSets, unblocking installing directly in several additional regions.</li> <li>As most features can be toggled on/off (per region), we expect most regions should be supportable both as a primary (or installation) region with the three above noted exceptions, and in these cases should still be fully supported as a managed (or secondary) region.</li> <li>Opt-in regions are not yet supported, but given enough demand, could easily be added.</li> </ul> <p>...Return to Accelerator Table of Contents</p>"},{"location":"operations/","title":"Accelerator Operations &amp; Troubleshooting Guide","text":"<p>This document is targeted at individuals installing or executing the AWS Secure Environment Accelerator. It is intended to guide individuals who are executing the Accelerator by providing an understanding as to what happens at each point throughout execution and to assist in troubleshooting state machine failures and/or errors. This is one component of the provided documentation package and should be read after the Installation Guide, but before the Developer Guide.</p> <ul> <li>System Overview</li> <li>Troubleshooting</li> <li>Common Tasks</li> </ul>"},{"location":"operations/common-tasks/","title":"1. Common Tasks","text":""},{"location":"operations/common-tasks/#11-restart-the-state-machine","title":"1.1. Restart the State Machine","text":"<p>The state machine can be stopped and restarted at any time. The Accelerator has been design to be able to rollback to a stable state, such that should the state machine be stopped or fail for any reason, subsequent state machine executions can simply proceed through the failed step without manual cleanup or issues (assuming the failure scenario has been resolved). An extensive amount of effort was placed on ensuring seamless customer recovery in failure situations. The Accelerator is idempotent - it can be run as many or as few times as desired with no negative effect. On each state machine execution, the state machine, primarily leveraging the capabilities of CDK, will evaluate the delta's between the old previously deployed configuration and the new configuration and update the environment as appropriate.</p> <p>The state machine will execute:</p> <ul> <li>automatically after each execution of the Code Pipeline (new installs, code upgrades, or manual pipeline executions)</li> <li>automatically when new AWS accounts are moved into any Accelerator controller OU in AWS Organizations</li> <li>when someone manual starts it: <code>Step Functions</code>, <code>ASEA-MainStateMachine_sm</code>, <code>Start Execution</code>, <code>Start Execution</code> (leave default values in name and json box)</li> </ul> <p>The state machine prevents users from accidentally performing certain major breaking changes, specifically unsupported AWS platform changes, changes that will fail to deploy, or changes that could be catastrophic to users. If someone knows exactly what they are doing and the full implications of these changes, we provide the option to override these checks. Customers should expect that items we have blocked CANNOT be changed after the Accelerator installation.</p> <p>These flags should be used with extreme caution. Specifying any of these flags without proper guidance will likely leave your Accelerator in a state of disrepair. These flags were added for internal purposes only - we do NOT support customers providing these flags.</p> <p>Providing this parameter to the state machine overrides all checks:</p> <pre><code>{\n\"overrideComparison\": true\n}\n</code></pre> <p>Providing any one or more of the following flags will only override the specified check(s):</p> <pre><code>{\n\"configOverrides\": {\n\"ov-global-options\": true,\n\"ov-del-accts\": true,\n\"ov-ren-accts\": true,\n\"ov-acct-email\": true,\n\"ov-acct-ou\": true,\n\"ov-acct-vpc\": true,\n\"ov-acct-subnet\": true,\n\"ov-tgw\": true,\n\"ov-mad\": true,\n\"ov-ou-vpc\": true,\n\"ov-ou-subnet\": true,\n\"ov-share-to-ou\": true,\n\"ov-share-to-accounts\": true,\n\"ov-nacl\": true,\n\"ov-nfw\": true\n}\n}\n</code></pre> <p>Providing this value allows for the forced rebuilding of the DynamoDB Outputs table:</p> <pre><code>{\n\"storeAllOutputs\": true\n}\n</code></pre>"},{"location":"operations/common-tasks/#12-switch-to-a-managed-account","title":"1.2. Switch To a Managed Account","text":"<p>To switch from the root account to a managed account you can click on your account name in the AWS Console. Then choose <code>Switch Role</code> in the menu.</p> <p></p> <p>In the page that appears next you need to fill out the account ID of the managed account you want to switch to. Next, you need to enter the role name defined in <code>organization-admin-role</code> (default: <code>AWSCloudFormationStackSetAdministrationRole</code>). And lastly, you need to enter a relevant name so you can later switch roles by using this name.</p> <p>TBD: This role may be locked down starting in v1.2.5 - Update process once direction finalized</p> <p>Caution: This mechanism is ONLY to be used for troubleshooting Accelerator problems. This role is outside the Accelerator governance process and bypasses all the preventative guardrails that protect the Accelerator contructs and prevent users from performing activities in violation of the security guardrails. This role should NOT be used outside this context, all users should be authenticating and logging into the environment through AWS SSO.</p> <p></p> <p>After switching to the managed account, the AWS Console header will look like the following image.</p> <p></p> <p>You can switch to the same account again quickly by clicking the name you entered previously in the menu.</p> <p></p> <p>[1]: https://docs.aws.amazon.com/cdk/latest/guide/home.html</p>"},{"location":"operations/operations-import-ALZAccount/","title":"1. How to migrate an AWS Landing Zone (ALZ) account \"as is\" into an AWS Secure Environment Accelerator (ASEA)","text":""},{"location":"operations/operations-import-ALZAccount/#11-overview","title":"1.1. Overview","text":"<p>This document describes the steps to migrate an existing linked account from an AWS Landing Zone (ALZ) to an AWS Secure Environment Accelerator (ASEA).</p>"},{"location":"operations/operations-import-ALZAccount/#12-prerequisites-setup","title":"1.2. Prerequisites / Setup","text":""},{"location":"operations/operations-import-ALZAccount/#121-confirm-asea-sso-and-ou-configuration","title":"1.2.1. Confirm ASEA SSO and OU configuration","text":"<p>On the ASEA, setup and run initial tests with SSO and permission sets with an account under the OU where the linked account will be migrated to. Confirm that SSO is properly configured with permissions required for the team members whose account is being migrated. This would include configuration of the ASEA\u2019s AWS Managed Active Directory (MAD) which should align with how the team migrating their account has their AWS SSO and MAD configured today.</p>"},{"location":"operations/operations-import-ALZAccount/#122-switch-the-alz-linked-account-payment-method-to-invoicing","title":"1.2.2. Switch the ALZ linked account payment method to invoicing","text":"<p>If working with your AWS account team (TAM/SA) they will reach out to an internal team within AWS to have the linked account payment method switched to invoicing. This way the customer doesn\u2019t have to enter a credit card when making the account standalone in the upcoming steps.</p>"},{"location":"operations/operations-import-ALZAccount/#123-confirm-console-access-to-the-alz-linked-account-and-also-to-the-email-account","title":"1.2.3. Confirm console access to the ALZ linked account and also to the email account","text":"<p>Confirm you have access to login as root to the ALZ linked account AWS console. Confirm you have access to the email account associated to the ALZ linked account. The upcoming steps will first make the account standalone (remove from ALZ organizations) so you need to make sure you have root access to the account. If required, you can reset the password following: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_change-root.html</p>"},{"location":"operations/operations-import-ALZAccount/#124-if-an-enterprise-support-es-customer-then-confirm-es-is-enabled-on-the-alz-linked-account","title":"1.2.4. If an Enterprise Support (ES) customer, then confirm ES is enabled on the ALZ linked account","text":"<p>If the ALZ management account is on Enterprise Support (ES), then make sure ES is enabled on the linked account being migrated to the ASEA. If its not, then raise a support case to activate ES on the linked account. This is to make sure an ES support case can be created and escalated during step 2 if any unforeseen issue occurs.</p>"},{"location":"operations/operations-import-ALZAccount/#125-confirm-the-alz-codepipeline-is-executing-successfully","title":"1.2.5. Confirm the ALZ CodePipeline is executing successfully","text":"<p>Make sure the ALZ CodePipeline is still running successfully. Execute the ALZ CodePipeline from the management account to make sure it runs successfully.</p> <ul> <li>AWS Console -&gt; CodePipeline</li> <li>Select \u201cAWS-Landing-Zone-CodePipeline\u201d</li> <li>Select \u201cRelease Change\u201d</li> <li>Click on the pipeline and confirm it successfully runs through to completion</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#126-confirm-cli-access-and-setup-python-and-the-aws-python-sdk-boto3","title":"1.2.6. Confirm CLI access and setup Python and the AWS Python SDK (boto3)","text":"<p>Confirm SSO temporary command line access from the management account with AdminAccess.</p> <ul> <li>SSO login \u2192 Select linked account \u2192 \u201cCommand line or programmatic access\u201d<ul> <li>Select Option 2 and add to your AWS credentials file under \u201c[default]\u201c</li> <li>This is required as the python script in step 3 takes a \u201cprofile\u201d parameter</li> </ul> </li> <li>Confirm you have the AWS CLI tool installed.<ul> <li>https://aws.amazon.com/cli/</li> <li>Confirm by running a command such as \u201caws s3 ls\u201d</li> </ul> </li> <li>Confirm you have python3 and the AWS python library (boto3) installed which is required in step 2 to confirm the account has been disassociated from the landing zone correctly.<ul> <li>https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html</li> </ul> </li> </ul>"},{"location":"operations/operations-import-ALZAccount/#13-landing-zone-disassociate-the-account-from-the-alz","title":"1.3. Landing Zone - Disassociate the account from the ALZ","text":"<ul> <li>Login to the ALZ management account, and go to \u201cService Catalog\u201d -&gt; \u201cProvisioned products\u201d</li> <li>Select \u201cAccess Filter\u201d -&gt; \u201cAccount\u201d to see a list of the account products</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#131-select-the-product-for-the-specific-linked-account","title":"1.3.1. Select the product for the specific linked account","text":"<ul> <li>Put the linked account name in the provisioned products search bar</li> <li>This will narrow down the list and show a product name \u201cAWS-Landing-Zone-Account-Vending-Machine\u201d with a name \u201clzapplicaitons*\u201d <li>Select that product and then \u201cActions-&gt;Terminate\u201d</li>"},{"location":"operations/operations-import-ALZAccount/#132-confirm-the-product-successfully-terminates","title":"1.3.2. Confirm the product successfully terminates","text":"<ul> <li>The provisioned product entry will show a status of \u201cUnder change\u201d</li> <li>You can also verify by going to CloudFormation\u2192Stacks and you will see \u201cDELETE IN PROGRESS\u201d for the AVM Template stack being deleted.<ul> <li>Go to the Resources tab to see the deleted resources associated to this stack.</li> </ul> </li> <li>Once the provisioned product no longer says \u201cUnder change\u201d move to the next step.</li> <li>Please note, this can take 1-2 hours.</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#133-go-to-the-linked-account-assume-role","title":"1.3.3. Go to the linked account (assume role)","text":"<ul> <li>From the management account, assume the role \u201cAWSCloudFormationStackSetExecutionRole\u201d to the linked account<ul> <li>or optionally, SSO with console access to that account</li> </ul> </li> </ul>"},{"location":"operations/operations-import-ALZAccount/#134-under-cloudformation-verify-that-the-alz-stacks-stacksets-from-alz-mgmt-were-deleted","title":"1.3.4. Under \u201cCloudFormation\u201d verify that the ALZ Stacks (StackSets from ALZ mgmt) were deleted","text":"<ul> <li>There should be no stack left in the linked account with the prefix \u201cStackSet-AWS-Landing-Zone-Baseline*\". For example:<ul> <li>StackSet-AWS-Landing-Zone-Baseline-CentralizedLoggingSpoke-</li> <li>StackSet-AWS-Landing-Zone-Baseline-EnableConfigRules-</li> <li>StackSet-AWS-Landing-Zone-Baseline-EnableNotifications-</li> <li>StackSet-AWS-Landing-Zone-Baseline-EnableConfigRulesGlobal-</li> <li>StackSet-AWS-Landing-Zone-Baseline-EnableConfig-</li> <li>StackSet-AWS-Landing-Zone-Baseline-ConfigRole-</li> <li>StackSet-AWS-Landing-Zone-Baseline-IamPasswordPolicy-</li> <li>StackSet-AWS-Landing-Zone-Baseline-SecurityRoles-</li> <li>StackSet-AWS-Landing-Zone-Baseline-EnableCloudTrail-</li> </ul> </li> </ul>"},{"location":"operations/operations-import-ALZAccount/#135-verify-that-the-account-is-ready-to-be-invited-and-baselined-by-the-asea","title":"1.3.5. Verify that the account is ready to be invited and baselined by the ASEA","text":"<ul> <li>You need to ensure that resources don\u2019t exist in the default VPC, there is no config recorder channel, no CloudTrail Trail and STS is active in all regions.</li> <li>This can be done manually, but ideally use this python script that can be run as well to automate the verification<ul> <li>https://github.com/paulbayer/Inventory_Scripts/blob/mainline/ALZ_CheckAccount.py</li> <li>mkdir test; cd test</li> <li>git clone https://github.com/paulbayer/Inventory_Scripts.git</li> <li>python3 ALZ_CheckAccount.py -a LINKED ACCOUNT_HERE -p default</li> </ul> </li> <li>It will run through 5 steps and output the following. If you were to run this script before the \u201cterminate\u201d step above is complete you would have warnings in steps 2 and 3 below.<ul> <li>Step 0 completed without issues</li> <li>Checking account 111122223333 for default VPCs in any region</li> <li>Step 1 completed with no issues</li> <li>Checking account 111122223333 for a Config Recorders and Delivery Channels in any region</li> <li>Step 2 completed with no issues</li> <li>Checking account 111122223333 for a specially named CloudTrail in all regions</li> <li>Step 3 completed with no issues</li> <li>Checking account 111122223333 for any GuardDuty invites</li> <li>Step 4 completed with no issues</li> <li>Checking that the account is part of the AWS Organization.</li> <li>Step 5 completed with no issues</li> <li>We've found NO issues that would hinder the adoption of this account ****</li> </ul> </li> </ul>"},{"location":"operations/operations-import-ALZAccount/#14-landing-zone-alz-remove-the-account-from-the-alz-organizations-and-make-standalone","title":"1.4. Landing Zone (ALZ) - Remove the account from the ALZ organizations and make standalone","text":"<p>Removing the account from the ALZ organizations and making it standalone is required so it can be invited into the ASEA organization.</p>"},{"location":"operations/operations-import-ALZAccount/#141-read-the-following-summaryconsiderations","title":"1.4.1. Read the following summary/considerations","text":"<ul> <li>https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#142-verify-access","title":"1.4.2. Verify access","text":"<ul> <li>As stated in the previous sections, verify you have a mechanism to access the account post leaving the ALZ organization<ul> <li>Former SSO roles will no longer function nor will the \u201cAWSCloudFormationStackSetExecutionRole\u201d role as it will have a trust relationship to the ALZ management account.</li> <li>Confirm the root credentials have been recovered and are usable</li> <li>As an alternative, confirm access with a new role/IAM user with Admin permissions on the account</li> </ul> </li> </ul>"},{"location":"operations/operations-import-ALZAccount/#143-verify-billing-flipped-to-invoicing","title":"1.4.3. Verify billing flipped to invoicing","text":"<ul> <li>As stated in the previous sections, verify the account payment method has been flipped to \u201cinvoicing\u201d to avoid having to enter a Credit Card when going standalone. This can be done working with your AWS account team who will coordinate internally, or by raising a support case describing the use case.</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#144-remove-the-account-from-the-organizations-and-make-standalone","title":"1.4.4. Remove the account from the organizations and make standalone","text":"<ul> <li>Follow the instructions on the following link to remove the account</li> <li>The short version is select the account from the ALZ mgmt account Organizations and select \"remove\"</li> <li>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_remove.html</li> <li>https://aws.amazon.com/blogs/security/aws-organizations-now-supports-self-service-removal-of-accounts-from-an-organization</li> <li>Note, when moving the account standalone do not select Enterprise Support. You shouldn't get a popup dialog asking for a Credit Card and the Support level since the account should have been moved to invoicing. Support can be reenabled on the linked account once it\u2019s invited into the ASEA organization.</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#15-accelerator-invite-the-account-into-its-organization","title":"1.5. Accelerator - Invite the account into its organization","text":""},{"location":"operations/operations-import-ALZAccount/#151-from-the-asea-mgmt-account-send-an-invite-to-the-standalone-account","title":"1.5.1. From the ASEA mgmt account, send an invite to the standalone account","text":"<ul> <li>Follow the instructions on the following link to invite the account</li> <li>The short version is go to the ASEA mgmt account organizations and select \"Add an account\" -&gt; \"Invite existing account\" -&gt; \"enter the linked account account ID\"</li> <li>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#152-in-the-former-alz-account-accept-the-invitation","title":"1.5.2. In the former ALZ account, Accept the invitation","text":"<ul> <li>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html#orgs_manage_accounts_accept-decline-invite</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#153-keep-the-linked-account-at-the-root-level-of-the-organizations","title":"1.5.3. Keep the linked account at the root level of the Organizations","text":"<ul> <li>Verify access to the linked account using your root login credentials</li> <li>If you had created an IAM role/user with Admin permissions, then verify access as well</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#154-activate-enterprise-support-es-on-this-linked-account","title":"1.5.4. Activate Enterprise Support (ES) on this linked account","text":"<ul> <li>If ES is enabled on the ASEA management account, open a support case to enable ES on this linked account</li> <li>Go to the Support center and create a billing support case with \"Account\" and \"Activation\"</li> <li>Subject \"Requesting ES enablement on linked account\"</li> <li>Body \"Requesting ES enablement on linked account \" <li>Your AWS TAM can escalate the case with the support team if it\u2019s time sensitive.</li> <li>This is to make sure an ES support case can be created and escalated during the next steps if any unforeseen issue occurs.</li>"},{"location":"operations/operations-import-ALZAccount/#155-update-or-add-the-organization-adming-role-so-one-can-assume-the-role-into-the-linked-account","title":"1.5.5. Update (or add) the Organization Adming Role so one can assume the role into the linked account","text":"<ul> <li>Login to the linked account which just joined the organization.</li> <li>Create a new Organization Admin role, as defined in the customers config file: \"organization-admin-role\": \"OrganizationAccountAccessRole\".</li> <li>With newer customers the default is \"OrganizationAccountAccessRole, with older customers it is \"AWSCloudFormationStackSetExecutionRole\".</li> <li>If \"AWSCloudFormationStackSetExecutionRole\" then you can edit the trust relationship directly<ul> <li>Go to IAM -&gt; Role -&gt; AWSCloudFormationStackSetExecutionRole</li> <li>Update the trust relationship to have the management account ID of the ASEA (instead of the account ID of the previous ALZ)</li> </ul> </li> <li>Verify that you can assume this role from the management account into the linked account</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#16-accelerator-move-the-linked-account-from-the-top-level-root-ou-into-the-appropriate-ou-managed-by-the-asea","title":"1.6. Accelerator - Move the linked account from the top level root OU into the appropriate OU managed by the ASEA","text":""},{"location":"operations/operations-import-ALZAccount/#161-plan-what-ou-this-account-will-be-moved-into","title":"1.6.1. Plan what OU this account will be moved into","text":"<ul> <li>Option 1 - Create a new OU and move the account into that OU<ul> <li>Before the migration, the team would have created a new OU (ie-similar to the sandbox OU).</li> <li>This would be needed if they need to isolate this account from TGW attachments/Networking and want to keep it isolated.</li> <li>The state machine will run and start to baseline the account.</li> <li>It will create a new VPC and deploy resources using CFN such as Config, CloudTrail, etc.</li> <li>Note, if the OU is setup similar to the sandbox OU it does not provide access to the shared VPCs that have the TGW attachments.</li> <li>Creating a new OU also requires adding that new OU and the OU persona to the config file in advance of the next state machine execution.</li> </ul> </li> <li>Option 2 - Move account into an existing OU (ie-prod)<ul> <li>The state machine will run and start to baseline the account.</li> <li>It will create a new VPC and deploy resources using CFN such as Config, CloudTrail, etc.</li> <li>The customers existing VPC will remain, as a 2nd DETACHED VPC.</li> <li>Mote. if it is non-compliant to security rules, it remains non-compliant and needs to be cleaned up and brought into compliance</li> <li>If the VPC is compliant and it has unique IP addresses, it could be attached to the TGW.</li> </ul> </li> </ul>"},{"location":"operations/operations-import-ALZAccount/#162-move-the-account-from-the-root-ou-to-the-correct-ou","title":"1.6.2. Move the account from the root OU to the correct OU","text":"<ul> <li>THIS CANNOT BE EASILY UNDONE - MAKE SURE YOU MOVE TO THE CORRECT OU</li> <li>Follow the instructions on the following link to move the account to the correct OU</li> <li>The short version is go to the ASEA management account organizations and \"select the account\" -&gt; \"actions\" -&gt; \"move\" -&gt; \"select the correct OU\"</li> <li>NOTE: The ASEA state machine will automatically start within 1-2 minutes of the account being moved into the OU</li> <li>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html#move_account_to_ou</li> <li>Verify that the ASEA main state machine (under AWS-&gt;Step Functions) is triggered and runs cleanly (~30-45 minutes)</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#17-accelerator-asea-verify-access-control-with-roles-sso-etc","title":"1.7. Accelerator (ASEA) - Verify access control with roles, SSO, etc","text":"<ul> <li>Update and verify SSO and permission sets for the linked account now part of the ASEA</li> <li>Verify you still have access to the linked account via root (or other mechanisms)</li> <li>Verify you still can assume the operations role into the linked account</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#18-landing-zone-close-down-the-alz-core-accounts-and-then-the-management-account","title":"1.8. Landing Zone - Close down the ALZ core accounts and then the management account","text":"<p>Once all workloads have been migrated from the ALZ to the ASEA, then you may decide to shutdown your ALZ.</p>"},{"location":"operations/operations-import-ALZAccount/#181-close-down-the-alz-linked-accounts","title":"1.8.1. Close down the ALZ linked accounts","text":"<ul> <li>Close all the linked accounts \u201cas is\u201d without making them standalone</li> <li>This will be the ALZ core linked accounts, but you might have some remaining workload accounts you decided not to migrate to the ASEA.</li> <li>https://aws.amazon.com/premiumsupport/knowledge-center/close-aws-account</li> <li>The management account will remain with organizations and the core accounts will show as suspended for 90 days.</li> </ul>"},{"location":"operations/operations-import-ALZAccount/#182-close-down-the-alz-management-account","title":"1.8.2. Close down the ALZ management account","text":"<ul> <li>After 90 days, the suspended linked accounts will be completely closed</li> <li>Go to the root account and turn off Organizations and then close the root account</li> </ul>"},{"location":"operations/system-overview/","title":"1. System Overview","text":"<p>This document is targeted at individuals installing or executing the AWS Secure Environment Accelerator. It is intended to guide individuals who are executing the Accelerator by providing an understanding as to what happens at each point throughout execution and to assist in troubleshooting state machine failures and/or errors. This is one component of the provided documentation package and should be read after the Installation Guide, but before the Developer Guide.</p>"},{"location":"operations/system-overview/#11-overview","title":"1.1. Overview","text":"<p>The system can be thought of in two levels. The first level of the system consists of Accelerator stacks and resources. Let's call these the Accelerator-management resource. The second level of the system consists of stacks and resources that are deployed by the Accelerator-management resource. Let's call these the Accelerator-managed resources. The Accelerator-management resources are responsible for deploying the Accelerator-managed resources.</p> <p>There are two Accelerator-management stacks:</p> <ul> <li>the <code>Installer</code> stack that is responsible for creating the next listed stack;</li> <li>the <code>Initial Setup</code> stack. This stack is responsible for reading configuration file and creating Accelerator-managed resources in the relevant accounts.</li> </ul> <p>There are multiple Accelerator-managed stacks. Currently there are as many as twelve Accelerator-managed stacks per managed account.</p> <p>The figure below shows a zoomed-out overview of the Accelerator. The top of the overview shows the Accelerator-management resources, i.e. the <code>Installer</code> stack and the <code>Initial Setup</code> stack. The bottom of the overview shows the Accelerator-managed resources in the different accounts.</p> <p></p>"},{"location":"operations/system-overview/#12-installer-stack","title":"1.2. Installer Stack","text":"<p>The Accelerator-management <code>Installer</code> stack contains the necessary resources to deploy the Accelerator-management <code>Initial Setup</code> stack in an AWS account. This AWS account will be referred to as the 'root' account in this document.</p> <p></p> <p>The Installer stack consists of the following resources:</p> <ul> <li><code>ASEA-InstallerPipeline</code>: this is a <code>AWS::CodePipeline::Pipeline</code> that pulls the latest Accelerator code from GitHub. It launches the CodeBuild project <code>ASEA-InstallerProject_pl</code>, executes the <code>ASEA-Installer-SaveApplicationVersion</code> Lambda and launches the Accelerator state machine.</li> <li><code>ASEA-InstallerProject_pl</code>: this is a <code>AWS::CodeBuild::Project</code> that installs the Accelerator in AWS account.</li> <li><code>ASEA-Installer-SaveApplicationVersion</code>: this is a <code>AWS::Lambda::Function</code> that stores the current Accelerator version into Parameter Store.</li> <li><code>ASEA-Installer-StartExecution</code>: this is a <code>AWS::Lambda::Function</code> that launches the Accelerator after CodeBuild deploys the Accelerator.</li> <li>Creation of AWS::DynamoDB::Table - <code>ASEA-Parameters</code> and <code>ASEA-Outputs</code> which are used for the internal operation of the Accelerator. <code>ASEA-Outputs</code> is used to share CloudFormation stack outputs between regions, stacks and phases. <code>ASEA-Parameters</code> is used to various configuration items like managed accounts, organizations structure, and limits.</li> </ul> <p></p> <p>The <code>ASEA-InstallerPipeline</code> starts when first installed using the CloudFormation template. The administrator can also start the pipeline manually by clicking the <code>Release Change</code> button in the AWS Console.</p> <p></p> <p>This starts the <code>ASEA-InstallerProject_pl</code> CodeBuild project. The CodeBuild project uses the GitHub source artifact. The CodeBuild projects spins up a new Linux instances and installs the Accelerator dependencies and starts the deployment of the Accelerator using the AWS Cloud Development Kit (CDK1).</p> <p>CDK bootstraps its environment and creates the <code>CDKToolkit</code> stack in the AWS account. It creates the S3 bucket <code>cdktoolkit-stagingbucket-*</code> and the ECR repository <code>aws-cdk/assets</code>.</p> <p>CDK copies assets to the bootstrap bucket and bootstrap repository that are used by the Accelerator. The assets that are stored on S3 include default IAM policies, default SCPs, default firewall configuration. The assets that are pushed to ECR include the Accelerator Docker build image. This Docker image is responsible for deploying Accelerator resources using the CDK.</p> <p>CDK finally deploys the <code>Initial Setup</code> stack. The Accelerator state machine is described in the next section.</p> <p>This diagram depicts the Accelerator Installer CodePipeline as of v1.2.1:</p> <p></p> <p>Once the Code Pipeline completes successfully:</p> <ul> <li>the Accelerator codebase was pulled from GitHub</li> <li>the Accelerator codebase was deployed/installed in the Organization Management (root) AWS account</li> <li>parameter store <code>/accelerator/version</code> was updated with the new version information<ul> <li>this provides a full history of all Accelerator versions and upgrades</li> </ul> </li> <li>the newly installed Accelerator state machine is started</li> </ul> <p>At this time the resources deployed by the Installer Stack are no longer required. The Installer stack could be removed (which would remove the Code Pipeline) with no impact on Accelerator functionality.</p> <p>If the Installer Stack was removed, it would need to be re-installed to upgrade the Accelerator. If the stack was not removed, an Accelerator codebase upgrade often only requires updating a single stack parameter to point to the latest Accelerator code branch, and re-releasing the pipeline. No files to manually copy, change or update, an upgrade can be initiated with a simple variable update.</p> <p></p>"},{"location":"operations/system-overview/#13-initial-setup-stack","title":"1.3. Initial Setup Stack","text":"<p>The Accelerator-management <code>Initial Setup</code> stack, named <code>ASEA-InitialSetup</code>, consists of a state machine, named <code>ASEA-MainStateMachine_sm</code>, that executes various steps to create the Accelerator-managed stacks and resources in the Accelerator-managed accounts. Using a state machine, we can clearly define the deployment process and systematically control branches of execution and handle exceptions.</p> <p>The Accelerator comprises a primary state machine <code>ASEA-MainStateMachine_sm</code>, and nine supporting state machines (as of v1.2.1). Customer will only ever Execute the <code>ASEA-MainStateMachine_sm</code>. All troubleshooting will also typically begin with the <code>ASEA-MainStateMachine_sm</code>.</p> <p></p> <p>The image below depicts the latest state <code>ASEA-MainStateMachine_sm</code> machine. Each green or white square in the image represents a step in the state machine. This all green diagram represents a successul Accelerator state machine execution.</p> <p></p> <p>Notice the extremely linear state machine design. This was done to reduce complexity and ease troubleshooting. You may notice a small fork in the state machine. The left path is executed if the Accelerator is deployed on top of an ALZ, The right path is executed if the Accelerator is deployed as a standalone solution. If we eventually add a Control Tower deployment option, it is likely it could also leverage the existing ALZ path.</p> <p>The state machine contains three different types of steps:</p> <ol> <li>steps that execute a Lambda function;</li> <li>steps that start another state machine, e.g. <code>Create Accounts</code> step;</li> <li>steps that start another state machine that starts a CodeBuild project, e.g. the <code>Phase {-1,0,1,2,3,4,5}</code> steps.</li> </ol> <p>The stack additionally consists of the following resources:</p> <ul> <li>AWS::CodeBuild::Project<ul> <li><code>ASEA-Deploy</code> or <code>ASEA-DeployPrebuilt</code></li> </ul> </li> <li>AWS::CodeCommit::Repository<ul> <li><code>ASEA-Config-Repo</code></li> </ul> </li> <li>AWS::IAM::Role<ul> <li><code>ASEA-L-SFN-MasterRole</code></li> <li><code>ASEA-L-SFN-Execution</code></li> </ul> </li> <li>AWS::Lambda::Function<ul> <li>A Lambda function for every Lambda function step in the state machine.</li> </ul> </li> <li>AWS::StepFunctions::StateMachine<ul> <li><code>ASEA-ALZCreateAccount_sm</code>: See Create Landing Zone Account;</li> <li><code>ASEA-OrgCreateAccount_sm</code>: See Create Organization Account;</li> <li><code>ASEA-InstallCfnRoleMaster_sm</code>: See Install CloudFormation Execution Role;</li> <li><code>ASEA-InstallRoles_sm</code>: See Install Execution Roles;</li> <li><code>ASEA-DeleteDefaultVpcs_sfn</code>: See Delete Default VPCs;</li> <li><code>ASEA-CodeBuild_sm</code>: See Deploy Phase 0;</li> <li><code>ASEA-CreateConfigRecorder_sfn</code>: See Create Config Recorders;</li> <li><code>ASEA-CreateAdConnector_sm</code>: See Create AD Connector;</li> <li><code>ASEA-StoreOutputs_sm</code>: See Share Outputs - new in v1.2.1.</li> </ul> </li> </ul> <p>Note: Most resources have a random suffix to their name. This is because we use CDK to deploy the resources. See https://docs.aws.amazon.com/cdk/latest/guide/identifiers.html#identifiers_logical_ids</p>"},{"location":"operations/system-overview/#131-get-or-create-configuration-from-s3","title":"1.3.1. Get or Create Configuration from S3","text":"<p>This step calls a Lambda function that finds or creates the configuration repository. Finds the configuration file(s) in the CodeCommit repository. If the configuration file cannot be found in the repository it is copied from the customer's S3 configuration bucket. If the copy is successful then the configuration file(s) in the S3 bucket will be removed.</p> <p>The configuration file <code>config.json</code> or <code>config.yaml</code> is parsed and validated. This step will fail if both file types exist, the configuration file is not valid JSON or YAML or does not adhere to the configuration file specification. Internally the Accelerator always leverages JSON, but accepts JSON or YAML as the source input file and converts it to JSON prior to each execution, storing the converted and fully expanded file if in the raw folder.</p> <p></p>"},{"location":"operations/system-overview/#132-get-baseline-from-configuration","title":"1.3.2. Get Baseline from Configuration","text":"<p>This step calls a Lambda function that gets the <code>alz-baseline</code> of the configuration file to decide which path in the state machine will be taken.</p>"},{"location":"operations/system-overview/#133-compare-configurations","title":"1.3.3. Compare Configurations","text":"<p>This step calls a Lambda function that compares the previous version of the configuration file with the current version of the configuration file. The previous configuration file CodeCommit commit id is stored in the secret <code>accelerator/config/last-successful-commit</code> in AWS Secrets Manager in the root account.</p> <p>The following configuration file changes are not allowed:</p> <ul> <li>changing ALZ baseline;</li> <li>changing root account or region;</li> <li>changing central log services account or region;</li> <li>changing the organizational unit, name or email address of an account;</li> <li>removing an account;</li> <li>changing the name, CIDR or region of a VPC;</li> <li>disabling a VPC;</li> <li>changing the name, availability zone, CIDR of a subnet;</li> <li>disabling or removing a subnet;</li> <li>changing the name, ASN, region or features of a transit gateway;</li> <li>changing the ID, VPC, subnet, region, size, DNS, Netbios of a Managed Active Directory;</li> <li>disabling a Managed Active Directory;</li> <li>changing the ASN of a virtual private gateway;</li> <li>changing the sharing to accounts of a VPC;</li> <li>changing the NACLs of a subnet.</li> </ul> <p>It is possible to ignore certain configuration file changes. See Restart the State Machine how to pass these options to the state machine.</p>"},{"location":"operations/system-overview/#134-load-landing-zone-configuration","title":"1.3.4. Load Landing Zone Configuration","text":"<p>Executed only when using AWS Landing Zone baseline</p> <p>This step calls a Lambda function that validates the ALZ deployment and configuration.</p> <p>This step fails when</p> <ul> <li>an existing ALZ deployment in the root account can not be found;</li> <li>an organizational unit that is used by ALZ or the Accelerator can not be found;</li> <li>an account is present in the Accelerator configuration but does not exist in AWS Organizations;</li> <li>an account has a name, email address or organizational unit that is different from the ALZ configuration;</li> <li>an account has a name, email address or organizational unit that is different from the Accelerator configuration;</li> <li>the organizational units in the ALZ configuration are different from the ones in AWS Organizations;</li> <li>the account type of ALZ configuration accounts can not be detected;</li> <li>the accounts of type <code>primary</code>', <code>security</code>, <code>log-archive</code>, <code>shared-services</code> are missing from the ALZ configuration.</li> </ul>"},{"location":"operations/system-overview/#135-add-execution-role-to-service-catalog","title":"1.3.5. Add Execution Role to Service Catalog","text":"<p>Executed only when using AWS Landing Zone baseline</p> <p>This step calls a Lambda function that adds the state machine's IAM role to the ALZ Account Vending Machine (AVM) service catalog portfolio.</p>"},{"location":"operations/system-overview/#136-create-landing-zone-account","title":"1.3.6. Create Landing Zone Account","text":"<p>Executed only when using AWS Landing Zone baseline</p> <p>This step starts the <code>ASEA-ALZCreateAccount_sm</code> state machine. This state machine is responsible for creating an account using the AVM and waits for the account to be created and configured.</p>"},{"location":"operations/system-overview/#137-organizational-unit-ou-validation","title":"1.3.7. Organizational Unit (OU) Validation","text":"<p>Executed only when using AWS Organizations baseline</p> <p>This step validates that the OU structure defined in the Accelerator configuration file matches the OU and account structure defined in AWS Organizations. Missing OUs are created. If any AWS Accounts or OUs have been renamed, this step updates the configuration file with the latest information. Accounts located in the incorrect top-level OU cause a state machine failure.</p>"},{"location":"operations/system-overview/#138-load-organization-configuration","title":"1.3.8. Load Organization Configuration","text":"<p>Executed only when using AWS Organizations baseline</p> <p>This step calls a Lambda function that validates the Accelerator deployment and configuration.</p> <p>This step fails when</p> <ul> <li>an organizational unit that is used by the Accelerator can not be found;</li> <li>an account is present in the Accelerator configuration but does not exist in AWS Organizations;</li> <li>an account has a name, email address or organizational unit that is different from the Accelerator configuration.</li> <li>returns accounts that have not been created</li> <li>duplicates some validation functionality</li> </ul>"},{"location":"operations/system-overview/#139-install-cloudformation-role-in-root","title":"1.3.9. Install CloudFormation Role in root","text":"<p>Executed only when using AWS Organizations baseline</p> <p>This step starts the <code>ASEA-InstallCfnRoleMaster_sm</code> state machine. This state machine is responsible for creating the IAM role defined in <code>organization-admin-role</code> (default: <code>AWSCloudFormationStackSetAdministrationRole</code>) in the root account. You can read more about why this role is created here.</p>"},{"location":"operations/system-overview/#1310-create-organization-account","title":"1.3.10. Create Organization Account","text":"<p>Executed only when using AWS Organizations baseline</p> <p>This step starts the <code>ASEA-OrgCreateAccount_sm</code> state machine. This state machine is responsible for creating an account using the AWS Organizations and waits for the account to be created and configured. The newly created account will be moved to the correct organizational unit and will have an the <code>ASEA-Quarantine-New-Object</code> Service Control Policy (SCP) attached to it. The SCP will only be removed after the Accelerator state machine has completed.</p> <p>Application of this SCP ensures that new AWS accounts cannot be used until they have been properly configured and had all Accelerator guardrails applied per the configuration file. It is important that guardrails be consistently in place from the time a user first accesses a new AWS account, until the account is decommissioned. Should the state machine fail, still be in-progress, or has not yet processed the account (for whatever reason), users who access it will be prevented from doing anything.</p>"},{"location":"operations/system-overview/#1311-load-organizational-units","title":"1.3.11. Load Organizational Units","text":"<p>This step calls a Lambda function that loads all Organizational Units. The Organizational Units are stored in the DynamoDB table <code>ASEA-Parameters</code> in the root account. The Organizational Units will later be used to determine which configuration file options apply to an AWS account.</p>"},{"location":"operations/system-overview/#1312-load-accounts","title":"1.3.12. Load Accounts","text":"<p>This step calls a Lambda function that loads all accounts. The accounts are stored in the DynamoDB table <code>ASEA-Parameters</code> in the root account. The accounts will later be used to deploy stacks in the correct accounts.</p>"},{"location":"operations/system-overview/#1313-install-execution-roles","title":"1.3.13. Install Execution Roles","text":"<p>This step calls a Lambda function that creates stack sets in all Accelerator accounts. This stack sets contains a single resource, i.e. an IAM role <code>ASEA-PipelineRole</code> that can be assumed by the <code>ASEA-L-SFN-MasterRole</code>. This IAM role allows the root account to administer the Accelerator accounts.</p>"},{"location":"operations/system-overview/#1314-delete-default-vpcs","title":"1.3.14. Delete Default VPCs","text":"<p>This step starts the <code>ASEA-DeleteDefaultVpcs_sfn</code> state machine. This state machine is responsible for deleting default subnets, internet gateways and VPCs for all regions and accounts in the Accelerator configuration.</p> <p>This step fails when one or more default VPCs cannot be deleted. This step does not fail on the first error, it executes on all accounts/regions and then fails with a complete list of errors.</p>"},{"location":"operations/system-overview/#1315-load-limits","title":"1.3.15. Load Limits","text":"<p>This step calls a Lambda function that loads service quotas and requests a service quota increase according to the configuration file. When a service quota increase request has been closed but not increased, then the service quota request will be issued again when the creation of the last request was at least two days ago.</p>"},{"location":"operations/system-overview/#1316-enable-trusted-access-for-services","title":"1.3.16. Enable Trusted Access for Services","text":"<p>This step calls a Lambda function that is responsible for</p> <ul> <li>enabling AWS service access in the organization;</li> <li>enabling AWS Resource Access Manager sharing in the organization;</li> <li>creating a service-linked role for AWS IAM Access Analyzer;</li> <li>setting the security account as delegated administrator for AWS Firewall Manager;</li> <li>setting the security account as delegated administrator for AWS IAM Access Analyzer;</li> <li>setting the security account as delegated administrator for Amazon GuardDuty.</li> </ul>"},{"location":"operations/system-overview/#1317-store-all-phase-outputs","title":"1.3.17. Store All Phase Outputs","text":"<p>This step only executes on the first run of the state machine after it has been upgraded to v1.2.0 or above. This step exists solely to support upgrades from Accelerator versions prior to v1.2.0 and can be removed when no existing customers are running versions older than v1.2.0. This steps populates the DynamoDB Outputs table with the outputs from previous executions which were previously stored in S3 (and at one time even stored in secrets manager).</p>"},{"location":"operations/system-overview/#1318-deploy-phase-1-negative-one","title":"1.3.18. Deploy Phase -1 (Negative one)","text":"<ul> <li>The following resources are deployed in phase -1:<ul> <li>Creating required roles for macie custom resources</li> <li>Creating required roles for guardDuty custom resources</li> <li>Creating required roles for securityHub custom resources</li> <li>Creating required roles for IamCreateRole custom resource</li> <li>Creating required roles for createSSMDocument custom resource</li> <li>Creating required roles for createLogGroup custom resource</li> <li>Creating required roles for CWLCentralLoggingSubscriptionFilterRole custom resource</li> <li>Creating required roles for TransitGatewayCreatePeeringAttachment custom resource</li> <li>Creating required roles for TransitGatewayAcceptPeeringAttachment custom resource</li> <li>Creating required roles for createLogsMetricFilter custom resource</li> <li>Creating required roles for SnsSubscriberLambda custom resource</li> </ul> </li> </ul>"},{"location":"operations/system-overview/#1319-store-phase-1-output","title":"1.3.19. Store Phase -1 Output","text":"<p>This step calls a Lambda function that stores the outputs from the deployed stacks in sub-accounts in the DynamoDB table <code>ASEA-Outputs</code> in AWS Secrets Manager in the root account.</p>"},{"location":"operations/system-overview/#1320-deploy-phase-0","title":"1.3.20. Deploy Phase 0","text":"<p>This step starts the <code>ASEA-CodeBuild_sm</code> state machine. This state machine is responsible for starting a CodeBuild execution that deploys CloudFormation stacks in Accelerator-managed accounts using CDK.</p> <p></p> <p></p> <p>The same CodeBuild project is used to deploy all phases. The environment variable <code>ACCELERATOR_PHASE</code> in the CodeBuild execution decides which phase to deploy.</p> <p>The following resources are deployed in phase 0:</p> <ul> <li>create default EBS encryption key;</li> <li>create an AWS log bucket with encryption key;</li> <li>create the central log services bucket with encryption key;</li> <li>create the Accelerator configuration bucket with encryption key;</li> <li>copy artifacts to the Accelerator configuration bucket:<ul> <li>SCPs;</li> <li>firewall configuration;</li> </ul> </li> <li>account warming (step 1);</li> <li>set password policy (step 1);</li> <li>create IAM users (step 1):<ul> <li>create passwords and store in Secrets Manager;</li> </ul> </li> <li>create MAD deployment (step 1):<ul> <li>create passwords and store in Secrets Manager;</li> <li>create service-linked role;</li> </ul> </li> <li>create <code>rsyslog</code> deployment (step 1);</li> <li>create firewalls (step 1);</li> <li>create budgets (step 1);</li> <li>create transit gateways (step 1);</li> <li>create Route53 DNS logging log group;</li> <li>enable Macie (step 1);</li> <li>enable GuardDuty;</li> <li>enable Access Analyzer;</li> </ul>"},{"location":"operations/system-overview/#1321-store-phase-0-output","title":"1.3.21. Store Phase 0 Output","text":"<p>This step calls a Lambda function that stores the outputs from the deployed stacks in sub-accounts in the secret <code>ASEA-Outputs</code> in AWS Secrets Manager in the root account.</p>"},{"location":"operations/system-overview/#1322-verify-files","title":"1.3.22. Verify Files","text":"<p>This step verifies that all required files to complete the installation have been provided by the user. If any required files have not been provided, this step will fail and provide a list of all missing files.</p> <p>Why do we wait until so late in the state machine execution to perform this task?</p> <ol> <li>we do not want to add or delete files from a customer supplied bucket</li> <li>we do not want to force customers to need to copy and supply standard configuration files for 3. prescriptive installation</li> <li>we want to allow customers to override the sample or standard configuration files</li> <li>the config bucket is only created in Phase 0</li> <li>we copy all the relevant sample files from reference-artifacts folders to our config bucket, and then, we copy any customer supplied config files over top of our sample files</li> <li>This allows customers to override the sample configs, with customer custom configs without needing to either fork the repo, or supply the files on a vanilla install</li> <li>Until we do this file copy, we cannot be sure that the combination of customer supplied and reference-artifact sample files provides all the required files.</li> </ol>"},{"location":"operations/system-overview/#1323-create-config-recorders","title":"1.3.23. Create Config Recorders","text":"<p>This step starts the <code>ASEA-CreateConfigRecorder_sfn</code> state machine. This state machine is responsible for creating Config recorders in all accounts and regions.</p>"},{"location":"operations/system-overview/#1324-add-scps-to-organization","title":"1.3.24. Add SCPs to Organization","text":"<p>This step calls a Lambda function that creates and attaches the SCPs listed in the Accelerator configuration. The SCP policies are loaded from the Accelerator configuration bucket.</p> <p>This step fails when</p> <ul> <li>an SCP policy cannot be found in the Accelerator configuration bucket;</li> <li>an SCP could not be attached to an organizational unit or account, e.g. when the maximum number of attached SCPs is exceeded</li> </ul>"},{"location":"operations/system-overview/#1325-deploy-phase-1","title":"1.3.25. Deploy Phase 1","text":"<ul> <li>Create S3 Bucket in all accounts and replicate to Log Account Bucket</li> <li>Deploy VPC:<ul> <li>Vpc</li> <li>Subnets</li> <li>Subnet sharing (RAM)</li> <li>Route tables</li> <li>Internet gateways</li> <li>NAT gateways</li> <li>Interface endpoints</li> <li>Gateway endpoints</li> <li>Transit Gateway Attachments</li> <li>IAM Role required for VPC Peering Auto accept</li> </ul> </li> <li>Firewall images subscription check</li> <li>Creates the customer gateways for the EIPs of the firewall</li> <li>Create IAM Roles, Users in account based on configuration</li> <li>Creates the additional budgets for the account stacks.</li> <li>Import Certificates</li> <li>Setup SSMSessionManagerDocument</li> <li>Create Cost and Usage reports</li> <li>Enable Macie in root Account</li> <li>GuardDuty setup in Security Account</li> <li>Setup CWL Central Logging</li> <li>Create Roles required for Flow Logs</li> <li>Transit Gateway Peering</li> <li>Create LogGroup required for DNS Logging</li> </ul>"},{"location":"operations/system-overview/#1326-store-phase-1-output","title":"1.3.26. Store Phase 1 Output","text":"<p>See Deploy Phase 0.</p>"},{"location":"operations/system-overview/#1327-account-default-settings","title":"1.3.27. Account Default Settings","text":"<p>This step calls a Lambda function that</p> <ul> <li>enables and sets EBS default encryption for all accounts in the Accelerator configuration;</li> <li>enables S3 object level ALZ CloudTrail logging;</li> <li>enables Log Insight events;</li> <li>enables KMS encryption using the CMK from the central logging account;</li> <li>sets AWS Systems Manager Session Manager default configuration in every Accelerator-managed account in every region with a VPC.</li> </ul>"},{"location":"operations/system-overview/#1328-deploy-phase-2","title":"1.3.28. Deploy Phase 2","text":"<ul> <li>Create CloudTrail in root account</li> <li>Create VPC Peering Connection</li> <li>Create Security Groups for shared VPC in sub accounts</li> <li>Setup Security Hub in Security Account</li> <li>Setup Cross Account CloudWatch logs sharing by creating roles in sub accounts</li> <li>Enable VPC FlowLogs</li> <li>Create Active Directory (MAD)</li> <li>Create Firewall clusters</li> <li>Create Firewall Management instance</li> <li>Create Transit Gateway Routes, Association and Propagation</li> <li>Enable Macie in Security account and Create Members, Update Config</li> <li>GuardDuty - Add existing Org accounts as members and allow new accounts to be members and Publish</li> <li>Create SNS Topics in Log Account</li> <li>TGW Peering Attachments</li> </ul>"},{"location":"operations/system-overview/#1329-store-phase-2-output","title":"1.3.29. Store Phase 2 Output","text":"<p>See Deploy Phase 0.</p>"},{"location":"operations/system-overview/#1330-deploy-phase-3","title":"1.3.30. Deploy Phase 3","text":"<ul> <li>create peering connection routes;</li> <li>create ALB (step 1);</li> <li>create <code>rsyslog</code> deployment (step 2);</li> <li>create hosted zones, resolver rules and resolver endpoints and Share;</li> <li>Enable Security Hub and Invite Sub accounts as members;</li> <li>TransitGateway Peering attachment and routes;</li> <li>Macie update Session;</li> </ul>"},{"location":"operations/system-overview/#1331-store-phase-3-output","title":"1.3.31. Store Phase 3 Output","text":"<p>See Deploy Phase 0.</p>"},{"location":"operations/system-overview/#1332-deploy-phase-4","title":"1.3.32. Deploy Phase 4","text":"<ul> <li>SecurityHub Disable Controls</li> <li>Creates CloudWatch Metrics on LogGroups</li> <li>Associate Shared Resolver Rules to VPC</li> <li>Associate Hosted Zones to VPC</li> </ul>"},{"location":"operations/system-overview/#1333-store-phase-4-output","title":"1.3.33. Store Phase 4 Output","text":"<p>See Deploy Phase 0.</p>"},{"location":"operations/system-overview/#1334-associate-hosted-zones-step-removed-in-v121","title":"1.3.34. Associate Hosted Zones (Step removed in v1.2.1)","text":"<p>This step calls a Lambda function that associates the private zones, all the interface endpoint zones, and the resolver rules with each VPC that leverages endpoint services. This step was removed in v1.2.1 of the Accelerator codebase.</p>"},{"location":"operations/system-overview/#1335-add-tags-to-shared-resources","title":"1.3.35. Add Tags to Shared Resources","text":"<p>This step calls a Lambda function that adds tags to shared resources in the share destination account. For example, when a subnet is shared into another account, this step will add the <code>Name</code> tag to the subnet in the shared account.</p> <p>The supported resources are</p> <ul> <li>VPCs;</li> <li>subnets;</li> <li>security groups;</li> <li>transit gateway attachments.</li> </ul>"},{"location":"operations/system-overview/#1336-enable-directory-sharing","title":"1.3.36. Enable Directory Sharing","text":"<p>This step calls a Lambda function that shares Managed Active Directory according to the Accelerator configuration. The directory is shared from the source account to the target account. The directory will be accepted in the target account.</p>"},{"location":"operations/system-overview/#1337-deploy-phase-5","title":"1.3.37. Deploy Phase 5","text":"<ul> <li>create Remote Desktop Gateway;<ul> <li>create launch configuration;</li> <li>create autoscaling group;</li> </ul> </li> <li>enable central logging to S3 (step 2);</li> <li>Create CloudWatch Events for moveAccount, policyChanges and createAccount</li> <li>Creates CloudWatch Alarms</li> </ul>"},{"location":"operations/system-overview/#1338-create-ad-connector","title":"1.3.38. Create AD Connector","text":"<p>This step starts the <code>ASEA-DeleteDefaultVpcs_sfn</code> state machine. This state machine is responsible for creating AD connectors according to the Accelerator configuration.</p> <p>This step fails when one or more AD connectors failed to be created.</p>"},{"location":"operations/system-overview/#1339-store-commit-id","title":"1.3.39. Store Commit ID","text":"<p>This step calls a Lambda function that stores the commit ID of the configuration file for which the state machine ran.</p>"},{"location":"operations/system-overview/#1340-detach-quarantine-scp","title":"1.3.40. Detach Quarantine SCP","text":"<p>Executed only when using AWS Organizations baseline</p> <p>This step calls a Lambda function that stores the commit ID for which the state machine just ran.</p>"},{"location":"operations/troubleshooting/","title":"1. Troubleshooting","text":""},{"location":"operations/troubleshooting/#11-overview","title":"1.1. Overview","text":"<p>Issues could occur in different parts of the Accelerator. We'll guide you through troubleshooting these issues in this section.</p>"},{"location":"operations/troubleshooting/#12-components","title":"1.2. Components","text":""},{"location":"operations/troubleshooting/#121-state-machine","title":"1.2.1. State Machine","text":"<p>Viewing the step function <code>Graph inspector</code> (depicted above in 2.2), the majority of the main state machine has a large colored box around which is the functionality to catch state machine failures <code>Main Try Catch block to Notify users</code>. This large outer box will be blue while the state machine is still executing, it will be green upon a successful state machine execution and will turn orange/yellow on a state machine failure.</p> <p>What if my State Machine fails? Why? Previous solutions had complex recovery processes, what's involved?</p> <p>If your main state machine fails, review the error(s), resolve the problem and simply re-run the state machine. We've put a huge focus on ensuring the solution is idempotent and to ensure recovery is a smooth and easy process.</p> <p>Ensuring the integrity of deployed guardrails is critical in operating and maintaining an environment hosting protected data. Based on customer feedback and security best practices, we purposely fail the state machine if we cannot successfully deploy guardrails.</p> <p>Additionally, with millions of active customers each supporting different and diverse use cases and with the rapid rate of evolution of the AWS platform, sometimes we will encounter unexpected circumstances and the state machine might fail.</p> <p>We've spent a lot of time over the course of the Accelerator development process ensuring the solution can roll forward, roll backward, be stopped, restarted, and rerun without issues. A huge focus was placed on dealing with and writing custom code to manage and deal with non-idempotent resources (like S3 buckets, log groups, KMS keys, etc.). We've spent a lot of time ensuring that any failed artifacts are automatically cleaned up and don't cause subsequent executions to fail. We've put a strong focus on ensuring you do not need to go into your various AWS sub-accounts and manually remove or cleanup resources or deployment failures. We've also tried to provide usable error messages that are easy to understand and troubleshoot. As new scenario's are brought to our attention, we continue to adjust the codebase to better handle these situations.</p> <p>Will your state machine fail at some point in time, likely. Will you be able to easily recover and move forward without extensive time and effort, YES!</p> <p>As the state machine executes, each step will turn from white (not started), to blue (executing), to green (Success), or grey/red (failure). To diagnose the problem select the grey/red step that failed. If you miss the step and select the outer box, you will have selected the <code>Main Try Catch block to Notify users</code>. You need to carefully select the failed step.</p> <p></p> <p>As stated in section 2.2, the state machine contains 3 different types of states, which are each diagnosed differently.</p> <p>a. If the step is calling a Lambda function then you will see the following after clicking the failed step.</p> <p></p> <p>In this case, you can see that the <code>Cause</code> section contains a useful message. This message will differ between Lambda functions. In case this message does not make the issue clear, you can click on the <code>CloudWatch Logs</code> link in the <code>Resource</code> section to view the output of the Lambda function that was called by the step. See the section CloudWatch Logs. Note: The <code>Resource</code> section contains two links that blend together. You need to click the second link (<code>CloudWatch Logs</code>), not the first link which will open the actual resource/Lambda.</p> <p>b. In case the failed step started another state machine, you will see the following after clicking the failed step.</p> <p></p> <p>To view the state machine execution that failed you can click the link in the <code>Resource</code> section.</p> <p>In case the failed step started the CodeBuild state machine, <code>ASEA-CodeBuild_sm</code>, you will be able to see the CodeBuild project and execution ID that failed by looking at the output of the <code>Start Build</code> step in the <code>ASEA-CodeBuild_sm</code> state machine.</p> <p>.</p> <p>In the image above the execution of CodeBuild project <code>ASEA-DeployPrebuilt</code> with ID <code>ASEA-DeployPrebuilt:717584a9-c406-4569-9cc2-0d23e9ff9ef0</code> failed. See the CodeBuild section to troubleshoot.</p>"},{"location":"operations/troubleshooting/#122-codebuild","title":"1.2.2. CodeBuild","text":"<p>The Accelerator deploys and leverages two CodeBuild projects. The <code>ASEA-InstallerProject_pl</code> project is used by the Code Pipeline/Installer stack and <code>ASEA-DeployPrebuilt</code> which is used throughout the Accelerator state machine. Both are similar in that they use CDK to deploy stacks. The installer project will not exist, if the installer has been removed.</p> <p></p> <p>After a successful installation you will see the following in Codebuild, for the <code>ASEA-DeployPrebuilt</code> project:</p> <p></p> <p>When an error occurs you will see that the CodeBuild project execution fails when looking in the execution overview.</p> <p></p> <p>You can click on the name of the CodeBuild execution and then look inside the logs what caused the failure. These logs can be confusing. We are deploying multiple stacks in parallel and all the messages for all the parallel deployments are interleaved together, so make sure you are correlating the events back to the correct event source. Because we are deploying to 16 regions in parallel, you will also see messages for the same stack deployment interleaved. Even though a task may indicate it is complete and then another seemingly identical task indicates in-progress, the second message is coming from one of the alternate regions.</p> <p></p> <p>You can for example see the error message <code>The stack named ASEA-Perimeter-Phase2 is in a failed state: UPDATE_ROLLBACK_COMPLETE</code>. This means the stack <code>ASEA-Perimeter-Phase2</code> failed to update and it had to rollback. The error indicated at the bottom of the Codebuild screen is typically NOT the cause of the failure, just the end result. You need to scroll up and find the FIRST occurrence of an error in the log file. Often starting at the top of the log file and searching for the text <code>FAIL</code> (case sensitive), will allow you to find the relevant error message(s) quickly. The failure is typically listed in the CloudFormation update logs.</p> <p></p> <p>In this example we can see that the resource <code>FirewallManager</code> failed to create through CloudFormation. One way to solve this issue is to deprovision the firewall manager in the configuration file and then run the state machine. Next, provision the firewall manager and run the state machine again.</p> <p>If the error message is not clear, or the error occurred in a nested stack, then a more detailed error will be available in the CloudFormation stack events. See the CloudFormation section below.</p> <p></p>"},{"location":"operations/troubleshooting/#123-cloudformation","title":"1.2.3. CloudFormation","text":"<p>In case you want to troubleshoot errors that occurred in CloudFormation, the best way is to look in the CloudFormation stack's events. This requires you to assume a role into the relevant sub-account, and to locate the relevant failed, rolled-back, or deleted stack. Unfortunately, we are unable to log the region of the error message, so depending on what's being deployed, you may need to search all 16 regions for the failed stack.</p> <p></p> <p></p> <p>When a native resource fails to create or update there are no additional logs available except what is displayed in the <code>Status reason</code> column. When a custom resource fails to create or update -- i.e. not a native CloudFormation resource but a resource backed by a custom Lambda function -- then we can find additional logs in CloudWatch.</p> <p>Often the stack failure occurrs in a managed account instead of the root account. See Switch To a Managed Account to switch to the CloudFormation console in the managed account.</p>"},{"location":"operations/troubleshooting/#124-custom-resource","title":"1.2.4. Custom Resource","text":"<p>Custom resources are backed by a Lambda function that implements the creation, modification or deletion or the resource. Every Lambda function has a CloudWatch log group that contains logs about the custom resource creation. To troubleshoot errors in custom resource, you need to check the custom resource's log group.</p> <p>Example custom resource log group names:</p> <pre><code>/aws/lambda/ASEA-Master-Phase1-CustomCurReportDefinitionL-14IHLQCC1LY8L\n/aws/lambda/ASEA-Master-Phase2-AWS679f53fac002430cb0da5b7-Z75Q4GG9LIV5\n/aws/lambda/ASEA-Operations-Phas-AWS679f53fac002430cb0da5-HMV2YF6OKJET\n/aws/lambda/ASEA-Operations-Phas-CustomGetDetectorIdLambd-HEM07DR0DOOJ\n</code></pre>"},{"location":"operations/troubleshooting/#125-cloudwatch","title":"1.2.5. CloudWatch","text":"<p>When you arrived in CloudWatch logs by clicking on the state machine's step <code>CloudWatch Logs</code> link you will immediately see the list of log streams. Every log stream represents an instance of the Lambda function.</p> <p>You can find errors in multiple log groups using CloudWatch Log Insights.</p> <p></p> <pre><code>fields @timestamp, @message\n| sort @timestamp desc\n| filter strcontains(@message, 'ERROR')\n| limit 100\n</code></pre>"},{"location":"operations/troubleshooting/#126-codepipeline","title":"1.2.6. CodePipeline","text":"<ul> <li>\"Internal Failure\" incorrect Github token, repo or branch</li> </ul>"},{"location":"operations/troubleshooting/#13-examples","title":"1.3. Examples","text":"<p>Lets walk through a couple of example:</p>"},{"location":"operations/troubleshooting/#131-example-1","title":"1.3.1. Example 1","text":"<p>State Machine failed (Lambda), click on the grey box, then click on the Resource object:  Click on the red failed box, click on <code>Step Input</code>. The error is clearly indicated, we could not delete a Default VPC because the default VPC had dependencies, in a specified account and region. In this case several dependencies exist and need to be cleaned up to proceed (EIP's and something less obvious like security groups). </p>"},{"location":"operations/troubleshooting/#132-example-2","title":"1.3.2. Example 2","text":"<p>In the next example the state machine failed (sub-state machine) on the create accounts step. In this case rather than clicking on the <code>Graph inspector</code> we are going to scroll down through the <code>Execution event history</code> underneath the Graph inspector. We are going to find the FIRST failed task from the top of the list and then select the state machine from the prior task:  We will then click onn the red failed box, select <code>Exception</code> and we can see a clear error message - we have exceeded the maximum number of AWS accounts allowed in your organization:  Alternatively, in case the <code>Exception</code> error is not clear, we can select <code>Details</code> and then select <code>CloudWatch logs</code> for the end of the <code>Resource</code> section:  If you open the latest log stream in the opened log group (<code>/aws/lambda/ASEA-InitialSetup-StartAccountCreationHandler-1IZ2N4EP29D72</code>) and review the last several messages in the stream, the following clear message also appears:</p> <p></p>"},{"location":"operations/troubleshooting/#133-example-3","title":"1.3.3. Example 3","text":"<p>In the next example the state machine failed in one of the CodeBuild state machine steps, based on the <code>Resource</code> name of the failed step.  Rather than tracing this failure through the sub-state machine and then into the failed CodeBuild task, we are simply going to open AWS CodeBuild, and open the <code>ASEA-DeployPrebuilt</code> task. The failed task should be on the top of the Codebuild <code>build run</code> list. Open the build job.  Using your browser, from the top of the page, search for \"FAIL\", and we are immediately brought to the error. In this particular case we had an issue with the creation of VPC endpoints. We defined something not supported by the current configuration file. The solution was to simply remove the offending endpoints from the config file and re-run the state machine. </p>"},{"location":"pricing/sample_pricing/","title":"1. Accelerator Pricing","text":""},{"location":"pricing/sample_pricing/#11-overview","title":"1.1. Overview","text":"<p>The AWS Secure Environment Accelerator (ASEA) is available free of charge as an open source solution on GitHub. You are responsible for the cost of the AWS services enabled, configured, and deployed by the solution.</p> <p>The ASEA solution enables, configures and deploys two types of AWS services: services leveraged by the ASEA itself to deliver its capabilities; and services orchestrated by the ASEA to help create a secure multi-account AWS foundation for your users and workloads.</p> <p>The pricing for services leveraged by the ASEA are relatively consistent and small. The pricing for services orchestrated by the ASEA can vary dramatically based on the underlying architecture, services and features selected by a customer through the customizable configuration file.</p> <p>Most of the provided example ASEA configuration files (except ultra-lite) build a highly available and scalable multi-datacenter environment with hyperscale routing and enterprise grade security worldwide, something that would cost tens of millions of dollars on-premises and still not achieve the same results.</p> <p>As shown below, different configuration files can dramatically change the monthly cost of running the solution from $30/month, to $1500/month, to $2400/month, to over $3700/month. The price of the deployed solution is 100% dependent on what the customer deploys, and not on the Accelerator automation engine itself. While the example deployment(s) may appear expensive when used solely for testing in a personal account, they typically only represent a very small percentage of a production customers AWS spend. The examples were designed to minimize costs as a customer scales.</p> <p>This document is designed to assist customers in understanding the pricing associated with operating the example ASEA configuration files. For full pricing details, please refer to each services pricing page.</p>"},{"location":"pricing/sample_pricing/#12-example-configuration-file-pricing","title":"1.2. Example Configuration File Pricing","text":"<p>The pricing found in this document is provided as an example only. Pricing represents reasonably steady state, minimal activity or traffic flows, and only includes sample workload accounts when they exist in the example config files.</p> <p>Pricing is based on the ca-central-1 region, a month with 31 days (744 hours), on-demand pricing and Bring Your Own Licensing (BYOL) for any 3rd party firewalls. This is estimated pricing, the solution is regularly updated and pricing is dependent on the actual version and configuration used to implement the solution.</p> <p>Any changes to the example configuration file will impact the pricing. These estimates do not include any customer workloads, workloads must be independently priced.</p>"},{"location":"pricing/sample_pricing/#121-pricing-by-configuration-file","title":"1.2.1. Pricing by Configuration file","text":"<p>The following table provides the estimated monthly pricing based on the example configuration. Additional information on each of the example config files can be found here.</p> ExampleConfiguration Description Estimated Monthly Pricing Ultra-Lite This configuration file was created to represent an extremely minimalistic Accelerator deployment, to demonstrate the art of the possible for an extremely simple config. This example is NOT recommended as it violates many AWS best practices. $30 Test Designed to reduce solution costs, while demonstrating full solution functionality (Use for testing Full/Lite configurations or Low Security Profiles). Based on Lite Config w/AWS Network Firewall. $1,500 Lite Same as Full Config with the following changes: 1) Reduces the FortiGate instance sizes from c5n.2xl to c5n.xl (VM08 to VM04); 2) Only deploys the 9 required centralized Interface Endpoints (removes 50). All services remain accessible using the AWS public endpoints, but require traversing the perimeter firewalls; 3) Removes the perimeter VPC Interface Endpoints; 4) Removes the Unclass ou and VPC.Four variants of the lite configuration file are provided:- AWS Control Tower w/AWS Network Firewall instead of IPSEC VPN Firewalls (recommended starting point)- AWS Network Firewall instead of IPSEC VPN Firewalls- IPSEC VPN integrated 3rd party firewalls- AWS Gateway Load Balancer integrated 3rd party firewalls $2,575  $2,550  $2,450+FW lic. $2475+FW lic. Full Large IPSEC VPN Firewalls w/Endpoints - The full configuration file was based on feedback from customers moving into AWS at scale and at a rapid pace. Customers of this nature have indicated that they do not want to have to upsize their perimeter firewalls or add Interface endpoints as their developers start to use new AWS services. These are the two most expensive components of the deployed architecture solution. $4,200"},{"location":"pricing/sample_pricing/#122-pricing-by-aws-account-all-configurations","title":"1.2.2. Pricing by AWS Account (All Configurations)","text":"<p>The following table provides the estimated monthly pricing per AWS account for each of the example configuration files.</p> AWS Account Description UltraLite Test Lite Full Management This is the organization management or root account. This account aggregates organization wide billing, and is used to manage the Accelerator, AWS SSO and SCPs. Access to this account must be highly restricted. This account should not contain any customer resources or workloads. $10 $75 $140 $140 Operations This Account is used for centralized IT operational resources (MAD, rsyslog, ITSM, etc.) which need to made available to all accounts in the organization and would generally be used and managed by the Cloud Operations team. - $275 $680 $680 Security The security account is generally used and managed by the customers security and compliance teams, and contains an organizations security tooling and consoles. This account functions as the organization administrative account for Security Hub, GuardDuty, Macie, Firewall Manager, and Access Analyzer. This account also has the ability to assume a view-only role in every account in the organization to conduct security investigations. $5 $10 $25 $25 Log Archive The log archive account provides a central aggregation and secure long-term storage location for all logs created within the AWS organization. Logs created in every account in the organization are centralized to an S3 bucket in this account. $15 $35 $55 $55 Perimeter This account is used as the centralized internet facing ingress/egress point and contains edge security services for the organizations IaaS based workloads. - $590 $410-$700 $1,200 Shared Network This account is used for centralized or shared networking resources and will typically contain a transit gateway to enable routing between different AWS based and on-premises networks. If a centralized or shared VPC architecture is deployed, this account will also contain VPCs (i.e. Dev, Test, Prod) which are shared via RAM sharing to accounts within designated OUs in the organization. If a spoke architecture is used, the Transit gateway is instead shared to the accounts within the organization. - $515 $825-$995 $1,950 MyDev1 This is an optional sample workload account which lives in the Dev organizational unit. Dev accounts have a full set of security guardrails similar to a production accounts and are designed to be used by developers. These accounts leverage either local or centralized networking and are connected to the organizations network via the centralized transit gateway, which is used to access the internet via the perimeter security account or on-premises networks. - - $80 $80 TheFunAccount This is an optional sample workload account that is created in Sandbox organizational unit. Sandbox accounts are designed for experimentation only, as they have the fewest guardrails, and provide the most cloud native experience. These accounts leverage localized networking and are fully isolated from all other organization networks, with no transit gateway connectivity and direct internet access via a local internet gateway. - - $70 $70 TOTAL Estimated Monthly Pricing $30 $1500 $2,450 - $2,575 $4,200"},{"location":"pricing/sample_pricing/#123-detailed-pricing-by-aws-service-lite-config-ipsec-vpn-activeactive-firewalls","title":"1.2.3. Detailed Pricing by AWS Service (Lite Config \u2013 IPSec VPN Active/Active Firewalls)","text":"<p>We picked a single example configuration file to provide detailed pricing per service.</p> <p>The following table provides the estimated monthly pricing per AWS services provisioned by the Accelerator, across all accounts, for the Lite \u2013 IPSec VPN configuration.</p> AWS service Quantity Estimated Monthly Pricing CloudTrail (All Regions) $28 CloudWatch (All Regions) $35 CloudWatch Events (All Regions) $0 CodeBuild $2 CodeCommit $0 CodePipeline $0 Config (All Regions) $85 Data Transfer $0 Directory Service - Managed Active Directory (2 domain controllers)- Shared Directory (2 accounts)- Small AD Connector (1) $444 DynamoDB $0 EC2 Container Registry (ECR) $0.2 Elastic Compute Cloud (EC2) - NAT Gateway (1)- Remote Desktop Gateway (1 x Windows t3.large)- rsyslog Servers (2 x Linux t3.large)- Fortinet Firewalls (2 x Linux c5n.xlarge)- EBS Volumes (30 GB x 3 instances, 100 GB x 2 instances) $669 Elastic Load Balancing - Application Load Balancing (2)- Network Load Balancing (rsyslog) (1) $55 GuardDuty (All Regions) $41 Key Management Service (All Regions) $44 Kinesis $12 Kinesis Firehose $2 Lambda (All Regions) $0 Macie (All Regions) $4 Route 53 - HostedZones (11)- Resolver Network Interfaces (4) $378 Secrets Manager $5 Security Hub (All Regions) $97 Simple Notification Service (All regions) $0 Simple Queue Service (All Regions) $0 Simple Storage Service (All regions) $6 Step Functions $1 Systems Manager $0 Virtual Private Cloud - VPC Endpoints (18)- VPN Connections (2)- Transit Gateway VPC Attachments (5)- Transit Gateway VPN Attachments (2) $542 TOTAL Estimated Monthly Pricing $2,450"},{"location":"schema/index-NotUsed/","title":"Configuration File Schema Documentation","text":"<p>English Documentation: https://aws-samples.github.io/aws-secure-environment-accelerator/schema/en/index.html</p> <p>Documentation Fran\u00e7aise: https://aws-samples.github.io/aws-secure-environment-accelerator/schema/fr/index.html</p>"},{"location":"workshops/","title":"Accelerator Workshops","text":""},{"location":"workshops/#accelerator-administrator-immersion-day","title":"Accelerator Administrator Immersion Day","text":"<p>The Accelerator Administrator Immersion Day is focused on helping administrators who will be administering the landing zone understand how they can design, build and operate the components in ASEA. Click here for an overview of the topics covered.</p>"},{"location":"workshops/#accelerator-workloadapplication-team-immersion-day","title":"Accelerator Workload/Application Team Immersion Day","text":"<p>The Accelerator Workload/Application Team Immersion Day is focused on helping project teams understand what it means to operate within an ASEA managed environment. Click here for an overview of the topics covered.</p>"}]}